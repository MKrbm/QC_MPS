{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import umps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "def embedding_pixel(batch, label: int = 0):\n",
    "    pixel_size = batch.shape[-1] * batch.shape[-2]\n",
    "    x = batch.view(*batch.shape[:-2], pixel_size)\n",
    "    # x[:] = 0\n",
    "    x = torch.stack([x, 1-x], dim=-1)\n",
    "    # x = x / torch.sum(x, dim=-1).unsqueeze(-1)\n",
    "    x = x / torch.norm(x, dim=-1).unsqueeze(-1)\n",
    "    return x\n",
    "\n",
    "def embedding_label(labels: torch.Tensor):\n",
    "    emb = torch.zeros(labels.shape[0], 2)\n",
    "    emb[torch.arange(labels.shape[0]), labels] = 1\n",
    "    return emb\n",
    "\n",
    "def filiter_single_channel(batch):\n",
    "    return batch[0, ...]\n",
    "\n",
    "def filter_dataset(dataset, allowed_digits=[0, 1]):\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if label in allowed_digits:\n",
    "            indices.append(i)\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "img_size = 16\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(filiter_single_channel),\n",
    "    transforms.Lambda(embedding_pixel),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.QMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# trainset = filter_dataset(trainset, allowed_digits=[0, 1])\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset,\n",
    "#     batch_size=128,\n",
    "#     shuffle=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS unitaries\n"
     ]
    }
   ],
   "source": [
    "import unitary_optimizer\n",
    "umpsm = umps.uMPS(N = 16 * 16, chi = 2, d = 2, l = 2, layers = 1, device = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(outputs, labels):\n",
    "    device = outputs.device\n",
    "    loss = torch.zeros(1, device=device, dtype=torch.float64)\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        prob = outputs[i] if labels[i] == 0 else 1 - outputs[i]\n",
    "        loss -= torch.log(prob + 1e-8)\n",
    "    return loss\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    predictions = (outputs < 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    accuracy = correct / labels.numel()\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mumpsm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      2\u001b[0m u \u001b[38;5;241m@\u001b[39m u\u001b[38;5;241m.\u001b[39mT\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "u = umpsm.params[-1].grad.reshape(4, 4)\n",
    "u @ u.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_batch(outputs, target)\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mumpsm_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m     14\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m calculate_accuracy(outputs, target)\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/unitary_optimizer.py:144\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m rg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m*\u001b[39m mhat \u001b[38;5;241m/\u001b[39m (torch\u001b[38;5;241m.\u001b[39msqrt(vhat) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Apply the exponential map to update the unitary\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m u_updated \u001b[38;5;241m=\u001b[39m \u001b[43mexp_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Update the unitary in the circuit\u001b[39;00m\n\u001b[1;32m    147\u001b[0m unitary\u001b[38;5;241m.\u001b[39mcopy_(u_updated\u001b[38;5;241m.\u001b[39mview(unitary\u001b[38;5;241m.\u001b[39mshape))\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/unitary_optimizer.py:16\u001b[0m, in \u001b[0;36mexp_map\u001b[0;34m(u, rg)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexp_map\u001b[39m(u: torch\u001b[38;5;241m.\u001b[39mTensor, rg: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    Exponential map on the unitary manifold.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     res \u001b[38;5;241m=\u001b[39m  \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mrg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m u\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m closest_unitary(res)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "umpsm_op = unitary_optimizer.Adam(umpsm, lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    acc = 0\n",
    "    for data, target in trainloader:    \n",
    "        data = data.permute(1, 0, 2)\n",
    "        umpsm_op.zero_grad()\n",
    "        outputs = umpsm(data)\n",
    "        loss = loss_batch(outputs, target)\n",
    "        loss.backward()\n",
    "        umpsm_op.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = calculate_accuracy(outputs, target)\n",
    "        # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        acc += accuracy\n",
    "    acc /= len(trainloader)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    # print(\"grad\", umpsm.params[-1].grad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps.kraus_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -1.5151e-09,  3.6242e-10, -8.2355e-09],\n",
       "        [-1.5151e-09,  1.0000e+00,  3.9718e-09,  5.4807e-09],\n",
       "        [ 3.6242e-10,  3.9718e-09,  1.0000e+00,  7.3616e-09],\n",
       "        [-8.2355e-09,  5.4807e-09,  7.3616e-09,  1.0000e+00]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I = torch.zeros(4, 4)\n",
    "\n",
    "for kraus_op in mps.kraus_ops[0]:\n",
    "    I += kraus_op.T @ kraus_op\n",
    "\n",
    "I\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4929, 0.1871],\n",
       "        [0.1871, 0.5071]], dtype=torch.float64, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tpcp_mps\n",
    "from importlib import reload\n",
    "reload(tpcp_mps)\n",
    "\n",
    "mps = tpcp_mps.MPSTPCP(N=16 * 16, K=1, d=2)\n",
    "\n",
    "outputs = mps.forward(data[:, :, :].to(torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3217, 0.5300, 0.6689, 0.4137, 0.4518, 0.4848, 0.4619, 0.6848, 0.8434,\n",
       "        0.4206, 0.6056, 0.4578, 0.6694, 0.7811, 0.2808, 0.4834, 0.4791, 0.4612,\n",
       "        0.0448, 0.7090, 0.4482, 0.6645, 0.3723, 0.4935, 0.3612, 0.5337, 0.6680,\n",
       "        0.3314, 0.4152, 0.8303, 0.4398, 0.4487, 0.7049, 0.9043, 0.7533, 0.2612,\n",
       "        0.8024, 0.4165, 0.2396, 0.3580, 0.3231, 0.2031, 0.5297, 0.4053, 0.1495,\n",
       "        0.6114, 0.3828, 0.1742, 0.3090, 0.6286, 0.4317, 0.6117, 0.6485, 0.6831,\n",
       "        0.4552, 0.5844, 0.5443, 0.6155, 0.3997, 0.4054, 0.1457, 0.1112, 0.3017,\n",
       "        0.4169, 0.4252, 0.3610, 0.7113, 0.3329, 0.3020, 0.2280, 0.4437, 0.6049,\n",
       "        0.2926, 0.2494, 0.5580, 0.5982, 0.6938, 0.4332, 0.3964, 0.6479, 0.6917,\n",
       "        0.5261, 0.6665, 0.5378, 0.5206, 0.6813, 0.5700, 0.5287, 0.3308, 0.7020,\n",
       "        0.4728, 0.6162, 0.4287, 0.5274, 0.7051, 0.4734, 0.4715, 0.1893, 0.6181,\n",
       "        0.4499], dtype=torch.float64, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_gen = torch.randn(100, 16 * 16, 2)\n",
    "rand_gen /= torch.norm(rand_gen, dim=-1).unsqueeze(-1)\n",
    "\n",
    "mps.forward(rand_gen.to(torch.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 4, 4])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  5.5511e-17, -5.5511e-17,  0.0000e+00],\n",
       "        [ 5.5511e-17,  1.0000e+00, -2.7756e-17,  2.7756e-17],\n",
       "        [-5.5511e-17, -2.7756e-17,  1.0000e+00,  5.5511e-17],\n",
       "        [ 0.0000e+00,  2.7756e-17,  5.5511e-17,  1.0000e+00]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = mps.kraus_ops[-1].grad[0]\n",
    "G = G / torch.norm(G)\n",
    "K = mps.kraus_ops[-1].detach()[0]\n",
    "\n",
    "\n",
    "A = torch.cat([G, K], dim=1)\n",
    "B = torch.cat([K, -G], dim=1)\n",
    "\n",
    "rg = A @ torch.linalg.inv(torch.eye(8) + 0.5 * 0.05 * B.T @ A) @ B.T @ K\n",
    "Kp = K - rg * 0.05\n",
    "\n",
    "Kp @ Kp.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  1.9005],\n",
       "         [ 0.0000,  0.0000,  0.0000, -0.1128],\n",
       "         [ 0.0000,  0.0000,  0.0000, 21.8310],\n",
       "         [ 0.0000,  0.0000,  0.0000,  2.1302]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps.kraus_ops[0].grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.0781, -0.2654,  0.9404, -0.1976],\n",
       "         [ 0.5015, -0.7846, -0.2378, -0.2761],\n",
       "         [-0.4079,  0.1087, -0.1895, -0.8865],\n",
       "         [-0.7589, -0.5496, -0.1522,  0.3143]]], dtype=torch.float64,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -3.3307e-16, -2.6368e-16,  4.1633e-17],\n",
       "        [-3.3307e-16,  1.0000e+00,  2.2898e-16, -3.6082e-16],\n",
       "        [-2.6368e-16,  2.2898e-16,  1.0000e+00, -2.2204e-16],\n",
       "        [ 4.1633e-17, -3.6082e-16, -2.2204e-16,  1.0000e+00]],\n",
       "       dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.params[0][0] @ optimizer.params[0][0].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.0462, -0.5530, -0.5585, -0.6166],\n",
       "         [ 0.3898, -0.3660,  0.7532, -0.3832],\n",
       "         [ 0.6462, -0.4353, -0.2547,  0.5727],\n",
       "         [-0.6544, -0.6089,  0.2366,  0.3808]]], dtype=torch.float64,\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps.kraus_ops[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -1.6662e-16, -6.0210e-17, -4.3451e-17],\n",
       "        [-1.6662e-16,  1.0000e+00,  1.8244e-17, -6.5638e-17],\n",
       "        [-6.0210e-17,  1.8244e-17,  1.0000e+00,  1.2042e-16],\n",
       "        [-4.3451e-17, -6.5638e-17,  1.2042e-16,  1.0000e+00]],\n",
       "       dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = mps.kraus_ops[0].reshape(8, 4) \n",
    "\n",
    "K.T @ K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -9.8817e-19,  1.8542e-17,  1.5244e-16],\n",
       "        [-9.8817e-19,  1.0000e+00, -2.7013e-18, -3.7641e-17],\n",
       "        [ 1.8542e-17, -2.7013e-18,  1.0000e+00, -1.6334e-17],\n",
       "        [ 1.5244e-16, -3.7641e-17, -1.6334e-17,  1.0000e+00]],\n",
       "       dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps.kraus_ops[0].T @ mps.kraus_ops[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88.33191124933472\n",
      "0.5390625\n",
      "88.33190862632068\n",
      "0.5390625\n",
      "88.33190369198766\n",
      "0.5390625\n",
      "88.33189689400702\n",
      "0.5390625\n",
      "88.3318887714528\n",
      "0.5390625\n",
      "88.33187990826016\n",
      "0.5390625\n",
      "88.331870888128\n",
      "0.5390625\n",
      "88.3318622538474\n",
      "0.5390625\n",
      "88.33185447344013\n",
      "0.5390625\n",
      "88.3318479147869\n",
      "0.5390625\n",
      "88.33184282967609\n",
      "0.5390625\n",
      "88.33183934745813\n",
      "0.5390625\n",
      "88.33183747780937\n",
      "0.5390625\n",
      "88.33183712152645\n",
      "0.5390625\n",
      "88.33183808782029\n",
      "0.5390625\n",
      "88.33184011627898\n",
      "0.5390625\n",
      "88.33184290152242\n",
      "0.5390625\n",
      "88.33184611857837\n",
      "0.5390625\n",
      "88.33184944715126\n",
      "0.5390625\n",
      "88.33185259321003\n",
      "0.5390625\n",
      "88.33185530665929\n",
      "0.5390625\n",
      "88.33185739425168\n",
      "0.5390625\n",
      "88.33185872730965\n",
      "0.5390625\n",
      "88.33185924422692\n",
      "0.5390625\n",
      "88.3318589480855\n",
      "0.5390625\n",
      "88.3318579000275\n",
      "0.5390625\n",
      "88.33185620925777\n",
      "0.5390625\n",
      "88.3318540206942\n",
      "0.5390625\n",
      "88.33185150135407\n",
      "0.5390625\n",
      "88.33184882654099\n",
      "0.5390625\n",
      "88.33184616680894\n",
      "0.5390625\n",
      "88.33184367653107\n",
      "0.5390625\n",
      "88.33184148470836\n",
      "0.5390625\n",
      "88.3318396884362\n",
      "0.5390625\n",
      "88.33183834922204\n",
      "0.5390625\n",
      "88.33183749213111\n",
      "0.5390625\n",
      "88.33183710754327\n",
      "0.5390625\n",
      "88.33183715514457\n",
      "0.5390625\n",
      "88.33183756965767\n",
      "0.5390625\n",
      "88.33183826774652\n",
      "0.5390625\n",
      "88.33183915550087\n",
      "0.5390625\n",
      "88.33184013592454\n",
      "0.5390625\n",
      "88.33184111591116\n",
      "0.5390625\n",
      "88.33184201227125\n",
      "0.5390625\n",
      "88.331842756487\n",
      "0.5390625\n",
      "88.3318432979895\n",
      "0.5390625\n",
      "88.33184360587457\n",
      "0.5390625\n",
      "88.33184366909131\n",
      "0.5390625\n",
      "88.33184349523891\n",
      "0.5390625\n",
      "88.3318431081909\n",
      "0.5390625\n",
      "88.33184254482423\n",
      "0.5390625\n",
      "88.33184185117\n",
      "0.5390625\n",
      "88.33184107830537\n",
      "0.5390625\n",
      "88.33184027829775\n",
      "0.5390625\n",
      "88.3318395004784\n",
      "0.5390625\n",
      "88.33183878826848\n",
      "0.5390625\n",
      "88.33183817672617\n",
      "0.5390625\n",
      "88.33183769091245\n",
      "0.5390625\n",
      "88.33183734511034\n",
      "0.5390625\n",
      "88.33183714286724\n",
      "0.5390625\n",
      "88.33183707777823\n",
      "0.5390625\n",
      "88.33183713488303\n",
      "0.5390625\n",
      "88.33183729252018\n",
      "0.5390625\n",
      "88.33183752446556\n",
      "0.5390625\n",
      "88.33183780217969\n",
      "0.5390625\n",
      "88.33183809699601\n",
      "0.5390625\n",
      "88.33183838210665\n",
      "0.5390625\n",
      "88.3318386342255\n",
      "0.5390625\n",
      "88.33183883484632\n",
      "0.5390625\n",
      "88.3318389710476\n",
      "0.5390625\n",
      "88.33183903583212\n",
      "0.5390625\n",
      "88.3318390280246\n",
      "0.5390625\n",
      "88.33183895177538\n",
      "0.5390625\n",
      "88.33183881574483\n",
      "0.5390625\n",
      "88.33183863205475\n",
      "0.5390625\n",
      "88.33183841510316\n",
      "0.5390625\n",
      "88.33183818033709\n",
      "0.5390625\n",
      "88.33183794307256\n",
      "0.5390625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[230], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m mps\u001b[38;5;241m.\u001b[39mforward(data[:, :, :]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat64))\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_batch(outputs, target)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import kraus_optimizer\n",
    "reload(kraus_optimizer)\n",
    "reload(tpcp_mps)\n",
    "# mps = tpcp_mps.MPSTPCP(N=16 * 16, K=2, d=2)\n",
    "# optimizer = kraus_optimizer.Adam(mps.kraus_ops, lr=0.0001)\n",
    "optimizer = kraus_optimizer.CayleySGDMomentum(mps.kraus_ops, lr=0.0001, beta=0.95, q=0.5, s=4)\n",
    "\n",
    "for _ in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mps.forward(data[:, :, :].to(torch.float64))\n",
    "    loss = loss_batch(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "    print(calculate_accuracy(outputs, target))\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     acc_tot = 0\n",
    "#     loss_tot = 0\n",
    "#     for data, target in trainloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = mps.forward(data[:, :, :].to(torch.float64))\n",
    "#         loss = loss_batch(outputs, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         acc = calculate_accuracy(outputs, target)\n",
    "#         acc_tot += acc\n",
    "#         loss_tot += loss.item()\n",
    "#     acc_tot /= len(trainloader)\n",
    "#     loss_tot /= len(trainloader)\n",
    "#     print(f\"Accuracy: {acc_tot:.4f}\")\n",
    "#     print(f\"Loss: {loss_tot:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0.0000, -20810.3705,      0.0000,  19047.8718],\n",
       "        [     0.0000,      0.0000,      0.0000,      0.0000],\n",
       "        [     0.0000, -25382.7747,      0.0000,   7647.4490],\n",
       "        [     0.0000,      0.0000,      0.0000,      0.0000]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps.kraus_ops[-1].gra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  7.2720e-15, -1.4322e-14,  2.0721e-15],\n",
       "        [ 7.2720e-15,  1.0000e+00,  2.4009e-15, -2.2520e-14],\n",
       "        [-1.4322e-14,  2.4009e-15,  1.0000e+00, -5.4816e-15],\n",
       "        [ 2.0721e-15, -2.2520e-14, -5.4816e-15,  1.0000e+00]],\n",
       "       dtype=torch.float64, grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = mps_tpcp.kraus_ops[-10]\n",
    "u @ u.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.5022, -0.2951],\n",
       "          [ 0.5226,  0.6225]],\n",
       "\n",
       "         [[-0.5633, -0.2183],\n",
       "          [-0.4017,  0.6883]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4872, -0.5938],\n",
       "          [-0.6219, -0.1525]],\n",
       "\n",
       "         [[ 0.4394,  0.7160],\n",
       "          [-0.4228,  0.3399]]]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umpsm.params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2863, 0.5068, 0.4480, 0.3460, 0.5092, 0.4719, 0.6662, 0.3924, 0.6893,\n",
       "        0.5020], dtype=torch.float64, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8430, 0.8851, 0.8424, 0.8830, 0.0178, 0.9027, 0.0251, 0.8285, 0.8845,\n",
      "        0.0244, 0.0148, 0.0168, 0.0300, 0.0149, 0.9047, 0.1952, 0.0144, 0.1937,\n",
      "        0.8536, 0.8440, 0.8338, 0.7988, 0.7240, 0.0137, 0.0134, 0.8092, 0.0148,\n",
      "        0.8996, 0.8683, 0.8271, 0.0218, 0.8844, 0.0131, 0.0134, 0.0241, 0.9100,\n",
      "        0.0253, 0.7668, 0.8456, 0.8256, 0.0136, 0.0143, 0.0164, 0.7096, 0.9464,\n",
      "        0.9265, 0.7339, 0.7295, 0.7967, 0.9186, 0.0278, 0.8579, 0.1444, 0.0309,\n",
      "        0.7035, 0.0141, 0.8855, 0.0148, 0.7287, 0.0274, 0.8310, 0.0129, 0.0217,\n",
      "        0.8733, 0.0219, 0.0436, 0.9204, 0.3600, 0.1956, 0.7671, 0.0143, 0.8237,\n",
      "        0.0142, 0.0132, 0.0165, 0.7980, 0.4413, 0.0176, 0.7909, 0.1937, 0.0148,\n",
      "        0.8425, 0.0761, 0.9172, 0.0302, 0.0144, 0.8631, 0.8260, 0.7069, 0.8629,\n",
      "        0.0128, 0.0527, 0.0152, 0.8021, 0.2644, 0.8366, 0.4279, 0.0167, 0.8583,\n",
      "        0.5997, 0.0132, 0.7826, 0.0138, 0.0157, 0.0142, 0.0141, 0.8855, 0.1429,\n",
      "        0.0293, 0.8130, 0.0581, 0.7953, 0.7932, 0.0141, 0.8751, 0.0637, 0.8552,\n",
      "        0.8860, 0.0350, 0.0137, 0.0139, 0.8362, 0.8095, 0.0145, 0.0137, 0.0232,\n",
      "        0.7260, 0.9488], dtype=torch.float64, grad_fn=<ViewBackward0>)\n",
      "tensor([0.8430, 0.8851, 0.8424, 0.8830, 0.0178, 0.9027, 0.0251, 0.8285, 0.8845,\n",
      "        0.0244, 0.0148, 0.0168, 0.0300, 0.0149, 0.9047, 0.1952, 0.0144, 0.1937,\n",
      "        0.8536, 0.8440, 0.8338, 0.7988, 0.7240, 0.0137, 0.0134, 0.8092, 0.0148,\n",
      "        0.8996, 0.8683, 0.8271, 0.0218, 0.8844, 0.0131, 0.0134, 0.0241, 0.9100,\n",
      "        0.0253, 0.7668, 0.8456, 0.8256, 0.0136, 0.0143, 0.0164, 0.7096, 0.9464,\n",
      "        0.9265, 0.7339, 0.7295, 0.7967, 0.9186, 0.0278, 0.8579, 0.1444, 0.0309,\n",
      "        0.7035, 0.0141, 0.8855, 0.0148, 0.7287, 0.0274, 0.8310, 0.0129, 0.0217,\n",
      "        0.8733, 0.0219, 0.0436, 0.9204, 0.3600, 0.1956, 0.7671, 0.0143, 0.8237,\n",
      "        0.0142, 0.0132, 0.0165, 0.7980, 0.4413, 0.0176, 0.7909, 0.1937, 0.0148,\n",
      "        0.8425, 0.0761, 0.9172, 0.0302, 0.0144, 0.8631, 0.8260, 0.7069, 0.8629,\n",
      "        0.0128, 0.0527, 0.0152, 0.8021, 0.2644, 0.8366, 0.4279, 0.0167, 0.8583,\n",
      "        0.5997, 0.0132, 0.7826, 0.0138, 0.0157, 0.0142, 0.0141, 0.8855, 0.1429,\n",
      "        0.0293, 0.8130, 0.0581, 0.7953, 0.7932, 0.0141, 0.8751, 0.0637, 0.8552,\n",
      "        0.8860, 0.0350, 0.0137, 0.0139, 0.8362, 0.8095, 0.0145, 0.0137, 0.0232,\n",
      "        0.7260, 0.9488], dtype=torch.float64, grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "reload(tpcp_mps)\n",
    "N = 16 * 16\n",
    "mps_tpcp = tpcp_mps.MPSTPCP(N=N, K=1, d=2)\n",
    "# mps_unitary = umps.uMPS(N=N, chi=2, d=2, l=2, layers=1, device=\"cpu\")\n",
    "mps_unitary = umpsm\n",
    "for i in range(len(mps_unitary.params)):\n",
    "    mps_tpcp.kraus_ops[i].data[:] = mps_unitary.params[i].reshape(4,4).T\n",
    "# random_gen = torch.randn(10, N, 2)\n",
    "random_gen = data.to(torch.float64)\n",
    "# random_gen /= torch.norm(random_gen, dim=-1).unsqueeze(-1)\n",
    "\n",
    "print(mps_unitary.forward(random_gen.to(torch.float64).permute(1, 0, 2)))\n",
    "print(mps_tpcp.forward(random_gen.to(torch.float64)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8600 / Loss: 45.5356\n",
      "Accuracy: 0.8600 / Loss: 45.5349\n",
      "Accuracy: 0.8600 / Loss: 45.5342\n",
      "Accuracy: 0.8600 / Loss: 45.5335\n",
      "Accuracy: 0.8600 / Loss: 45.5328\n",
      "Accuracy: 0.8600 / Loss: 45.5321\n",
      "Accuracy: 0.8600 / Loss: 45.5314\n",
      "Accuracy: 0.8600 / Loss: 45.5307\n",
      "Accuracy: 0.8600 / Loss: 45.5300\n",
      "Accuracy: 0.8600 / Loss: 45.5293\n",
      "Accuracy: 0.8600 / Loss: 45.5287\n",
      "Accuracy: 0.8600 / Loss: 45.5280\n",
      "Accuracy: 0.8600 / Loss: 45.5273\n",
      "Accuracy: 0.8600 / Loss: 45.5266\n",
      "Accuracy: 0.8600 / Loss: 45.5259\n",
      "Accuracy: 0.8600 / Loss: 45.5252\n",
      "Accuracy: 0.8600 / Loss: 45.5246\n",
      "Accuracy: 0.8600 / Loss: 45.5239\n",
      "Accuracy: 0.8600 / Loss: 45.5232\n",
      "Accuracy: 0.8600 / Loss: 45.5225\n",
      "Accuracy: 0.8600 / Loss: 45.5218\n",
      "Accuracy: 0.8600 / Loss: 45.5211\n",
      "Accuracy: 0.8600 / Loss: 45.5205\n",
      "Accuracy: 0.8600 / Loss: 45.5198\n",
      "Accuracy: 0.8600 / Loss: 45.5191\n",
      "Accuracy: 0.8600 / Loss: 45.5184\n",
      "Accuracy: 0.8600 / Loss: 45.5177\n",
      "Accuracy: 0.8600 / Loss: 45.5171\n",
      "Accuracy: 0.8600 / Loss: 45.5164\n",
      "Accuracy: 0.8600 / Loss: 45.5157\n",
      "Accuracy: 0.8600 / Loss: 45.5150\n",
      "Accuracy: 0.8600 / Loss: 45.5143\n",
      "Accuracy: 0.8600 / Loss: 45.5137\n",
      "Accuracy: 0.8600 / Loss: 45.5130\n",
      "Accuracy: 0.8600 / Loss: 45.5123\n",
      "Accuracy: 0.8600 / Loss: 45.5116\n",
      "Accuracy: 0.8600 / Loss: 45.5109\n",
      "Accuracy: 0.8600 / Loss: 45.5102\n",
      "Accuracy: 0.8600 / Loss: 45.5095\n",
      "Accuracy: 0.8600 / Loss: 45.5089\n",
      "Accuracy: 0.8600 / Loss: 45.5082\n",
      "Accuracy: 0.8600 / Loss: 45.5075\n",
      "Accuracy: 0.8600 / Loss: 45.5068\n",
      "Accuracy: 0.8600 / Loss: 45.5061\n",
      "Accuracy: 0.8600 / Loss: 45.5054\n",
      "Accuracy: 0.8600 / Loss: 45.5047\n",
      "Accuracy: 0.8600 / Loss: 45.5040\n",
      "Accuracy: 0.8600 / Loss: 45.5033\n",
      "Accuracy: 0.8600 / Loss: 45.5026\n",
      "Accuracy: 0.8600 / Loss: 45.5019\n",
      "Accuracy: 0.8600 / Loss: 45.5012\n",
      "Accuracy: 0.8600 / Loss: 45.5005\n",
      "Accuracy: 0.8600 / Loss: 45.4998\n",
      "Accuracy: 0.8600 / Loss: 45.4991\n",
      "Accuracy: 0.8600 / Loss: 45.4984\n",
      "Accuracy: 0.8600 / Loss: 45.4976\n",
      "Accuracy: 0.8600 / Loss: 45.4969\n",
      "Accuracy: 0.8600 / Loss: 45.4962\n",
      "Accuracy: 0.8600 / Loss: 45.4955\n",
      "Accuracy: 0.8600 / Loss: 45.4948\n",
      "Accuracy: 0.8600 / Loss: 45.4940\n",
      "Accuracy: 0.8600 / Loss: 45.4933\n",
      "Accuracy: 0.8600 / Loss: 45.4926\n",
      "Accuracy: 0.8600 / Loss: 45.4919\n",
      "Accuracy: 0.8600 / Loss: 45.4911\n",
      "Accuracy: 0.8600 / Loss: 45.4904\n",
      "Accuracy: 0.8600 / Loss: 45.4897\n",
      "Accuracy: 0.8600 / Loss: 45.4889\n",
      "Accuracy: 0.8600 / Loss: 45.4882\n",
      "Accuracy: 0.8600 / Loss: 45.4874\n",
      "Accuracy: 0.8600 / Loss: 45.4867\n",
      "Accuracy: 0.8600 / Loss: 45.4859\n",
      "Accuracy: 0.8600 / Loss: 45.4852\n",
      "Accuracy: 0.8600 / Loss: 45.4844\n",
      "Accuracy: 0.8600 / Loss: 45.4837\n",
      "Accuracy: 0.8600 / Loss: 45.4829\n",
      "Accuracy: 0.8600 / Loss: 45.4821\n",
      "Accuracy: 0.8600 / Loss: 45.4814\n",
      "Accuracy: 0.8600 / Loss: 45.4806\n",
      "Accuracy: 0.8600 / Loss: 45.4798\n",
      "Accuracy: 0.8600 / Loss: 45.4791\n",
      "Accuracy: 0.8600 / Loss: 45.4783\n",
      "Accuracy: 0.8600 / Loss: 45.4775\n",
      "Accuracy: 0.8600 / Loss: 45.4767\n",
      "Accuracy: 0.8600 / Loss: 45.4759\n",
      "Accuracy: 0.8600 / Loss: 45.4752\n",
      "Accuracy: 0.8600 / Loss: 45.4744\n",
      "Accuracy: 0.8600 / Loss: 45.4736\n",
      "Accuracy: 0.8600 / Loss: 45.4728\n",
      "Accuracy: 0.8600 / Loss: 45.4720\n",
      "Accuracy: 0.8600 / Loss: 45.4712\n",
      "Accuracy: 0.8600 / Loss: 45.4704\n",
      "Accuracy: 0.8600 / Loss: 45.4696\n",
      "Accuracy: 0.8600 / Loss: 45.4687\n",
      "Accuracy: 0.8600 / Loss: 45.4679\n",
      "Accuracy: 0.8600 / Loss: 45.4671\n",
      "Accuracy: 0.8600 / Loss: 45.4663\n",
      "Accuracy: 0.8600 / Loss: 45.4655\n",
      "Accuracy: 0.8600 / Loss: 45.4646\n",
      "Accuracy: 0.8600 / Loss: 45.4638\n",
      "Accuracy: 0.8600 / Loss: 45.4630\n",
      "Accuracy: 0.8600 / Loss: 45.4622\n",
      "Accuracy: 0.8600 / Loss: 45.4613\n",
      "Accuracy: 0.8600 / Loss: 45.4605\n",
      "Accuracy: 0.8600 / Loss: 45.4596\n",
      "Accuracy: 0.8600 / Loss: 45.4588\n",
      "Accuracy: 0.8600 / Loss: 45.4579\n",
      "Accuracy: 0.8600 / Loss: 45.4571\n",
      "Accuracy: 0.8600 / Loss: 45.4562\n",
      "Accuracy: 0.8600 / Loss: 45.4554\n",
      "Accuracy: 0.8600 / Loss: 45.4545\n",
      "Accuracy: 0.8600 / Loss: 45.4536\n",
      "Accuracy: 0.8600 / Loss: 45.4528\n",
      "Accuracy: 0.8600 / Loss: 45.4519\n",
      "Accuracy: 0.8600 / Loss: 45.4510\n",
      "Accuracy: 0.8600 / Loss: 45.4502\n",
      "Accuracy: 0.8600 / Loss: 45.4493\n",
      "Accuracy: 0.8600 / Loss: 45.4484\n",
      "Accuracy: 0.8600 / Loss: 45.4475\n",
      "Accuracy: 0.8600 / Loss: 45.4466\n",
      "Accuracy: 0.8600 / Loss: 45.4457\n",
      "Accuracy: 0.8600 / Loss: 45.4448\n",
      "Accuracy: 0.8600 / Loss: 45.4440\n",
      "Accuracy: 0.8600 / Loss: 45.4431\n",
      "Accuracy: 0.8600 / Loss: 45.4422\n",
      "Accuracy: 0.8600 / Loss: 45.4413\n",
      "Accuracy: 0.8600 / Loss: 45.4404\n",
      "Accuracy: 0.8600 / Loss: 45.4394\n",
      "Accuracy: 0.8600 / Loss: 45.4385\n",
      "Accuracy: 0.8600 / Loss: 45.4376\n",
      "Accuracy: 0.8600 / Loss: 45.4367\n",
      "Accuracy: 0.8600 / Loss: 45.4358\n",
      "Accuracy: 0.8600 / Loss: 45.4349\n",
      "Accuracy: 0.8600 / Loss: 45.4340\n",
      "Accuracy: 0.8600 / Loss: 45.4330\n",
      "Accuracy: 0.8600 / Loss: 45.4321\n",
      "Accuracy: 0.8600 / Loss: 45.4312\n",
      "Accuracy: 0.8600 / Loss: 45.4303\n",
      "Accuracy: 0.8600 / Loss: 45.4293\n",
      "Accuracy: 0.8600 / Loss: 45.4284\n",
      "Accuracy: 0.8600 / Loss: 45.4275\n",
      "Accuracy: 0.8600 / Loss: 45.4265\n",
      "Accuracy: 0.8600 / Loss: 45.4256\n",
      "Accuracy: 0.8600 / Loss: 45.4246\n",
      "Accuracy: 0.8600 / Loss: 45.4237\n",
      "Accuracy: 0.8600 / Loss: 45.4228\n",
      "Accuracy: 0.8600 / Loss: 45.4218\n",
      "Accuracy: 0.8600 / Loss: 45.4209\n",
      "Accuracy: 0.8600 / Loss: 45.4199\n",
      "Accuracy: 0.8600 / Loss: 45.4190\n",
      "Accuracy: 0.8600 / Loss: 45.4180\n",
      "Accuracy: 0.8600 / Loss: 45.4171\n",
      "Accuracy: 0.8600 / Loss: 45.4161\n",
      "Accuracy: 0.8600 / Loss: 45.4151\n",
      "Accuracy: 0.8600 / Loss: 45.4142\n",
      "Accuracy: 0.8600 / Loss: 45.4132\n",
      "Accuracy: 0.8600 / Loss: 45.4123\n",
      "Accuracy: 0.8600 / Loss: 45.4113\n",
      "Accuracy: 0.8600 / Loss: 45.4104\n",
      "Accuracy: 0.8600 / Loss: 45.4094\n",
      "Accuracy: 0.8600 / Loss: 45.4084\n",
      "Accuracy: 0.8600 / Loss: 45.4075\n",
      "Accuracy: 0.8600 / Loss: 45.4065\n",
      "Accuracy: 0.8600 / Loss: 45.4055\n",
      "Accuracy: 0.8600 / Loss: 45.4046\n",
      "Accuracy: 0.8600 / Loss: 45.4036\n",
      "Accuracy: 0.8600 / Loss: 45.4026\n",
      "Accuracy: 0.8600 / Loss: 45.4017\n",
      "Accuracy: 0.8600 / Loss: 45.4007\n",
      "Accuracy: 0.8600 / Loss: 45.3997\n",
      "Accuracy: 0.8600 / Loss: 45.3988\n",
      "Accuracy: 0.8600 / Loss: 45.3978\n",
      "Accuracy: 0.8600 / Loss: 45.3968\n",
      "Accuracy: 0.8600 / Loss: 45.3959\n",
      "Accuracy: 0.8600 / Loss: 45.3949\n",
      "Accuracy: 0.8600 / Loss: 45.3939\n",
      "Accuracy: 0.8600 / Loss: 45.3930\n",
      "Accuracy: 0.8600 / Loss: 45.3920\n",
      "Accuracy: 0.8600 / Loss: 45.3910\n",
      "Accuracy: 0.8600 / Loss: 45.3901\n",
      "Accuracy: 0.8600 / Loss: 45.3891\n",
      "Accuracy: 0.8600 / Loss: 45.3881\n",
      "Accuracy: 0.8600 / Loss: 45.3872\n",
      "Accuracy: 0.8600 / Loss: 45.3862\n",
      "Accuracy: 0.8600 / Loss: 45.3853\n",
      "Accuracy: 0.8600 / Loss: 45.3843\n",
      "Accuracy: 0.8600 / Loss: 45.3833\n",
      "Accuracy: 0.8600 / Loss: 45.3824\n",
      "Accuracy: 0.8600 / Loss: 45.3814\n",
      "Accuracy: 0.8600 / Loss: 45.3805\n",
      "Accuracy: 0.8600 / Loss: 45.3795\n",
      "Accuracy: 0.8600 / Loss: 45.3786\n",
      "Accuracy: 0.8600 / Loss: 45.3776\n",
      "Accuracy: 0.8600 / Loss: 45.3767\n",
      "Accuracy: 0.8600 / Loss: 45.3757\n",
      "Accuracy: 0.8600 / Loss: 45.3748\n",
      "Accuracy: 0.8600 / Loss: 45.3738\n",
      "Accuracy: 0.8600 / Loss: 45.3729\n",
      "Accuracy: 0.8600 / Loss: 45.3719\n",
      "Accuracy: 0.8600 / Loss: 45.3710\n",
      "Accuracy: 0.8600 / Loss: 45.3700\n",
      "Accuracy: 0.8600 / Loss: 45.3691\n",
      "Accuracy: 0.8600 / Loss: 45.3682\n",
      "Accuracy: 0.8600 / Loss: 45.3672\n",
      "Accuracy: 0.8600 / Loss: 45.3663\n",
      "Accuracy: 0.8600 / Loss: 45.3654\n",
      "Accuracy: 0.8600 / Loss: 45.3645\n",
      "Accuracy: 0.8600 / Loss: 45.3635\n",
      "Accuracy: 0.8600 / Loss: 45.3626\n",
      "Accuracy: 0.8600 / Loss: 45.3617\n",
      "Accuracy: 0.8600 / Loss: 45.3608\n",
      "Accuracy: 0.8600 / Loss: 45.3599\n",
      "Accuracy: 0.8600 / Loss: 45.3589\n",
      "Accuracy: 0.8600 / Loss: 45.3580\n",
      "Accuracy: 0.8600 / Loss: 45.3571\n",
      "Accuracy: 0.8600 / Loss: 45.3562\n",
      "Accuracy: 0.8600 / Loss: 45.3553\n",
      "Accuracy: 0.8600 / Loss: 45.3544\n",
      "Accuracy: 0.8600 / Loss: 45.3535\n",
      "Accuracy: 0.8600 / Loss: 45.3527\n",
      "Accuracy: 0.8600 / Loss: 45.3518\n",
      "Accuracy: 0.8600 / Loss: 45.3509\n",
      "Accuracy: 0.8600 / Loss: 45.3500\n",
      "Accuracy: 0.8600 / Loss: 45.3491\n",
      "Accuracy: 0.8600 / Loss: 45.3482\n",
      "Accuracy: 0.8600 / Loss: 45.3474\n",
      "Accuracy: 0.8600 / Loss: 45.3465\n",
      "Accuracy: 0.8600 / Loss: 45.3456\n",
      "Accuracy: 0.8600 / Loss: 45.3448\n",
      "Accuracy: 0.8600 / Loss: 45.3439\n",
      "Accuracy: 0.8600 / Loss: 45.3431\n",
      "Accuracy: 0.8600 / Loss: 45.3422\n",
      "Accuracy: 0.8600 / Loss: 45.3414\n",
      "Accuracy: 0.8600 / Loss: 45.3405\n",
      "Accuracy: 0.8600 / Loss: 45.3397\n",
      "Accuracy: 0.8600 / Loss: 45.3389\n",
      "Accuracy: 0.8600 / Loss: 45.3380\n",
      "Accuracy: 0.8600 / Loss: 45.3372\n",
      "Accuracy: 0.8600 / Loss: 45.3364\n",
      "Accuracy: 0.8600 / Loss: 45.3356\n",
      "Accuracy: 0.8600 / Loss: 45.3347\n",
      "Accuracy: 0.8600 / Loss: 45.3339\n",
      "Accuracy: 0.8600 / Loss: 45.3331\n",
      "Accuracy: 0.8600 / Loss: 45.3323\n",
      "Accuracy: 0.8600 / Loss: 45.3315\n",
      "Accuracy: 0.8600 / Loss: 45.3307\n",
      "Accuracy: 0.8600 / Loss: 45.3299\n",
      "Accuracy: 0.8600 / Loss: 45.3291\n",
      "Accuracy: 0.8600 / Loss: 45.3284\n",
      "Accuracy: 0.8600 / Loss: 45.3276\n",
      "Accuracy: 0.8600 / Loss: 45.3268\n",
      "Accuracy: 0.8600 / Loss: 45.3260\n",
      "Accuracy: 0.8600 / Loss: 45.3253\n",
      "Accuracy: 0.8600 / Loss: 45.3245\n",
      "Accuracy: 0.8600 / Loss: 45.3237\n",
      "Accuracy: 0.8600 / Loss: 45.3230\n",
      "Accuracy: 0.8600 / Loss: 45.3222\n",
      "Accuracy: 0.8600 / Loss: 45.3215\n",
      "Accuracy: 0.8600 / Loss: 45.3208\n",
      "Accuracy: 0.8600 / Loss: 45.3200\n",
      "Accuracy: 0.8600 / Loss: 45.3193\n",
      "Accuracy: 0.8600 / Loss: 45.3186\n",
      "Accuracy: 0.8600 / Loss: 45.3178\n",
      "Accuracy: 0.8600 / Loss: 45.3171\n",
      "Accuracy: 0.8600 / Loss: 45.3164\n",
      "Accuracy: 0.8600 / Loss: 45.3157\n",
      "Accuracy: 0.8600 / Loss: 45.3150\n",
      "Accuracy: 0.8600 / Loss: 45.3143\n",
      "Accuracy: 0.8600 / Loss: 45.3136\n",
      "Accuracy: 0.8600 / Loss: 45.3129\n",
      "Accuracy: 0.8600 / Loss: 45.3122\n",
      "Accuracy: 0.8600 / Loss: 45.3115\n",
      "Accuracy: 0.8600 / Loss: 45.3109\n",
      "Accuracy: 0.8600 / Loss: 45.3102\n",
      "Accuracy: 0.8600 / Loss: 45.3095\n",
      "Accuracy: 0.8600 / Loss: 45.3088\n",
      "Accuracy: 0.8600 / Loss: 45.3082\n",
      "Accuracy: 0.8600 / Loss: 45.3075\n",
      "Accuracy: 0.8600 / Loss: 45.3069\n",
      "Accuracy: 0.8600 / Loss: 45.3062\n",
      "Accuracy: 0.8600 / Loss: 45.3056\n",
      "Accuracy: 0.8600 / Loss: 45.3049\n",
      "Accuracy: 0.8600 / Loss: 45.3043\n",
      "Accuracy: 0.8600 / Loss: 45.3037\n",
      "Accuracy: 0.8600 / Loss: 45.3031\n",
      "Accuracy: 0.8600 / Loss: 45.3024\n",
      "Accuracy: 0.8600 / Loss: 45.3018\n",
      "Accuracy: 0.8600 / Loss: 45.3012\n",
      "Accuracy: 0.8600 / Loss: 45.3006\n",
      "Accuracy: 0.8600 / Loss: 45.3000\n",
      "Accuracy: 0.8600 / Loss: 45.2994\n",
      "Accuracy: 0.8600 / Loss: 45.2988\n",
      "Accuracy: 0.8600 / Loss: 45.2982\n",
      "Accuracy: 0.8600 / Loss: 45.2976\n",
      "Accuracy: 0.8600 / Loss: 45.2970\n",
      "Accuracy: 0.8600 / Loss: 45.2964\n",
      "Accuracy: 0.8600 / Loss: 45.2959\n",
      "Accuracy: 0.8600 / Loss: 45.2953\n",
      "Accuracy: 0.8600 / Loss: 45.2947\n",
      "Accuracy: 0.8600 / Loss: 45.2942\n",
      "Accuracy: 0.8600 / Loss: 45.2936\n",
      "Accuracy: 0.8600 / Loss: 45.2931\n",
      "Accuracy: 0.8600 / Loss: 45.2925\n",
      "Accuracy: 0.8600 / Loss: 45.2920\n",
      "Accuracy: 0.8600 / Loss: 45.2914\n",
      "Accuracy: 0.8600 / Loss: 45.2909\n",
      "Accuracy: 0.8600 / Loss: 45.2904\n",
      "Accuracy: 0.8600 / Loss: 45.2898\n",
      "Accuracy: 0.8600 / Loss: 45.2893\n",
      "Accuracy: 0.8600 / Loss: 45.2888\n",
      "Accuracy: 0.8600 / Loss: 45.2883\n",
      "Accuracy: 0.8600 / Loss: 45.2877\n",
      "Accuracy: 0.8600 / Loss: 45.2872\n",
      "Accuracy: 0.8600 / Loss: 45.2867\n",
      "Accuracy: 0.8600 / Loss: 45.2862\n",
      "Accuracy: 0.8600 / Loss: 45.2857\n",
      "Accuracy: 0.8600 / Loss: 45.2852\n",
      "Accuracy: 0.8600 / Loss: 45.2847\n",
      "Accuracy: 0.8600 / Loss: 45.2842\n",
      "Accuracy: 0.8600 / Loss: 45.2838\n",
      "Accuracy: 0.8600 / Loss: 45.2833\n",
      "Accuracy: 0.8600 / Loss: 45.2828\n",
      "Accuracy: 0.8600 / Loss: 45.2823\n",
      "Accuracy: 0.8600 / Loss: 45.2818\n",
      "Accuracy: 0.8600 / Loss: 45.2814\n",
      "Accuracy: 0.8600 / Loss: 45.2809\n",
      "Accuracy: 0.8600 / Loss: 45.2805\n",
      "Accuracy: 0.8600 / Loss: 45.2800\n",
      "Accuracy: 0.8600 / Loss: 45.2795\n",
      "Accuracy: 0.8600 / Loss: 45.2791\n",
      "Accuracy: 0.8600 / Loss: 45.2786\n",
      "Accuracy: 0.8600 / Loss: 45.2782\n",
      "Accuracy: 0.8600 / Loss: 45.2778\n",
      "Accuracy: 0.8600 / Loss: 45.2773\n",
      "Accuracy: 0.8600 / Loss: 45.2769\n",
      "Accuracy: 0.8600 / Loss: 45.2765\n",
      "Accuracy: 0.8600 / Loss: 45.2760\n",
      "Accuracy: 0.8600 / Loss: 45.2756\n",
      "Accuracy: 0.8600 / Loss: 45.2752\n",
      "Accuracy: 0.8600 / Loss: 45.2748\n",
      "Accuracy: 0.8600 / Loss: 45.2744\n",
      "Accuracy: 0.8600 / Loss: 45.2739\n",
      "Accuracy: 0.8600 / Loss: 45.2735\n",
      "Accuracy: 0.8600 / Loss: 45.2731\n",
      "Accuracy: 0.8600 / Loss: 45.2727\n",
      "Accuracy: 0.8600 / Loss: 45.2723\n",
      "Accuracy: 0.8600 / Loss: 45.2719\n",
      "Accuracy: 0.8600 / Loss: 45.2715\n",
      "Accuracy: 0.8600 / Loss: 45.2711\n",
      "Accuracy: 0.8600 / Loss: 45.2707\n",
      "Accuracy: 0.8600 / Loss: 45.2704\n",
      "Accuracy: 0.8600 / Loss: 45.2700\n",
      "Accuracy: 0.8600 / Loss: 45.2696\n",
      "Accuracy: 0.8600 / Loss: 45.2692\n",
      "Accuracy: 0.8600 / Loss: 45.2688\n",
      "Accuracy: 0.8600 / Loss: 45.2685\n",
      "Accuracy: 0.8600 / Loss: 45.2681\n",
      "Accuracy: 0.8600 / Loss: 45.2677\n",
      "Accuracy: 0.8600 / Loss: 45.2674\n",
      "Accuracy: 0.8600 / Loss: 45.2670\n",
      "Accuracy: 0.8600 / Loss: 45.2666\n",
      "Accuracy: 0.8600 / Loss: 45.2663\n",
      "Accuracy: 0.8600 / Loss: 45.2659\n",
      "Accuracy: 0.8600 / Loss: 45.2656\n",
      "Accuracy: 0.8600 / Loss: 45.2652\n",
      "Accuracy: 0.8600 / Loss: 45.2649\n",
      "Accuracy: 0.8600 / Loss: 45.2645\n",
      "Accuracy: 0.8600 / Loss: 45.2642\n",
      "Accuracy: 0.8600 / Loss: 45.2638\n",
      "Accuracy: 0.8600 / Loss: 45.2635\n",
      "Accuracy: 0.8600 / Loss: 45.2631\n",
      "Accuracy: 0.8600 / Loss: 45.2628\n",
      "Accuracy: 0.8600 / Loss: 45.2625\n",
      "Accuracy: 0.8600 / Loss: 45.2621\n",
      "Accuracy: 0.8600 / Loss: 45.2618\n",
      "Accuracy: 0.8600 / Loss: 45.2615\n",
      "Accuracy: 0.8600 / Loss: 45.2611\n",
      "Accuracy: 0.8600 / Loss: 45.2608\n",
      "Accuracy: 0.8600 / Loss: 45.2605\n",
      "Accuracy: 0.8600 / Loss: 45.2602\n",
      "Accuracy: 0.8600 / Loss: 45.2598\n",
      "Accuracy: 0.8600 / Loss: 45.2595\n",
      "Accuracy: 0.8600 / Loss: 45.2592\n",
      "Accuracy: 0.8600 / Loss: 45.2589\n",
      "Accuracy: 0.8600 / Loss: 45.2586\n",
      "Accuracy: 0.8600 / Loss: 45.2583\n",
      "Accuracy: 0.8600 / Loss: 45.2580\n",
      "Accuracy: 0.8600 / Loss: 45.2576\n",
      "Accuracy: 0.8600 / Loss: 45.2573\n",
      "Accuracy: 0.8600 / Loss: 45.2570\n",
      "Accuracy: 0.8600 / Loss: 45.2567\n",
      "Accuracy: 0.8600 / Loss: 45.2564\n",
      "Accuracy: 0.8600 / Loss: 45.2561\n",
      "Accuracy: 0.8600 / Loss: 45.2558\n",
      "Accuracy: 0.8600 / Loss: 45.2555\n",
      "Accuracy: 0.8600 / Loss: 45.2552\n",
      "Accuracy: 0.8600 / Loss: 45.2549\n",
      "Accuracy: 0.8600 / Loss: 45.2546\n",
      "Accuracy: 0.8600 / Loss: 45.2544\n",
      "Accuracy: 0.8600 / Loss: 45.2541\n",
      "Accuracy: 0.8600 / Loss: 45.2538\n",
      "Accuracy: 0.8600 / Loss: 45.2535\n",
      "Accuracy: 0.8600 / Loss: 45.2532\n",
      "Accuracy: 0.8600 / Loss: 45.2529\n",
      "Accuracy: 0.8600 / Loss: 45.2526\n",
      "Accuracy: 0.8600 / Loss: 45.2523\n",
      "Accuracy: 0.8600 / Loss: 45.2521\n",
      "Accuracy: 0.8600 / Loss: 45.2518\n",
      "Accuracy: 0.8600 / Loss: 45.2515\n",
      "Accuracy: 0.8600 / Loss: 45.2512\n",
      "Accuracy: 0.8600 / Loss: 45.2509\n",
      "Accuracy: 0.8600 / Loss: 45.2507\n",
      "Accuracy: 0.8600 / Loss: 45.2504\n",
      "Accuracy: 0.8600 / Loss: 45.2501\n",
      "Accuracy: 0.8600 / Loss: 45.2498\n",
      "Accuracy: 0.8600 / Loss: 45.2496\n",
      "Accuracy: 0.8600 / Loss: 45.2493\n",
      "Accuracy: 0.8600 / Loss: 45.2490\n",
      "Accuracy: 0.8600 / Loss: 45.2488\n",
      "Accuracy: 0.8600 / Loss: 45.2485\n",
      "Accuracy: 0.8600 / Loss: 45.2482\n",
      "Accuracy: 0.8600 / Loss: 45.2479\n",
      "Accuracy: 0.8600 / Loss: 45.2477\n",
      "Accuracy: 0.8600 / Loss: 45.2474\n",
      "Accuracy: 0.8600 / Loss: 45.2472\n",
      "Accuracy: 0.8600 / Loss: 45.2469\n",
      "Accuracy: 0.8600 / Loss: 45.2466\n",
      "Accuracy: 0.8600 / Loss: 45.2464\n",
      "Accuracy: 0.8600 / Loss: 45.2461\n",
      "Accuracy: 0.8600 / Loss: 45.2458\n",
      "Accuracy: 0.8600 / Loss: 45.2456\n",
      "Accuracy: 0.8600 / Loss: 45.2453\n",
      "Accuracy: 0.8600 / Loss: 45.2451\n",
      "Accuracy: 0.8600 / Loss: 45.2448\n",
      "Accuracy: 0.8600 / Loss: 45.2445\n",
      "Accuracy: 0.8600 / Loss: 45.2443\n",
      "Accuracy: 0.8600 / Loss: 45.2440\n",
      "Accuracy: 0.8600 / Loss: 45.2438\n",
      "Accuracy: 0.8600 / Loss: 45.2435\n",
      "Accuracy: 0.8600 / Loss: 45.2433\n",
      "Accuracy: 0.8600 / Loss: 45.2430\n",
      "Accuracy: 0.8600 / Loss: 45.2427\n",
      "Accuracy: 0.8600 / Loss: 45.2425\n",
      "Accuracy: 0.8600 / Loss: 45.2422\n",
      "Accuracy: 0.8600 / Loss: 45.2420\n",
      "Accuracy: 0.8600 / Loss: 45.2417\n",
      "Accuracy: 0.8600 / Loss: 45.2415\n",
      "Accuracy: 0.8600 / Loss: 45.2412\n",
      "Accuracy: 0.8600 / Loss: 45.2410\n",
      "Accuracy: 0.8600 / Loss: 45.2407\n",
      "Accuracy: 0.8600 / Loss: 45.2405\n",
      "Accuracy: 0.8600 / Loss: 45.2402\n",
      "Accuracy: 0.8600 / Loss: 45.2400\n",
      "Accuracy: 0.8600 / Loss: 45.2397\n",
      "Accuracy: 0.8600 / Loss: 45.2395\n",
      "Accuracy: 0.8600 / Loss: 45.2392\n",
      "Accuracy: 0.8600 / Loss: 45.2390\n",
      "Accuracy: 0.8600 / Loss: 45.2387\n",
      "Accuracy: 0.8600 / Loss: 45.2385\n",
      "Accuracy: 0.8600 / Loss: 45.2382\n",
      "Accuracy: 0.8600 / Loss: 45.2380\n",
      "Accuracy: 0.8600 / Loss: 45.2377\n",
      "Accuracy: 0.8600 / Loss: 45.2375\n",
      "Accuracy: 0.8600 / Loss: 45.2372\n",
      "Accuracy: 0.8600 / Loss: 45.2370\n",
      "Accuracy: 0.8600 / Loss: 45.2367\n",
      "Accuracy: 0.8600 / Loss: 45.2365\n",
      "Accuracy: 0.8600 / Loss: 45.2362\n",
      "Accuracy: 0.8600 / Loss: 45.2360\n",
      "Accuracy: 0.8600 / Loss: 45.2357\n",
      "Accuracy: 0.8600 / Loss: 45.2355\n",
      "Accuracy: 0.8600 / Loss: 45.2353\n",
      "Accuracy: 0.8600 / Loss: 45.2350\n",
      "Accuracy: 0.8600 / Loss: 45.2348\n",
      "Accuracy: 0.8600 / Loss: 45.2345\n",
      "Accuracy: 0.8600 / Loss: 45.2343\n",
      "Accuracy: 0.8600 / Loss: 45.2340\n",
      "Accuracy: 0.8600 / Loss: 45.2338\n",
      "Accuracy: 0.8600 / Loss: 45.2335\n",
      "Accuracy: 0.8600 / Loss: 45.2333\n",
      "Accuracy: 0.8600 / Loss: 45.2330\n",
      "Accuracy: 0.8600 / Loss: 45.2328\n",
      "Accuracy: 0.8600 / Loss: 45.2325\n",
      "Accuracy: 0.8600 / Loss: 45.2323\n",
      "Accuracy: 0.8600 / Loss: 45.2320\n",
      "Accuracy: 0.8600 / Loss: 45.2318\n",
      "Accuracy: 0.8600 / Loss: 45.2316\n",
      "Accuracy: 0.8600 / Loss: 45.2313\n",
      "Accuracy: 0.8600 / Loss: 45.2311\n",
      "Accuracy: 0.8600 / Loss: 45.2308\n",
      "Accuracy: 0.8600 / Loss: 45.2306\n",
      "Accuracy: 0.8600 / Loss: 45.2303\n",
      "Accuracy: 0.8600 / Loss: 45.2301\n",
      "Accuracy: 0.8600 / Loss: 45.2298\n",
      "Accuracy: 0.8600 / Loss: 45.2296\n",
      "Accuracy: 0.8600 / Loss: 45.2293\n",
      "Accuracy: 0.8600 / Loss: 45.2291\n",
      "Accuracy: 0.8600 / Loss: 45.2288\n",
      "Accuracy: 0.8600 / Loss: 45.2286\n",
      "Accuracy: 0.8600 / Loss: 45.2283\n",
      "Accuracy: 0.8600 / Loss: 45.2281\n",
      "Accuracy: 0.8600 / Loss: 45.2278\n",
      "Accuracy: 0.8600 / Loss: 45.2276\n",
      "Accuracy: 0.8600 / Loss: 45.2273\n",
      "Accuracy: 0.8600 / Loss: 45.2271\n",
      "Accuracy: 0.8600 / Loss: 45.2268\n",
      "Accuracy: 0.8600 / Loss: 45.2266\n",
      "Accuracy: 0.8600 / Loss: 45.2263\n",
      "Accuracy: 0.8600 / Loss: 45.2261\n",
      "Accuracy: 0.8600 / Loss: 45.2258\n",
      "Accuracy: 0.8600 / Loss: 45.2256\n",
      "Accuracy: 0.8600 / Loss: 45.2253\n",
      "Accuracy: 0.8600 / Loss: 45.2251\n",
      "Accuracy: 0.8600 / Loss: 45.2248\n",
      "Accuracy: 0.8600 / Loss: 45.2246\n",
      "Accuracy: 0.8600 / Loss: 45.2243\n",
      "Accuracy: 0.8600 / Loss: 45.2241\n",
      "Accuracy: 0.8600 / Loss: 45.2238\n",
      "Accuracy: 0.8600 / Loss: 45.2236\n",
      "Accuracy: 0.8600 / Loss: 45.2233\n",
      "Accuracy: 0.8600 / Loss: 45.2231\n",
      "Accuracy: 0.8600 / Loss: 45.2228\n",
      "Accuracy: 0.8600 / Loss: 45.2225\n",
      "Accuracy: 0.8600 / Loss: 45.2223\n",
      "Accuracy: 0.8600 / Loss: 45.2220\n",
      "Accuracy: 0.8600 / Loss: 45.2218\n",
      "Accuracy: 0.8600 / Loss: 45.2215\n",
      "Accuracy: 0.8600 / Loss: 45.2213\n",
      "Accuracy: 0.8600 / Loss: 45.2210\n",
      "Accuracy: 0.8600 / Loss: 45.2208\n",
      "Accuracy: 0.8600 / Loss: 45.2205\n",
      "Accuracy: 0.8600 / Loss: 45.2202\n",
      "Accuracy: 0.8600 / Loss: 45.2200\n",
      "Accuracy: 0.8600 / Loss: 45.2197\n",
      "Accuracy: 0.8600 / Loss: 45.2195\n",
      "Accuracy: 0.8600 / Loss: 45.2192\n",
      "Accuracy: 0.8600 / Loss: 45.2189\n",
      "Accuracy: 0.8600 / Loss: 45.2187\n",
      "Accuracy: 0.8600 / Loss: 45.2184\n",
      "Accuracy: 0.8600 / Loss: 45.2182\n",
      "Accuracy: 0.8600 / Loss: 45.2179\n",
      "Accuracy: 0.8600 / Loss: 45.2176\n",
      "Accuracy: 0.8600 / Loss: 45.2174\n",
      "Accuracy: 0.8600 / Loss: 45.2171\n",
      "Accuracy: 0.8600 / Loss: 45.2168\n",
      "Accuracy: 0.8600 / Loss: 45.2166\n",
      "Accuracy: 0.8600 / Loss: 45.2163\n",
      "Accuracy: 0.8600 / Loss: 45.2161\n",
      "Accuracy: 0.8600 / Loss: 45.2158\n",
      "Accuracy: 0.8600 / Loss: 45.2155\n",
      "Accuracy: 0.8600 / Loss: 45.2153\n",
      "Accuracy: 0.8600 / Loss: 45.2150\n",
      "Accuracy: 0.8600 / Loss: 45.2147\n",
      "Accuracy: 0.8500 / Loss: 45.2145\n",
      "Accuracy: 0.8500 / Loss: 45.2142\n",
      "Accuracy: 0.8500 / Loss: 45.2139\n",
      "Accuracy: 0.8500 / Loss: 45.2137\n",
      "Accuracy: 0.8500 / Loss: 45.2134\n",
      "Accuracy: 0.8500 / Loss: 45.2131\n",
      "Accuracy: 0.8500 / Loss: 45.2129\n",
      "Accuracy: 0.8500 / Loss: 45.2126\n",
      "Accuracy: 0.8500 / Loss: 45.2123\n",
      "Accuracy: 0.8500 / Loss: 45.2120\n",
      "Accuracy: 0.8500 / Loss: 45.2118\n",
      "Accuracy: 0.8500 / Loss: 45.2115\n",
      "Accuracy: 0.8500 / Loss: 45.2112\n",
      "Accuracy: 0.8500 / Loss: 45.2110\n",
      "Accuracy: 0.8500 / Loss: 45.2107\n",
      "Accuracy: 0.8500 / Loss: 45.2104\n",
      "Accuracy: 0.8500 / Loss: 45.2101\n",
      "Accuracy: 0.8500 / Loss: 45.2099\n",
      "Accuracy: 0.8500 / Loss: 45.2096\n",
      "Accuracy: 0.8500 / Loss: 45.2093\n",
      "Accuracy: 0.8500 / Loss: 45.2090\n",
      "Accuracy: 0.8500 / Loss: 45.2088\n",
      "Accuracy: 0.8500 / Loss: 45.2085\n",
      "Accuracy: 0.8500 / Loss: 45.2082\n",
      "Accuracy: 0.8500 / Loss: 45.2079\n",
      "Accuracy: 0.8500 / Loss: 45.2077\n",
      "Accuracy: 0.8500 / Loss: 45.2074\n",
      "Accuracy: 0.8500 / Loss: 45.2071\n",
      "Accuracy: 0.8500 / Loss: 45.2068\n",
      "Accuracy: 0.8500 / Loss: 45.2065\n",
      "Accuracy: 0.8500 / Loss: 45.2063\n",
      "Accuracy: 0.8500 / Loss: 45.2060\n",
      "Accuracy: 0.8500 / Loss: 45.2057\n",
      "Accuracy: 0.8500 / Loss: 45.2054\n",
      "Accuracy: 0.8500 / Loss: 45.2051\n",
      "Accuracy: 0.8500 / Loss: 45.2049\n",
      "Accuracy: 0.8500 / Loss: 45.2046\n",
      "Accuracy: 0.8500 / Loss: 45.2043\n",
      "Accuracy: 0.8500 / Loss: 45.2040\n",
      "Accuracy: 0.8500 / Loss: 45.2037\n",
      "Accuracy: 0.8500 / Loss: 45.2034\n",
      "Accuracy: 0.8500 / Loss: 45.2031\n",
      "Accuracy: 0.8500 / Loss: 45.2029\n",
      "Accuracy: 0.8500 / Loss: 45.2026\n",
      "Accuracy: 0.8500 / Loss: 45.2023\n",
      "Accuracy: 0.8500 / Loss: 45.2020\n",
      "Accuracy: 0.8500 / Loss: 45.2017\n",
      "Accuracy: 0.8500 / Loss: 45.2014\n",
      "Accuracy: 0.8500 / Loss: 45.2011\n",
      "Accuracy: 0.8500 / Loss: 45.2009\n",
      "Accuracy: 0.8500 / Loss: 45.2006\n",
      "Accuracy: 0.8500 / Loss: 45.2003\n",
      "Accuracy: 0.8500 / Loss: 45.2000\n",
      "Accuracy: 0.8500 / Loss: 45.1997\n",
      "Accuracy: 0.8500 / Loss: 45.1994\n",
      "Accuracy: 0.8500 / Loss: 45.1991\n",
      "Accuracy: 0.8500 / Loss: 45.1988\n",
      "Accuracy: 0.8500 / Loss: 45.1985\n",
      "Accuracy: 0.8500 / Loss: 45.1982\n",
      "Accuracy: 0.8500 / Loss: 45.1979\n",
      "Accuracy: 0.8500 / Loss: 45.1976\n",
      "Accuracy: 0.8500 / Loss: 45.1973\n",
      "Accuracy: 0.8500 / Loss: 45.1971\n",
      "Accuracy: 0.8500 / Loss: 45.1968\n",
      "Accuracy: 0.8500 / Loss: 45.1965\n",
      "Accuracy: 0.8500 / Loss: 45.1962\n",
      "Accuracy: 0.8500 / Loss: 45.1959\n",
      "Accuracy: 0.8500 / Loss: 45.1956\n",
      "Accuracy: 0.8500 / Loss: 45.1953\n",
      "Accuracy: 0.8500 / Loss: 45.1950\n",
      "Accuracy: 0.8500 / Loss: 45.1947\n",
      "Accuracy: 0.8500 / Loss: 45.1944\n",
      "Accuracy: 0.8500 / Loss: 45.1941\n",
      "Accuracy: 0.8500 / Loss: 45.1938\n",
      "Accuracy: 0.8500 / Loss: 45.1935\n",
      "Accuracy: 0.8500 / Loss: 45.1932\n",
      "Accuracy: 0.8500 / Loss: 45.1929\n",
      "Accuracy: 0.8500 / Loss: 45.1926\n",
      "Accuracy: 0.8500 / Loss: 45.1923\n",
      "Accuracy: 0.8500 / Loss: 45.1920\n",
      "Accuracy: 0.8500 / Loss: 45.1917\n",
      "Accuracy: 0.8500 / Loss: 45.1914\n",
      "Accuracy: 0.8500 / Loss: 45.1911\n",
      "Accuracy: 0.8500 / Loss: 45.1908\n",
      "Accuracy: 0.8500 / Loss: 45.1905\n",
      "Accuracy: 0.8500 / Loss: 45.1902\n",
      "Accuracy: 0.8500 / Loss: 45.1899\n",
      "Accuracy: 0.8500 / Loss: 45.1895\n",
      "Accuracy: 0.8500 / Loss: 45.1892\n",
      "Accuracy: 0.8500 / Loss: 45.1889\n",
      "Accuracy: 0.8500 / Loss: 45.1886\n",
      "Accuracy: 0.8500 / Loss: 45.1883\n",
      "Accuracy: 0.8500 / Loss: 45.1880\n",
      "Accuracy: 0.8500 / Loss: 45.1877\n",
      "Accuracy: 0.8500 / Loss: 45.1874\n",
      "Accuracy: 0.8500 / Loss: 45.1871\n",
      "Accuracy: 0.8500 / Loss: 45.1868\n",
      "Accuracy: 0.8500 / Loss: 45.1865\n",
      "Accuracy: 0.8500 / Loss: 45.1862\n",
      "Accuracy: 0.8500 / Loss: 45.1859\n",
      "Accuracy: 0.8500 / Loss: 45.1855\n",
      "Accuracy: 0.8500 / Loss: 45.1852\n",
      "Accuracy: 0.8500 / Loss: 45.1849\n",
      "Accuracy: 0.8500 / Loss: 45.1846\n",
      "Accuracy: 0.8500 / Loss: 45.1843\n",
      "Accuracy: 0.8500 / Loss: 45.1840\n",
      "Accuracy: 0.8500 / Loss: 45.1837\n",
      "Accuracy: 0.8500 / Loss: 45.1834\n",
      "Accuracy: 0.8500 / Loss: 45.1831\n",
      "Accuracy: 0.8500 / Loss: 45.1827\n",
      "Accuracy: 0.8500 / Loss: 45.1824\n",
      "Accuracy: 0.8500 / Loss: 45.1821\n",
      "Accuracy: 0.8500 / Loss: 45.1818\n",
      "Accuracy: 0.8500 / Loss: 45.1815\n",
      "Accuracy: 0.8500 / Loss: 45.1812\n",
      "Accuracy: 0.8500 / Loss: 45.1809\n",
      "Accuracy: 0.8500 / Loss: 45.1805\n",
      "Accuracy: 0.8500 / Loss: 45.1802\n",
      "Accuracy: 0.8500 / Loss: 45.1799\n",
      "Accuracy: 0.8500 / Loss: 45.1796\n",
      "Accuracy: 0.8500 / Loss: 45.1793\n",
      "Accuracy: 0.8500 / Loss: 45.1790\n",
      "Accuracy: 0.8500 / Loss: 45.1786\n",
      "Accuracy: 0.8500 / Loss: 45.1783\n",
      "Accuracy: 0.8500 / Loss: 45.1780\n",
      "Accuracy: 0.8500 / Loss: 45.1777\n",
      "Accuracy: 0.8500 / Loss: 45.1774\n",
      "Accuracy: 0.8500 / Loss: 45.1771\n",
      "Accuracy: 0.8500 / Loss: 45.1767\n",
      "Accuracy: 0.8500 / Loss: 45.1764\n",
      "Accuracy: 0.8500 / Loss: 45.1761\n",
      "Accuracy: 0.8500 / Loss: 45.1758\n",
      "Accuracy: 0.8500 / Loss: 45.1755\n",
      "Accuracy: 0.8500 / Loss: 45.1751\n",
      "Accuracy: 0.8500 / Loss: 45.1748\n",
      "Accuracy: 0.8500 / Loss: 45.1745\n",
      "Accuracy: 0.8500 / Loss: 45.1742\n",
      "Accuracy: 0.8500 / Loss: 45.1738\n",
      "Accuracy: 0.8500 / Loss: 45.1735\n",
      "Accuracy: 0.8500 / Loss: 45.1732\n",
      "Accuracy: 0.8500 / Loss: 45.1729\n",
      "Accuracy: 0.8500 / Loss: 45.1726\n",
      "Accuracy: 0.8500 / Loss: 45.1722\n",
      "Accuracy: 0.8500 / Loss: 45.1719\n",
      "Accuracy: 0.8500 / Loss: 45.1716\n",
      "Accuracy: 0.8500 / Loss: 45.1713\n",
      "Accuracy: 0.8500 / Loss: 45.1709\n",
      "Accuracy: 0.8500 / Loss: 45.1706\n",
      "Accuracy: 0.8500 / Loss: 45.1703\n",
      "Accuracy: 0.8500 / Loss: 45.1700\n",
      "Accuracy: 0.8500 / Loss: 45.1696\n",
      "Accuracy: 0.8500 / Loss: 45.1693\n",
      "Accuracy: 0.8500 / Loss: 45.1690\n",
      "Accuracy: 0.8500 / Loss: 45.1687\n",
      "Accuracy: 0.8500 / Loss: 45.1683\n",
      "Accuracy: 0.8500 / Loss: 45.1680\n",
      "Accuracy: 0.8500 / Loss: 45.1677\n",
      "Accuracy: 0.8500 / Loss: 45.1674\n",
      "Accuracy: 0.8500 / Loss: 45.1670\n",
      "Accuracy: 0.8500 / Loss: 45.1667\n",
      "Accuracy: 0.8500 / Loss: 45.1664\n",
      "Accuracy: 0.8500 / Loss: 45.1661\n",
      "Accuracy: 0.8500 / Loss: 45.1657\n",
      "Accuracy: 0.8500 / Loss: 45.1654\n",
      "Accuracy: 0.8500 / Loss: 45.1651\n",
      "Accuracy: 0.8500 / Loss: 45.1647\n",
      "Accuracy: 0.8500 / Loss: 45.1644\n",
      "Accuracy: 0.8500 / Loss: 45.1641\n",
      "Accuracy: 0.8500 / Loss: 45.1638\n",
      "Accuracy: 0.8500 / Loss: 45.1634\n",
      "Accuracy: 0.8500 / Loss: 45.1631\n",
      "Accuracy: 0.8500 / Loss: 45.1628\n",
      "Accuracy: 0.8500 / Loss: 45.1624\n",
      "Accuracy: 0.8500 / Loss: 45.1621\n",
      "Accuracy: 0.8500 / Loss: 45.1618\n",
      "Accuracy: 0.8500 / Loss: 45.1615\n",
      "Accuracy: 0.8500 / Loss: 45.1611\n",
      "Accuracy: 0.8500 / Loss: 45.1608\n",
      "Accuracy: 0.8500 / Loss: 45.1605\n",
      "Accuracy: 0.8500 / Loss: 45.1601\n",
      "Accuracy: 0.8500 / Loss: 45.1598\n",
      "Accuracy: 0.8500 / Loss: 45.1595\n",
      "Accuracy: 0.8500 / Loss: 45.1591\n",
      "Accuracy: 0.8500 / Loss: 45.1588\n",
      "Accuracy: 0.8500 / Loss: 45.1585\n",
      "Accuracy: 0.8500 / Loss: 45.1582\n",
      "Accuracy: 0.8500 / Loss: 45.1578\n",
      "Accuracy: 0.8500 / Loss: 45.1575\n",
      "Accuracy: 0.8500 / Loss: 45.1572\n",
      "Accuracy: 0.8500 / Loss: 45.1568\n",
      "Accuracy: 0.8500 / Loss: 45.1565\n",
      "Accuracy: 0.8500 / Loss: 45.1562\n",
      "Accuracy: 0.8500 / Loss: 45.1558\n",
      "Accuracy: 0.8500 / Loss: 45.1555\n",
      "Accuracy: 0.8500 / Loss: 45.1552\n",
      "Accuracy: 0.8500 / Loss: 45.1548\n",
      "Accuracy: 0.8500 / Loss: 45.1545\n",
      "Accuracy: 0.8500 / Loss: 45.1542\n",
      "Accuracy: 0.8500 / Loss: 45.1538\n",
      "Accuracy: 0.8500 / Loss: 45.1535\n",
      "Accuracy: 0.8500 / Loss: 45.1532\n",
      "Accuracy: 0.8500 / Loss: 45.1528\n",
      "Accuracy: 0.8500 / Loss: 45.1525\n",
      "Accuracy: 0.8500 / Loss: 45.1522\n",
      "Accuracy: 0.8500 / Loss: 45.1518\n",
      "Accuracy: 0.8500 / Loss: 45.1515\n",
      "Accuracy: 0.8500 / Loss: 45.1512\n",
      "Accuracy: 0.8500 / Loss: 45.1508\n",
      "Accuracy: 0.8500 / Loss: 45.1505\n",
      "Accuracy: 0.8500 / Loss: 45.1502\n",
      "Accuracy: 0.8500 / Loss: 45.1498\n",
      "Accuracy: 0.8500 / Loss: 45.1495\n",
      "Accuracy: 0.8500 / Loss: 45.1491\n",
      "Accuracy: 0.8500 / Loss: 45.1488\n",
      "Accuracy: 0.8500 / Loss: 45.1485\n",
      "Accuracy: 0.8500 / Loss: 45.1481\n",
      "Accuracy: 0.8500 / Loss: 45.1478\n",
      "Accuracy: 0.8500 / Loss: 45.1475\n",
      "Accuracy: 0.8500 / Loss: 45.1471\n",
      "Accuracy: 0.8500 / Loss: 45.1468\n",
      "Accuracy: 0.8500 / Loss: 45.1465\n",
      "Accuracy: 0.8500 / Loss: 45.1461\n",
      "Accuracy: 0.8500 / Loss: 45.1458\n",
      "Accuracy: 0.8500 / Loss: 45.1454\n",
      "Accuracy: 0.8500 / Loss: 45.1451\n",
      "Accuracy: 0.8500 / Loss: 45.1448\n",
      "Accuracy: 0.8500 / Loss: 45.1444\n",
      "Accuracy: 0.8500 / Loss: 45.1441\n",
      "Accuracy: 0.8500 / Loss: 45.1438\n",
      "Accuracy: 0.8500 / Loss: 45.1434\n",
      "Accuracy: 0.8500 / Loss: 45.1431\n",
      "Accuracy: 0.8500 / Loss: 45.1427\n",
      "Accuracy: 0.8500 / Loss: 45.1424\n",
      "Accuracy: 0.8500 / Loss: 45.1421\n",
      "Accuracy: 0.8500 / Loss: 45.1417\n",
      "Accuracy: 0.8500 / Loss: 45.1414\n",
      "Accuracy: 0.8500 / Loss: 45.1411\n",
      "Accuracy: 0.8500 / Loss: 45.1407\n",
      "Accuracy: 0.8500 / Loss: 45.1404\n",
      "Accuracy: 0.8500 / Loss: 45.1400\n",
      "Accuracy: 0.8500 / Loss: 45.1397\n",
      "Accuracy: 0.8500 / Loss: 45.1394\n",
      "Accuracy: 0.8500 / Loss: 45.1390\n",
      "Accuracy: 0.8500 / Loss: 45.1387\n",
      "Accuracy: 0.8500 / Loss: 45.1383\n",
      "Accuracy: 0.8500 / Loss: 45.1380\n",
      "Accuracy: 0.8500 / Loss: 45.1377\n",
      "Accuracy: 0.8500 / Loss: 45.1373\n",
      "Accuracy: 0.8500 / Loss: 45.1370\n",
      "Accuracy: 0.8500 / Loss: 45.1366\n",
      "Accuracy: 0.8500 / Loss: 45.1363\n",
      "Accuracy: 0.8500 / Loss: 45.1360\n",
      "Accuracy: 0.8500 / Loss: 45.1356\n",
      "Accuracy: 0.8500 / Loss: 45.1353\n",
      "Accuracy: 0.8500 / Loss: 45.1349\n",
      "Accuracy: 0.8500 / Loss: 45.1346\n",
      "Accuracy: 0.8500 / Loss: 45.1343\n",
      "Accuracy: 0.8500 / Loss: 45.1339\n",
      "Accuracy: 0.8500 / Loss: 45.1336\n",
      "Accuracy: 0.8500 / Loss: 45.1332\n",
      "Accuracy: 0.8500 / Loss: 45.1329\n",
      "Accuracy: 0.8500 / Loss: 45.1325\n",
      "Accuracy: 0.8500 / Loss: 45.1322\n",
      "Accuracy: 0.8500 / Loss: 45.1319\n",
      "Accuracy: 0.8500 / Loss: 45.1315\n",
      "Accuracy: 0.8500 / Loss: 45.1312\n",
      "Accuracy: 0.8500 / Loss: 45.1308\n",
      "Accuracy: 0.8500 / Loss: 45.1305\n",
      "Accuracy: 0.8500 / Loss: 45.1301\n",
      "Accuracy: 0.8500 / Loss: 45.1298\n",
      "Accuracy: 0.8500 / Loss: 45.1295\n",
      "Accuracy: 0.8500 / Loss: 45.1291\n",
      "Accuracy: 0.8500 / Loss: 45.1288\n",
      "Accuracy: 0.8500 / Loss: 45.1284\n",
      "Accuracy: 0.8500 / Loss: 45.1281\n",
      "Accuracy: 0.8500 / Loss: 45.1277\n",
      "Accuracy: 0.8500 / Loss: 45.1274\n",
      "Accuracy: 0.8500 / Loss: 45.1270\n",
      "Accuracy: 0.8500 / Loss: 45.1267\n",
      "Accuracy: 0.8500 / Loss: 45.1264\n",
      "Accuracy: 0.8500 / Loss: 45.1260\n",
      "Accuracy: 0.8500 / Loss: 45.1257\n",
      "Accuracy: 0.8500 / Loss: 45.1253\n",
      "Accuracy: 0.8500 / Loss: 45.1250\n",
      "Accuracy: 0.8500 / Loss: 45.1246\n",
      "Accuracy: 0.8500 / Loss: 45.1243\n",
      "Accuracy: 0.8500 / Loss: 45.1239\n",
      "Accuracy: 0.8500 / Loss: 45.1236\n",
      "Accuracy: 0.8500 / Loss: 45.1232\n",
      "Accuracy: 0.8500 / Loss: 45.1229\n",
      "Accuracy: 0.8500 / Loss: 45.1225\n",
      "Accuracy: 0.8500 / Loss: 45.1222\n",
      "Accuracy: 0.8500 / Loss: 45.1218\n",
      "Accuracy: 0.8500 / Loss: 45.1215\n",
      "Accuracy: 0.8500 / Loss: 45.1211\n",
      "Accuracy: 0.8500 / Loss: 45.1208\n",
      "Accuracy: 0.8500 / Loss: 45.1204\n",
      "Accuracy: 0.8500 / Loss: 45.1201\n",
      "Accuracy: 0.8500 / Loss: 45.1197\n",
      "Accuracy: 0.8500 / Loss: 45.1194\n",
      "Accuracy: 0.8500 / Loss: 45.1190\n",
      "Accuracy: 0.8500 / Loss: 45.1187\n",
      "Accuracy: 0.8500 / Loss: 45.1183\n",
      "Accuracy: 0.8500 / Loss: 45.1180\n",
      "Accuracy: 0.8500 / Loss: 45.1176\n",
      "Accuracy: 0.8500 / Loss: 45.1173\n",
      "Accuracy: 0.8500 / Loss: 45.1169\n",
      "Accuracy: 0.8500 / Loss: 45.1166\n",
      "Accuracy: 0.8500 / Loss: 45.1162\n",
      "Accuracy: 0.8500 / Loss: 45.1159\n",
      "Accuracy: 0.8500 / Loss: 45.1155\n",
      "Accuracy: 0.8500 / Loss: 45.1152\n",
      "Accuracy: 0.8500 / Loss: 45.1148\n",
      "Accuracy: 0.8500 / Loss: 45.1145\n",
      "Accuracy: 0.8500 / Loss: 45.1141\n",
      "Accuracy: 0.8500 / Loss: 45.1138\n",
      "Accuracy: 0.8500 / Loss: 45.1134\n",
      "Accuracy: 0.8500 / Loss: 45.1131\n",
      "Accuracy: 0.8500 / Loss: 45.1127\n",
      "Accuracy: 0.8500 / Loss: 45.1123\n",
      "Accuracy: 0.8500 / Loss: 45.1120\n",
      "Accuracy: 0.8500 / Loss: 45.1116\n",
      "Accuracy: 0.8500 / Loss: 45.1113\n",
      "Accuracy: 0.8500 / Loss: 45.1109\n",
      "Accuracy: 0.8500 / Loss: 45.1106\n",
      "Accuracy: 0.8500 / Loss: 45.1102\n",
      "Accuracy: 0.8500 / Loss: 45.1098\n",
      "Accuracy: 0.8500 / Loss: 45.1095\n",
      "Accuracy: 0.8500 / Loss: 45.1091\n",
      "Accuracy: 0.8500 / Loss: 45.1088\n",
      "Accuracy: 0.8500 / Loss: 45.1084\n",
      "Accuracy: 0.8500 / Loss: 45.1081\n",
      "Accuracy: 0.8500 / Loss: 45.1077\n",
      "Accuracy: 0.8500 / Loss: 45.1073\n",
      "Accuracy: 0.8500 / Loss: 45.1070\n",
      "Accuracy: 0.8500 / Loss: 45.1066\n",
      "Accuracy: 0.8500 / Loss: 45.1063\n",
      "Accuracy: 0.8500 / Loss: 45.1059\n",
      "Accuracy: 0.8500 / Loss: 45.1055\n",
      "Accuracy: 0.8500 / Loss: 45.1052\n",
      "Accuracy: 0.8500 / Loss: 45.1048\n",
      "Accuracy: 0.8500 / Loss: 45.1044\n",
      "Accuracy: 0.8500 / Loss: 45.1041\n",
      "Accuracy: 0.8500 / Loss: 45.1037\n",
      "Accuracy: 0.8500 / Loss: 45.1033\n",
      "Accuracy: 0.8500 / Loss: 45.1030\n",
      "Accuracy: 0.8500 / Loss: 45.1026\n",
      "Accuracy: 0.8500 / Loss: 45.1023\n",
      "Accuracy: 0.8500 / Loss: 45.1019\n",
      "Accuracy: 0.8500 / Loss: 45.1015\n",
      "Accuracy: 0.8500 / Loss: 45.1012\n",
      "Accuracy: 0.8500 / Loss: 45.1008\n",
      "Accuracy: 0.8500 / Loss: 45.1004\n",
      "Accuracy: 0.8500 / Loss: 45.1001\n",
      "Accuracy: 0.8500 / Loss: 45.0997\n",
      "Accuracy: 0.8500 / Loss: 45.0993\n",
      "Accuracy: 0.8500 / Loss: 45.0989\n",
      "Accuracy: 0.8500 / Loss: 45.0986\n",
      "Accuracy: 0.8500 / Loss: 45.0982\n",
      "Accuracy: 0.8500 / Loss: 45.0978\n",
      "Accuracy: 0.8500 / Loss: 45.0975\n",
      "Accuracy: 0.8500 / Loss: 45.0971\n",
      "Accuracy: 0.8500 / Loss: 45.0967\n",
      "Accuracy: 0.8500 / Loss: 45.0964\n",
      "Accuracy: 0.8500 / Loss: 45.0960\n",
      "Accuracy: 0.8500 / Loss: 45.0956\n",
      "Accuracy: 0.8500 / Loss: 45.0952\n",
      "Accuracy: 0.8500 / Loss: 45.0949\n",
      "Accuracy: 0.8500 / Loss: 45.0945\n",
      "Accuracy: 0.8500 / Loss: 45.0941\n",
      "Accuracy: 0.8500 / Loss: 45.0937\n",
      "Accuracy: 0.8500 / Loss: 45.0934\n",
      "Accuracy: 0.8500 / Loss: 45.0930\n",
      "Accuracy: 0.8500 / Loss: 45.0926\n",
      "Accuracy: 0.8500 / Loss: 45.0922\n",
      "Accuracy: 0.8500 / Loss: 45.0919\n",
      "Accuracy: 0.8500 / Loss: 45.0915\n",
      "Accuracy: 0.8500 / Loss: 45.0911\n",
      "Accuracy: 0.8500 / Loss: 45.0907\n",
      "Accuracy: 0.8500 / Loss: 45.0903\n",
      "Accuracy: 0.8500 / Loss: 45.0900\n",
      "Accuracy: 0.8500 / Loss: 45.0896\n",
      "Accuracy: 0.8500 / Loss: 45.0892\n",
      "Accuracy: 0.8500 / Loss: 45.0888\n",
      "Accuracy: 0.8500 / Loss: 45.0884\n",
      "Accuracy: 0.8500 / Loss: 45.0880\n",
      "Accuracy: 0.8500 / Loss: 45.0877\n",
      "Accuracy: 0.8500 / Loss: 45.0873\n",
      "Accuracy: 0.8500 / Loss: 45.0869\n",
      "Accuracy: 0.8500 / Loss: 45.0865\n",
      "Accuracy: 0.8500 / Loss: 45.0861\n",
      "Accuracy: 0.8500 / Loss: 45.0857\n",
      "Accuracy: 0.8500 / Loss: 45.0854\n",
      "Accuracy: 0.8500 / Loss: 45.0850\n",
      "Accuracy: 0.8500 / Loss: 45.0846\n",
      "Accuracy: 0.8500 / Loss: 45.0842\n",
      "Accuracy: 0.8500 / Loss: 45.0838\n",
      "Accuracy: 0.8500 / Loss: 45.0834\n",
      "Accuracy: 0.8500 / Loss: 45.0830\n",
      "Accuracy: 0.8500 / Loss: 45.0826\n",
      "Accuracy: 0.8500 / Loss: 45.0822\n",
      "Accuracy: 0.8500 / Loss: 45.0819\n",
      "Accuracy: 0.8500 / Loss: 45.0815\n",
      "Accuracy: 0.8500 / Loss: 45.0811\n",
      "Accuracy: 0.8500 / Loss: 45.0807\n",
      "Accuracy: 0.8500 / Loss: 45.0803\n",
      "Accuracy: 0.8500 / Loss: 45.0799\n",
      "Accuracy: 0.8500 / Loss: 45.0795\n",
      "Accuracy: 0.8500 / Loss: 45.0791\n",
      "Accuracy: 0.8500 / Loss: 45.0787\n",
      "Accuracy: 0.8500 / Loss: 45.0783\n",
      "Accuracy: 0.8500 / Loss: 45.0779\n",
      "Accuracy: 0.8500 / Loss: 45.0775\n",
      "Accuracy: 0.8500 / Loss: 45.0771\n",
      "Accuracy: 0.8500 / Loss: 45.0767\n",
      "Accuracy: 0.8500 / Loss: 45.0763\n",
      "Accuracy: 0.8500 / Loss: 45.0759\n",
      "Accuracy: 0.8500 / Loss: 45.0755\n",
      "Accuracy: 0.8500 / Loss: 45.0751\n",
      "Accuracy: 0.8500 / Loss: 45.0747\n",
      "Accuracy: 0.8500 / Loss: 45.0743\n",
      "Accuracy: 0.8500 / Loss: 45.0739\n",
      "Accuracy: 0.8500 / Loss: 45.0735\n",
      "Accuracy: 0.8500 / Loss: 45.0731\n",
      "Accuracy: 0.8500 / Loss: 45.0727\n",
      "Accuracy: 0.8500 / Loss: 45.0723\n",
      "Accuracy: 0.8500 / Loss: 45.0719\n",
      "Accuracy: 0.8500 / Loss: 45.0715\n",
      "Accuracy: 0.8500 / Loss: 45.0711\n",
      "Accuracy: 0.8500 / Loss: 45.0707\n",
      "Accuracy: 0.8500 / Loss: 45.0703\n",
      "Accuracy: 0.8500 / Loss: 45.0698\n",
      "Accuracy: 0.8500 / Loss: 45.0694\n",
      "Accuracy: 0.8500 / Loss: 45.0690\n",
      "Accuracy: 0.8500 / Loss: 45.0686\n",
      "Accuracy: 0.8500 / Loss: 45.0682\n",
      "Accuracy: 0.8500 / Loss: 45.0678\n",
      "Accuracy: 0.8500 / Loss: 45.0674\n",
      "Accuracy: 0.8500 / Loss: 45.0670\n",
      "Accuracy: 0.8500 / Loss: 45.0666\n",
      "Accuracy: 0.8500 / Loss: 45.0661\n",
      "Accuracy: 0.8500 / Loss: 45.0657\n",
      "Accuracy: 0.8500 / Loss: 45.0653\n",
      "Accuracy: 0.8500 / Loss: 45.0649\n",
      "Accuracy: 0.8500 / Loss: 45.0645\n",
      "Accuracy: 0.8500 / Loss: 45.0641\n",
      "Accuracy: 0.8500 / Loss: 45.0636\n",
      "Accuracy: 0.8500 / Loss: 45.0632\n",
      "Accuracy: 0.8500 / Loss: 45.0628\n",
      "Accuracy: 0.8500 / Loss: 45.0624\n",
      "Accuracy: 0.8500 / Loss: 45.0620\n",
      "Accuracy: 0.8500 / Loss: 45.0615\n",
      "Accuracy: 0.8500 / Loss: 45.0611\n",
      "Accuracy: 0.8500 / Loss: 45.0607\n",
      "Accuracy: 0.8500 / Loss: 45.0603\n",
      "Accuracy: 0.8500 / Loss: 45.0598\n",
      "Accuracy: 0.8500 / Loss: 45.0594\n",
      "Accuracy: 0.8500 / Loss: 45.0590\n",
      "Accuracy: 0.8500 / Loss: 45.0586\n",
      "Accuracy: 0.8500 / Loss: 45.0581\n",
      "Accuracy: 0.8500 / Loss: 45.0577\n",
      "Accuracy: 0.8500 / Loss: 45.0573\n",
      "Accuracy: 0.8500 / Loss: 45.0569\n",
      "Accuracy: 0.8500 / Loss: 45.0564\n",
      "Accuracy: 0.8500 / Loss: 45.0560\n",
      "Accuracy: 0.8500 / Loss: 45.0556\n",
      "Accuracy: 0.8500 / Loss: 45.0551\n",
      "Accuracy: 0.8500 / Loss: 45.0547\n",
      "Accuracy: 0.8500 / Loss: 45.0543\n",
      "Accuracy: 0.8500 / Loss: 45.0539\n",
      "Accuracy: 0.8500 / Loss: 45.0534\n",
      "Accuracy: 0.8500 / Loss: 45.0530\n",
      "Accuracy: 0.8500 / Loss: 45.0526\n",
      "Accuracy: 0.8500 / Loss: 45.0521\n",
      "Accuracy: 0.8500 / Loss: 45.0517\n",
      "Accuracy: 0.8500 / Loss: 45.0512\n",
      "Accuracy: 0.8500 / Loss: 45.0508\n",
      "Accuracy: 0.8500 / Loss: 45.0504\n",
      "Accuracy: 0.8500 / Loss: 45.0499\n",
      "Accuracy: 0.8500 / Loss: 45.0495\n",
      "Accuracy: 0.8500 / Loss: 45.0491\n",
      "Accuracy: 0.8500 / Loss: 45.0486\n",
      "Accuracy: 0.8500 / Loss: 45.0482\n",
      "Accuracy: 0.8500 / Loss: 45.0477\n",
      "Accuracy: 0.8500 / Loss: 45.0473\n",
      "Accuracy: 0.8500 / Loss: 45.0469\n",
      "Accuracy: 0.8500 / Loss: 45.0464\n",
      "Accuracy: 0.8500 / Loss: 45.0460\n",
      "Accuracy: 0.8500 / Loss: 45.0455\n",
      "Accuracy: 0.8500 / Loss: 45.0451\n",
      "Accuracy: 0.8500 / Loss: 45.0446\n",
      "Accuracy: 0.8500 / Loss: 45.0442\n",
      "Accuracy: 0.8500 / Loss: 45.0438\n",
      "Accuracy: 0.8500 / Loss: 45.0433\n",
      "Accuracy: 0.8500 / Loss: 45.0429\n",
      "Accuracy: 0.8500 / Loss: 45.0424\n",
      "Accuracy: 0.8500 / Loss: 45.0420\n",
      "Accuracy: 0.8500 / Loss: 45.0415\n",
      "Accuracy: 0.8500 / Loss: 45.0411\n",
      "Accuracy: 0.8500 / Loss: 45.0406\n",
      "Accuracy: 0.8500 / Loss: 45.0402\n",
      "Accuracy: 0.8500 / Loss: 45.0397\n",
      "Accuracy: 0.8500 / Loss: 45.0393\n",
      "Accuracy: 0.8500 / Loss: 45.0388\n",
      "Accuracy: 0.8500 / Loss: 45.0384\n",
      "Accuracy: 0.8500 / Loss: 45.0379\n",
      "Accuracy: 0.8500 / Loss: 45.0375\n",
      "Accuracy: 0.8500 / Loss: 45.0370\n",
      "Accuracy: 0.8500 / Loss: 45.0366\n",
      "Accuracy: 0.8500 / Loss: 45.0361\n",
      "Accuracy: 0.8500 / Loss: 45.0357\n",
      "Accuracy: 0.8500 / Loss: 45.0352\n",
      "Accuracy: 0.8500 / Loss: 45.0348\n",
      "Accuracy: 0.8500 / Loss: 45.0343\n",
      "Accuracy: 0.8500 / Loss: 45.0338\n",
      "Accuracy: 0.8500 / Loss: 45.0334\n",
      "Accuracy: 0.8500 / Loss: 45.0329\n",
      "Accuracy: 0.8500 / Loss: 45.0325\n",
      "Accuracy: 0.8500 / Loss: 45.0320\n",
      "Accuracy: 0.8500 / Loss: 45.0316\n",
      "Accuracy: 0.8500 / Loss: 45.0311\n",
      "Accuracy: 0.8500 / Loss: 45.0306\n",
      "Accuracy: 0.8500 / Loss: 45.0302\n",
      "Accuracy: 0.8500 / Loss: 45.0297\n",
      "Accuracy: 0.8500 / Loss: 45.0293\n",
      "Accuracy: 0.8500 / Loss: 45.0288\n",
      "Accuracy: 0.8500 / Loss: 45.0283\n",
      "Accuracy: 0.8500 / Loss: 45.0279\n",
      "Accuracy: 0.8500 / Loss: 45.0274\n",
      "Accuracy: 0.8500 / Loss: 45.0270\n",
      "Accuracy: 0.8500 / Loss: 45.0265\n",
      "Accuracy: 0.8500 / Loss: 45.0260\n",
      "Accuracy: 0.8500 / Loss: 45.0256\n",
      "Accuracy: 0.8500 / Loss: 45.0251\n",
      "Accuracy: 0.8500 / Loss: 45.0246\n",
      "Accuracy: 0.8500 / Loss: 45.0242\n",
      "Accuracy: 0.8500 / Loss: 45.0237\n",
      "Accuracy: 0.8500 / Loss: 45.0232\n",
      "Accuracy: 0.8500 / Loss: 45.0228\n",
      "Accuracy: 0.8500 / Loss: 45.0223\n",
      "Accuracy: 0.8500 / Loss: 45.0219\n",
      "Accuracy: 0.8500 / Loss: 45.0214\n",
      "Accuracy: 0.8500 / Loss: 45.0209\n",
      "Accuracy: 0.8500 / Loss: 45.0205\n",
      "Accuracy: 0.8500 / Loss: 45.0200\n",
      "Accuracy: 0.8500 / Loss: 45.0195\n",
      "Accuracy: 0.8500 / Loss: 45.0191\n",
      "Accuracy: 0.8500 / Loss: 45.0186\n",
      "Accuracy: 0.8500 / Loss: 45.0181\n",
      "Accuracy: 0.8500 / Loss: 45.0177\n",
      "Accuracy: 0.8500 / Loss: 45.0172\n",
      "Accuracy: 0.8500 / Loss: 45.0167\n",
      "Accuracy: 0.8500 / Loss: 45.0162\n",
      "Accuracy: 0.8500 / Loss: 45.0158\n",
      "Accuracy: 0.8500 / Loss: 45.0153\n",
      "Accuracy: 0.8500 / Loss: 45.0148\n",
      "Accuracy: 0.8500 / Loss: 45.0144\n",
      "Accuracy: 0.8500 / Loss: 45.0139\n",
      "Accuracy: 0.8500 / Loss: 45.0134\n",
      "Accuracy: 0.8500 / Loss: 45.0130\n",
      "Accuracy: 0.8500 / Loss: 45.0125\n",
      "Accuracy: 0.8500 / Loss: 45.0120\n",
      "Accuracy: 0.8500 / Loss: 45.0116\n",
      "Accuracy: 0.8500 / Loss: 45.0111\n",
      "Accuracy: 0.8500 / Loss: 45.0106\n",
      "Accuracy: 0.8500 / Loss: 45.0101\n",
      "Accuracy: 0.8500 / Loss: 45.0097\n",
      "Accuracy: 0.8500 / Loss: 45.0092\n",
      "Accuracy: 0.8500 / Loss: 45.0087\n",
      "Accuracy: 0.8500 / Loss: 45.0083\n",
      "Accuracy: 0.8500 / Loss: 45.0078\n",
      "Accuracy: 0.8500 / Loss: 45.0073\n",
      "Accuracy: 0.8500 / Loss: 45.0068\n",
      "Accuracy: 0.8500 / Loss: 45.0064\n",
      "Accuracy: 0.8500 / Loss: 45.0059\n",
      "Accuracy: 0.8500 / Loss: 45.0054\n",
      "Accuracy: 0.8500 / Loss: 45.0050\n",
      "Accuracy: 0.8500 / Loss: 45.0045\n",
      "Accuracy: 0.8500 / Loss: 45.0040\n",
      "Accuracy: 0.8500 / Loss: 45.0035\n",
      "Accuracy: 0.8500 / Loss: 45.0031\n",
      "Accuracy: 0.8500 / Loss: 45.0026\n",
      "Accuracy: 0.8500 / Loss: 45.0021\n",
      "Accuracy: 0.8500 / Loss: 45.0017\n",
      "Accuracy: 0.8500 / Loss: 45.0012\n",
      "Accuracy: 0.8500 / Loss: 45.0007\n",
      "Accuracy: 0.8500 / Loss: 45.0002\n",
      "Accuracy: 0.8500 / Loss: 44.9998\n",
      "Accuracy: 0.8500 / Loss: 44.9993\n",
      "Accuracy: 0.8500 / Loss: 44.9988\n",
      "Accuracy: 0.8500 / Loss: 44.9984\n",
      "Accuracy: 0.8500 / Loss: 44.9979\n",
      "Accuracy: 0.8500 / Loss: 44.9974\n",
      "Accuracy: 0.8500 / Loss: 44.9969\n",
      "Accuracy: 0.8500 / Loss: 44.9965\n",
      "Accuracy: 0.8500 / Loss: 44.9960\n",
      "Accuracy: 0.8500 / Loss: 44.9955\n",
      "Accuracy: 0.8500 / Loss: 44.9951\n",
      "Accuracy: 0.8500 / Loss: 44.9946\n",
      "Accuracy: 0.8500 / Loss: 44.9941\n",
      "Accuracy: 0.8500 / Loss: 44.9936\n",
      "Accuracy: 0.8500 / Loss: 44.9932\n",
      "Accuracy: 0.8500 / Loss: 44.9927\n",
      "Accuracy: 0.8500 / Loss: 44.9922\n",
      "Accuracy: 0.8500 / Loss: 44.9918\n",
      "Accuracy: 0.8500 / Loss: 44.9913\n",
      "Accuracy: 0.8500 / Loss: 44.9908\n",
      "Accuracy: 0.8500 / Loss: 44.9904\n",
      "Accuracy: 0.8500 / Loss: 44.9899\n",
      "Accuracy: 0.8500 / Loss: 44.9894\n",
      "Accuracy: 0.8500 / Loss: 44.9890\n",
      "Accuracy: 0.8500 / Loss: 44.9885\n",
      "Accuracy: 0.8500 / Loss: 44.9880\n",
      "Accuracy: 0.8500 / Loss: 44.9876\n",
      "Accuracy: 0.8500 / Loss: 44.9871\n",
      "Accuracy: 0.8500 / Loss: 44.9866\n",
      "Accuracy: 0.8500 / Loss: 44.9861\n",
      "Accuracy: 0.8500 / Loss: 44.9857\n",
      "Accuracy: 0.8500 / Loss: 44.9852\n",
      "Accuracy: 0.8500 / Loss: 44.9847\n",
      "Accuracy: 0.8500 / Loss: 44.9843\n",
      "Accuracy: 0.8500 / Loss: 44.9838\n",
      "Accuracy: 0.8500 / Loss: 44.9834\n",
      "Accuracy: 0.8500 / Loss: 44.9829\n",
      "Accuracy: 0.8500 / Loss: 44.9824\n",
      "Accuracy: 0.8500 / Loss: 44.9820\n",
      "Accuracy: 0.8500 / Loss: 44.9815\n",
      "Accuracy: 0.8500 / Loss: 44.9810\n",
      "Accuracy: 0.8500 / Loss: 44.9806\n",
      "Accuracy: 0.8500 / Loss: 44.9801\n",
      "Accuracy: 0.8500 / Loss: 44.9796\n",
      "Accuracy: 0.8500 / Loss: 44.9792\n",
      "Accuracy: 0.8500 / Loss: 44.9787\n",
      "Accuracy: 0.8500 / Loss: 44.9783\n",
      "Accuracy: 0.8500 / Loss: 44.9778\n",
      "Accuracy: 0.8500 / Loss: 44.9773\n",
      "Accuracy: 0.8500 / Loss: 44.9769\n",
      "Accuracy: 0.8500 / Loss: 44.9764\n",
      "Accuracy: 0.8500 / Loss: 44.9760\n",
      "Accuracy: 0.8500 / Loss: 44.9755\n",
      "Accuracy: 0.8500 / Loss: 44.9750\n",
      "Accuracy: 0.8500 / Loss: 44.9746\n",
      "Accuracy: 0.8500 / Loss: 44.9741\n",
      "Accuracy: 0.8500 / Loss: 44.9737\n",
      "Accuracy: 0.8500 / Loss: 44.9732\n",
      "Accuracy: 0.8500 / Loss: 44.9728\n",
      "Accuracy: 0.8500 / Loss: 44.9723\n",
      "Accuracy: 0.8500 / Loss: 44.9718\n",
      "Accuracy: 0.8500 / Loss: 44.9714\n",
      "Accuracy: 0.8500 / Loss: 44.9709\n",
      "Accuracy: 0.8500 / Loss: 44.9705\n",
      "Accuracy: 0.8500 / Loss: 44.9700\n",
      "Accuracy: 0.8500 / Loss: 44.9696\n",
      "Accuracy: 0.8500 / Loss: 44.9691\n",
      "Accuracy: 0.8500 / Loss: 44.9687\n",
      "Accuracy: 0.8500 / Loss: 44.9682\n",
      "Accuracy: 0.8500 / Loss: 44.9678\n",
      "Accuracy: 0.8500 / Loss: 44.9673\n",
      "Accuracy: 0.8500 / Loss: 44.9669\n",
      "Accuracy: 0.8500 / Loss: 44.9664\n",
      "Accuracy: 0.8500 / Loss: 44.9660\n",
      "Accuracy: 0.8500 / Loss: 44.9655\n",
      "Accuracy: 0.8500 / Loss: 44.9651\n",
      "Accuracy: 0.8500 / Loss: 44.9646\n",
      "Accuracy: 0.8500 / Loss: 44.9642\n",
      "Accuracy: 0.8500 / Loss: 44.9637\n",
      "Accuracy: 0.8500 / Loss: 44.9633\n",
      "Accuracy: 0.8500 / Loss: 44.9628\n",
      "Accuracy: 0.8500 / Loss: 44.9624\n",
      "Accuracy: 0.8500 / Loss: 44.9619\n",
      "Accuracy: 0.8500 / Loss: 44.9615\n",
      "Accuracy: 0.8500 / Loss: 44.9611\n",
      "Accuracy: 0.8500 / Loss: 44.9606\n",
      "Accuracy: 0.8500 / Loss: 44.9602\n",
      "Accuracy: 0.8500 / Loss: 44.9597\n",
      "Accuracy: 0.8500 / Loss: 44.9593\n",
      "Accuracy: 0.8500 / Loss: 44.9588\n",
      "Accuracy: 0.8500 / Loss: 44.9584\n",
      "Accuracy: 0.8500 / Loss: 44.9580\n",
      "Accuracy: 0.8500 / Loss: 44.9575\n",
      "Accuracy: 0.8500 / Loss: 44.9571\n",
      "Accuracy: 0.8500 / Loss: 44.9566\n",
      "Accuracy: 0.8500 / Loss: 44.9562\n",
      "Accuracy: 0.8500 / Loss: 44.9558\n",
      "Accuracy: 0.8500 / Loss: 44.9553\n",
      "Accuracy: 0.8500 / Loss: 44.9549\n",
      "Accuracy: 0.8500 / Loss: 44.9545\n",
      "Accuracy: 0.8500 / Loss: 44.9540\n",
      "Accuracy: 0.8500 / Loss: 44.9536\n",
      "Accuracy: 0.8500 / Loss: 44.9532\n",
      "Accuracy: 0.8500 / Loss: 44.9527\n",
      "Accuracy: 0.8500 / Loss: 44.9523\n",
      "Accuracy: 0.8500 / Loss: 44.9519\n",
      "Accuracy: 0.8500 / Loss: 44.9514\n",
      "Accuracy: 0.8500 / Loss: 44.9510\n",
      "Accuracy: 0.8500 / Loss: 44.9506\n",
      "Accuracy: 0.8500 / Loss: 44.9502\n",
      "Accuracy: 0.8500 / Loss: 44.9497\n",
      "Accuracy: 0.8500 / Loss: 44.9493\n",
      "Accuracy: 0.8500 / Loss: 44.9489\n",
      "Accuracy: 0.8500 / Loss: 44.9485\n",
      "Accuracy: 0.8500 / Loss: 44.9480\n",
      "Accuracy: 0.8500 / Loss: 44.9476\n",
      "Accuracy: 0.8500 / Loss: 44.9472\n",
      "Accuracy: 0.8500 / Loss: 44.9468\n",
      "Accuracy: 0.8500 / Loss: 44.9463\n",
      "Accuracy: 0.8500 / Loss: 44.9459\n",
      "Accuracy: 0.8500 / Loss: 44.9455\n",
      "Accuracy: 0.8500 / Loss: 44.9451\n",
      "Accuracy: 0.8500 / Loss: 44.9447\n",
      "Accuracy: 0.8500 / Loss: 44.9442\n",
      "Accuracy: 0.8500 / Loss: 44.9438\n",
      "Accuracy: 0.8500 / Loss: 44.9434\n",
      "Accuracy: 0.8500 / Loss: 44.9430\n",
      "Accuracy: 0.8500 / Loss: 44.9426\n",
      "Accuracy: 0.8500 / Loss: 44.9421\n",
      "Accuracy: 0.8500 / Loss: 44.9417\n",
      "Accuracy: 0.8500 / Loss: 44.9413\n",
      "Accuracy: 0.8500 / Loss: 44.9409\n",
      "Accuracy: 0.8500 / Loss: 44.9405\n",
      "Accuracy: 0.8500 / Loss: 44.9401\n",
      "Accuracy: 0.8500 / Loss: 44.9397\n",
      "Accuracy: 0.8500 / Loss: 44.9393\n",
      "Accuracy: 0.8500 / Loss: 44.9388\n",
      "Accuracy: 0.8500 / Loss: 44.9384\n",
      "Accuracy: 0.8500 / Loss: 44.9380\n",
      "Accuracy: 0.8500 / Loss: 44.9376\n",
      "Accuracy: 0.8500 / Loss: 44.9372\n",
      "Accuracy: 0.8500 / Loss: 44.9368\n",
      "Accuracy: 0.8500 / Loss: 44.9364\n",
      "Accuracy: 0.8500 / Loss: 44.9360\n",
      "Accuracy: 0.8500 / Loss: 44.9356\n",
      "Accuracy: 0.8500 / Loss: 44.9352\n",
      "Accuracy: 0.8500 / Loss: 44.9348\n",
      "Accuracy: 0.8500 / Loss: 44.9344\n",
      "Accuracy: 0.8500 / Loss: 44.9340\n",
      "Accuracy: 0.8500 / Loss: 44.9336\n",
      "Accuracy: 0.8500 / Loss: 44.9332\n",
      "Accuracy: 0.8500 / Loss: 44.9328\n",
      "Accuracy: 0.8500 / Loss: 44.9324\n",
      "Accuracy: 0.8500 / Loss: 44.9320\n",
      "Accuracy: 0.8500 / Loss: 44.9316\n",
      "Accuracy: 0.8500 / Loss: 44.9312\n",
      "Accuracy: 0.8500 / Loss: 44.9308\n",
      "Accuracy: 0.8500 / Loss: 44.9304\n",
      "Accuracy: 0.8500 / Loss: 44.9300\n",
      "Accuracy: 0.8500 / Loss: 44.9296\n",
      "Accuracy: 0.8500 / Loss: 44.9292\n",
      "Accuracy: 0.8500 / Loss: 44.9288\n",
      "Accuracy: 0.8500 / Loss: 44.9284\n",
      "Accuracy: 0.8500 / Loss: 44.9280\n",
      "Accuracy: 0.8500 / Loss: 44.9276\n",
      "Accuracy: 0.8500 / Loss: 44.9272\n",
      "Accuracy: 0.8500 / Loss: 44.9269\n",
      "Accuracy: 0.8500 / Loss: 44.9265\n",
      "Accuracy: 0.8500 / Loss: 44.9261\n",
      "Accuracy: 0.8500 / Loss: 44.9257\n",
      "Accuracy: 0.8500 / Loss: 44.9253\n",
      "Accuracy: 0.8500 / Loss: 44.9249\n",
      "Accuracy: 0.8500 / Loss: 44.9245\n",
      "Accuracy: 0.8500 / Loss: 44.9241\n",
      "Accuracy: 0.8500 / Loss: 44.9238\n",
      "Accuracy: 0.8500 / Loss: 44.9234\n",
      "Accuracy: 0.8500 / Loss: 44.9230\n",
      "Accuracy: 0.8500 / Loss: 44.9226\n",
      "Accuracy: 0.8500 / Loss: 44.9222\n",
      "Accuracy: 0.8500 / Loss: 44.9218\n",
      "Accuracy: 0.8500 / Loss: 44.9215\n",
      "Accuracy: 0.8500 / Loss: 44.9211\n",
      "Accuracy: 0.8500 / Loss: 44.9207\n",
      "Accuracy: 0.8500 / Loss: 44.9203\n",
      "Accuracy: 0.8500 / Loss: 44.9200\n",
      "Accuracy: 0.8500 / Loss: 44.9196\n",
      "Accuracy: 0.8500 / Loss: 44.9192\n",
      "Accuracy: 0.8500 / Loss: 44.9188\n",
      "Accuracy: 0.8500 / Loss: 44.9184\n",
      "Accuracy: 0.8500 / Loss: 44.9181\n",
      "Accuracy: 0.8500 / Loss: 44.9177\n",
      "Accuracy: 0.8500 / Loss: 44.9173\n",
      "Accuracy: 0.8500 / Loss: 44.9170\n",
      "Accuracy: 0.8500 / Loss: 44.9166\n",
      "Accuracy: 0.8500 / Loss: 44.9162\n",
      "Accuracy: 0.8500 / Loss: 44.9158\n",
      "Accuracy: 0.8500 / Loss: 44.9155\n",
      "Accuracy: 0.8500 / Loss: 44.9151\n",
      "Accuracy: 0.8500 / Loss: 44.9147\n",
      "Accuracy: 0.8500 / Loss: 44.9144\n",
      "Accuracy: 0.8500 / Loss: 44.9140\n",
      "Accuracy: 0.8500 / Loss: 44.9136\n",
      "Accuracy: 0.8500 / Loss: 44.9133\n",
      "Accuracy: 0.8500 / Loss: 44.9129\n",
      "Accuracy: 0.8500 / Loss: 44.9125\n",
      "Accuracy: 0.8500 / Loss: 44.9122\n",
      "Accuracy: 0.8500 / Loss: 44.9118\n",
      "Accuracy: 0.8500 / Loss: 44.9114\n",
      "Accuracy: 0.8500 / Loss: 44.9111\n",
      "Accuracy: 0.8500 / Loss: 44.9107\n",
      "Accuracy: 0.8500 / Loss: 44.9104\n",
      "Accuracy: 0.8500 / Loss: 44.9100\n",
      "Accuracy: 0.8500 / Loss: 44.9096\n",
      "Accuracy: 0.8500 / Loss: 44.9093\n",
      "Accuracy: 0.8500 / Loss: 44.9089\n",
      "Accuracy: 0.8500 / Loss: 44.9086\n",
      "Accuracy: 0.8500 / Loss: 44.9082\n",
      "Accuracy: 0.8500 / Loss: 44.9079\n",
      "Accuracy: 0.8500 / Loss: 44.9075\n",
      "Accuracy: 0.8500 / Loss: 44.9072\n",
      "Accuracy: 0.8500 / Loss: 44.9068\n",
      "Accuracy: 0.8500 / Loss: 44.9064\n",
      "Accuracy: 0.8500 / Loss: 44.9061\n",
      "Accuracy: 0.8500 / Loss: 44.9057\n",
      "Accuracy: 0.8500 / Loss: 44.9054\n",
      "Accuracy: 0.8500 / Loss: 44.9050\n",
      "Accuracy: 0.8500 / Loss: 44.9047\n",
      "Accuracy: 0.8500 / Loss: 44.9043\n",
      "Accuracy: 0.8500 / Loss: 44.9040\n",
      "Accuracy: 0.8500 / Loss: 44.9036\n",
      "Accuracy: 0.8500 / Loss: 44.9033\n",
      "Accuracy: 0.8500 / Loss: 44.9029\n",
      "Accuracy: 0.8500 / Loss: 44.9026\n",
      "Accuracy: 0.8500 / Loss: 44.9023\n",
      "Accuracy: 0.8500 / Loss: 44.9019\n",
      "Accuracy: 0.8500 / Loss: 44.9016\n",
      "Accuracy: 0.8500 / Loss: 44.9012\n",
      "Accuracy: 0.8500 / Loss: 44.9009\n",
      "Accuracy: 0.8500 / Loss: 44.9005\n",
      "Accuracy: 0.8500 / Loss: 44.9002\n",
      "Accuracy: 0.8500 / Loss: 44.8999\n",
      "Accuracy: 0.8500 / Loss: 44.8995\n",
      "Accuracy: 0.8500 / Loss: 44.8992\n",
      "Accuracy: 0.8500 / Loss: 44.8988\n",
      "Accuracy: 0.8500 / Loss: 44.8985\n",
      "Accuracy: 0.8500 / Loss: 44.8982\n",
      "Accuracy: 0.8500 / Loss: 44.8978\n",
      "Accuracy: 0.8500 / Loss: 44.8975\n",
      "Accuracy: 0.8500 / Loss: 44.8971\n",
      "Accuracy: 0.8500 / Loss: 44.8968\n",
      "Accuracy: 0.8500 / Loss: 44.8965\n",
      "Accuracy: 0.8500 / Loss: 44.8961\n",
      "Accuracy: 0.8500 / Loss: 44.8958\n",
      "Accuracy: 0.8500 / Loss: 44.8955\n",
      "Accuracy: 0.8500 / Loss: 44.8951\n",
      "Accuracy: 0.8500 / Loss: 44.8948\n",
      "Accuracy: 0.8500 / Loss: 44.8945\n",
      "Accuracy: 0.8500 / Loss: 44.8942\n",
      "Accuracy: 0.8500 / Loss: 44.8938\n",
      "Accuracy: 0.8500 / Loss: 44.8935\n",
      "Accuracy: 0.8500 / Loss: 44.8932\n",
      "Accuracy: 0.8500 / Loss: 44.8928\n",
      "Accuracy: 0.8500 / Loss: 44.8925\n",
      "Accuracy: 0.8500 / Loss: 44.8922\n",
      "Accuracy: 0.8500 / Loss: 44.8919\n",
      "Accuracy: 0.8500 / Loss: 44.8915\n",
      "Accuracy: 0.8500 / Loss: 44.8912\n",
      "Accuracy: 0.8500 / Loss: 44.8909\n",
      "Accuracy: 0.8500 / Loss: 44.8906\n",
      "Accuracy: 0.8500 / Loss: 44.8902\n",
      "Accuracy: 0.8500 / Loss: 44.8899\n",
      "Accuracy: 0.8500 / Loss: 44.8896\n",
      "Accuracy: 0.8500 / Loss: 44.8893\n",
      "Accuracy: 0.8500 / Loss: 44.8889\n",
      "Accuracy: 0.8500 / Loss: 44.8886\n",
      "Accuracy: 0.8500 / Loss: 44.8883\n",
      "Accuracy: 0.8500 / Loss: 44.8880\n",
      "Accuracy: 0.8500 / Loss: 44.8877\n",
      "Accuracy: 0.8500 / Loss: 44.8874\n",
      "Accuracy: 0.8500 / Loss: 44.8870\n",
      "Accuracy: 0.8500 / Loss: 44.8867\n",
      "Accuracy: 0.8500 / Loss: 44.8864\n",
      "Accuracy: 0.8500 / Loss: 44.8861\n",
      "Accuracy: 0.8500 / Loss: 44.8858\n",
      "Accuracy: 0.8500 / Loss: 44.8855\n",
      "Accuracy: 0.8500 / Loss: 44.8851\n",
      "Accuracy: 0.8500 / Loss: 44.8848\n",
      "Accuracy: 0.8500 / Loss: 44.8845\n",
      "Accuracy: 0.8500 / Loss: 44.8842\n",
      "Accuracy: 0.8500 / Loss: 44.8839\n",
      "Accuracy: 0.8500 / Loss: 44.8836\n",
      "Accuracy: 0.8500 / Loss: 44.8833\n",
      "Accuracy: 0.8500 / Loss: 44.8830\n",
      "Accuracy: 0.8500 / Loss: 44.8827\n",
      "Accuracy: 0.8500 / Loss: 44.8824\n",
      "Accuracy: 0.8500 / Loss: 44.8820\n",
      "Accuracy: 0.8500 / Loss: 44.8817\n",
      "Accuracy: 0.8500 / Loss: 44.8814\n",
      "Accuracy: 0.8500 / Loss: 44.8811\n",
      "Accuracy: 0.8500 / Loss: 44.8808\n",
      "Accuracy: 0.8500 / Loss: 44.8805\n",
      "Accuracy: 0.8500 / Loss: 44.8802\n",
      "Accuracy: 0.8500 / Loss: 44.8799\n",
      "Accuracy: 0.8500 / Loss: 44.8796\n",
      "Accuracy: 0.8500 / Loss: 44.8793\n",
      "Accuracy: 0.8500 / Loss: 44.8790\n",
      "Accuracy: 0.8500 / Loss: 44.8787\n",
      "Accuracy: 0.8500 / Loss: 44.8784\n",
      "Accuracy: 0.8500 / Loss: 44.8781\n",
      "Accuracy: 0.8500 / Loss: 44.8778\n",
      "Accuracy: 0.8500 / Loss: 44.8775\n",
      "Accuracy: 0.8500 / Loss: 44.8772\n",
      "Accuracy: 0.8500 / Loss: 44.8769\n",
      "Accuracy: 0.8500 / Loss: 44.8766\n",
      "Accuracy: 0.8500 / Loss: 44.8763\n",
      "Accuracy: 0.8500 / Loss: 44.8760\n",
      "Accuracy: 0.8500 / Loss: 44.8757\n",
      "Accuracy: 0.8500 / Loss: 44.8754\n",
      "Accuracy: 0.8500 / Loss: 44.8751\n",
      "Accuracy: 0.8500 / Loss: 44.8748\n",
      "Accuracy: 0.8500 / Loss: 44.8746\n",
      "Accuracy: 0.8500 / Loss: 44.8743\n",
      "Accuracy: 0.8500 / Loss: 44.8740\n",
      "Accuracy: 0.8500 / Loss: 44.8737\n",
      "Accuracy: 0.8600 / Loss: 44.8734\n",
      "Accuracy: 0.8600 / Loss: 44.8731\n",
      "Accuracy: 0.8600 / Loss: 44.8728\n",
      "Accuracy: 0.8600 / Loss: 44.8725\n",
      "Accuracy: 0.8600 / Loss: 44.8722\n",
      "Accuracy: 0.8600 / Loss: 44.8719\n",
      "Accuracy: 0.8600 / Loss: 44.8717\n",
      "Accuracy: 0.8600 / Loss: 44.8714\n",
      "Accuracy: 0.8600 / Loss: 44.8711\n",
      "Accuracy: 0.8600 / Loss: 44.8708\n",
      "Accuracy: 0.8600 / Loss: 44.8705\n",
      "Accuracy: 0.8600 / Loss: 44.8702\n",
      "Accuracy: 0.8600 / Loss: 44.8700\n",
      "Accuracy: 0.8600 / Loss: 44.8697\n",
      "Accuracy: 0.8600 / Loss: 44.8694\n",
      "Accuracy: 0.8600 / Loss: 44.8691\n",
      "Accuracy: 0.8600 / Loss: 44.8688\n",
      "Accuracy: 0.8600 / Loss: 44.8685\n",
      "Accuracy: 0.8600 / Loss: 44.8683\n",
      "Accuracy: 0.8600 / Loss: 44.8680\n",
      "Accuracy: 0.8600 / Loss: 44.8677\n",
      "Accuracy: 0.8600 / Loss: 44.8674\n",
      "Accuracy: 0.8600 / Loss: 44.8672\n",
      "Accuracy: 0.8600 / Loss: 44.8669\n",
      "Accuracy: 0.8600 / Loss: 44.8666\n",
      "Accuracy: 0.8600 / Loss: 44.8663\n",
      "Accuracy: 0.8600 / Loss: 44.8660\n",
      "Accuracy: 0.8600 / Loss: 44.8658\n",
      "Accuracy: 0.8600 / Loss: 44.8655\n",
      "Accuracy: 0.8600 / Loss: 44.8652\n",
      "Accuracy: 0.8600 / Loss: 44.8650\n",
      "Accuracy: 0.8600 / Loss: 44.8647\n",
      "Accuracy: 0.8600 / Loss: 44.8644\n",
      "Accuracy: 0.8600 / Loss: 44.8641\n",
      "Accuracy: 0.8600 / Loss: 44.8639\n",
      "Accuracy: 0.8600 / Loss: 44.8636\n",
      "Accuracy: 0.8600 / Loss: 44.8633\n",
      "Accuracy: 0.8600 / Loss: 44.8631\n",
      "Accuracy: 0.8600 / Loss: 44.8628\n",
      "Accuracy: 0.8600 / Loss: 44.8625\n",
      "Accuracy: 0.8600 / Loss: 44.8623\n",
      "Accuracy: 0.8600 / Loss: 44.8620\n",
      "Accuracy: 0.8600 / Loss: 44.8617\n",
      "Accuracy: 0.8600 / Loss: 44.8615\n",
      "Accuracy: 0.8600 / Loss: 44.8612\n",
      "Accuracy: 0.8600 / Loss: 44.8609\n",
      "Accuracy: 0.8600 / Loss: 44.8607\n",
      "Accuracy: 0.8600 / Loss: 44.8604\n",
      "Accuracy: 0.8600 / Loss: 44.8601\n",
      "Accuracy: 0.8600 / Loss: 44.8599\n",
      "Accuracy: 0.8600 / Loss: 44.8596\n",
      "Accuracy: 0.8600 / Loss: 44.8594\n",
      "Accuracy: 0.8600 / Loss: 44.8591\n",
      "Accuracy: 0.8600 / Loss: 44.8588\n",
      "Accuracy: 0.8600 / Loss: 44.8586\n",
      "Accuracy: 0.8600 / Loss: 44.8583\n",
      "Accuracy: 0.8600 / Loss: 44.8581\n",
      "Accuracy: 0.8600 / Loss: 44.8578\n",
      "Accuracy: 0.8600 / Loss: 44.8575\n",
      "Accuracy: 0.8600 / Loss: 44.8573\n",
      "Accuracy: 0.8600 / Loss: 44.8570\n",
      "Accuracy: 0.8600 / Loss: 44.8568\n",
      "Accuracy: 0.8600 / Loss: 44.8565\n",
      "Accuracy: 0.8600 / Loss: 44.8563\n",
      "Accuracy: 0.8600 / Loss: 44.8560\n",
      "Accuracy: 0.8600 / Loss: 44.8558\n",
      "Accuracy: 0.8600 / Loss: 44.8555\n",
      "Accuracy: 0.8600 / Loss: 44.8553\n",
      "Accuracy: 0.8600 / Loss: 44.8550\n",
      "Accuracy: 0.8600 / Loss: 44.8548\n",
      "Accuracy: 0.8600 / Loss: 44.8545\n",
      "Accuracy: 0.8600 / Loss: 44.8543\n",
      "Accuracy: 0.8600 / Loss: 44.8540\n",
      "Accuracy: 0.8600 / Loss: 44.8538\n",
      "Accuracy: 0.8600 / Loss: 44.8535\n",
      "Accuracy: 0.8600 / Loss: 44.8533\n",
      "Accuracy: 0.8600 / Loss: 44.8530\n",
      "Accuracy: 0.8600 / Loss: 44.8528\n",
      "Accuracy: 0.8600 / Loss: 44.8525\n",
      "Accuracy: 0.8600 / Loss: 44.8523\n",
      "Accuracy: 0.8600 / Loss: 44.8520\n",
      "Accuracy: 0.8600 / Loss: 44.8518\n",
      "Accuracy: 0.8600 / Loss: 44.8516\n",
      "Accuracy: 0.8600 / Loss: 44.8513\n",
      "Accuracy: 0.8600 / Loss: 44.8511\n",
      "Accuracy: 0.8600 / Loss: 44.8508\n",
      "Accuracy: 0.8600 / Loss: 44.8506\n",
      "Accuracy: 0.8600 / Loss: 44.8503\n",
      "Accuracy: 0.8600 / Loss: 44.8501\n",
      "Accuracy: 0.8600 / Loss: 44.8499\n",
      "Accuracy: 0.8600 / Loss: 44.8496\n",
      "Accuracy: 0.8600 / Loss: 44.8494\n",
      "Accuracy: 0.8600 / Loss: 44.8492\n",
      "Accuracy: 0.8600 / Loss: 44.8489\n",
      "Accuracy: 0.8600 / Loss: 44.8487\n",
      "Accuracy: 0.8600 / Loss: 44.8484\n",
      "Accuracy: 0.8600 / Loss: 44.8482\n",
      "Accuracy: 0.8600 / Loss: 44.8480\n",
      "Accuracy: 0.8600 / Loss: 44.8477\n",
      "Accuracy: 0.8600 / Loss: 44.8475\n",
      "Accuracy: 0.8600 / Loss: 44.8473\n",
      "Accuracy: 0.8600 / Loss: 44.8470\n",
      "Accuracy: 0.8600 / Loss: 44.8468\n",
      "Accuracy: 0.8600 / Loss: 44.8466\n",
      "Accuracy: 0.8600 / Loss: 44.8463\n",
      "Accuracy: 0.8600 / Loss: 44.8461\n",
      "Accuracy: 0.8600 / Loss: 44.8459\n",
      "Accuracy: 0.8600 / Loss: 44.8457\n",
      "Accuracy: 0.8600 / Loss: 44.8454\n",
      "Accuracy: 0.8600 / Loss: 44.8452\n",
      "Accuracy: 0.8600 / Loss: 44.8450\n",
      "Accuracy: 0.8600 / Loss: 44.8447\n",
      "Accuracy: 0.8600 / Loss: 44.8445\n",
      "Accuracy: 0.8600 / Loss: 44.8443\n",
      "Accuracy: 0.8600 / Loss: 44.8441\n",
      "Accuracy: 0.8600 / Loss: 44.8438\n",
      "Accuracy: 0.8600 / Loss: 44.8436\n",
      "Accuracy: 0.8600 / Loss: 44.8434\n",
      "Accuracy: 0.8600 / Loss: 44.8432\n",
      "Accuracy: 0.8600 / Loss: 44.8429\n",
      "Accuracy: 0.8600 / Loss: 44.8427\n",
      "Accuracy: 0.8600 / Loss: 44.8425\n",
      "Accuracy: 0.8600 / Loss: 44.8423\n",
      "Accuracy: 0.8600 / Loss: 44.8421\n",
      "Accuracy: 0.8600 / Loss: 44.8418\n",
      "Accuracy: 0.8600 / Loss: 44.8416\n",
      "Accuracy: 0.8600 / Loss: 44.8414\n",
      "Accuracy: 0.8600 / Loss: 44.8412\n",
      "Accuracy: 0.8600 / Loss: 44.8410\n",
      "Accuracy: 0.8600 / Loss: 44.8408\n",
      "Accuracy: 0.8600 / Loss: 44.8405\n",
      "Accuracy: 0.8600 / Loss: 44.8403\n",
      "Accuracy: 0.8600 / Loss: 44.8401\n",
      "Accuracy: 0.8600 / Loss: 44.8399\n",
      "Accuracy: 0.8600 / Loss: 44.8397\n",
      "Accuracy: 0.8600 / Loss: 44.8395\n",
      "Accuracy: 0.8600 / Loss: 44.8392\n",
      "Accuracy: 0.8600 / Loss: 44.8390\n",
      "Accuracy: 0.8600 / Loss: 44.8388\n",
      "Accuracy: 0.8600 / Loss: 44.8386\n",
      "Accuracy: 0.8600 / Loss: 44.8384\n",
      "Accuracy: 0.8600 / Loss: 44.8382\n",
      "Accuracy: 0.8600 / Loss: 44.8380\n",
      "Accuracy: 0.8600 / Loss: 44.8378\n",
      "Accuracy: 0.8600 / Loss: 44.8376\n",
      "Accuracy: 0.8600 / Loss: 44.8374\n",
      "Accuracy: 0.8600 / Loss: 44.8371\n",
      "Accuracy: 0.8600 / Loss: 44.8369\n",
      "Accuracy: 0.8600 / Loss: 44.8367\n",
      "Accuracy: 0.8600 / Loss: 44.8365\n",
      "Accuracy: 0.8600 / Loss: 44.8363\n",
      "Accuracy: 0.8600 / Loss: 44.8361\n",
      "Accuracy: 0.8600 / Loss: 44.8359\n",
      "Accuracy: 0.8600 / Loss: 44.8357\n",
      "Accuracy: 0.8600 / Loss: 44.8355\n",
      "Accuracy: 0.8600 / Loss: 44.8353\n",
      "Accuracy: 0.8600 / Loss: 44.8351\n",
      "Accuracy: 0.8600 / Loss: 44.8349\n",
      "Accuracy: 0.8600 / Loss: 44.8347\n",
      "Accuracy: 0.8600 / Loss: 44.8345\n",
      "Accuracy: 0.8600 / Loss: 44.8343\n",
      "Accuracy: 0.8600 / Loss: 44.8341\n",
      "Accuracy: 0.8600 / Loss: 44.8339\n",
      "Accuracy: 0.8600 / Loss: 44.8337\n",
      "Accuracy: 0.8600 / Loss: 44.8335\n",
      "Accuracy: 0.8600 / Loss: 44.8333\n",
      "Accuracy: 0.8600 / Loss: 44.8331\n",
      "Accuracy: 0.8600 / Loss: 44.8329\n",
      "Accuracy: 0.8600 / Loss: 44.8327\n",
      "Accuracy: 0.8600 / Loss: 44.8325\n",
      "Accuracy: 0.8600 / Loss: 44.8323\n",
      "Accuracy: 0.8600 / Loss: 44.8321\n",
      "Accuracy: 0.8600 / Loss: 44.8319\n",
      "Accuracy: 0.8600 / Loss: 44.8317\n",
      "Accuracy: 0.8600 / Loss: 44.8315\n",
      "Accuracy: 0.8600 / Loss: 44.8313\n",
      "Accuracy: 0.8600 / Loss: 44.8312\n",
      "Accuracy: 0.8600 / Loss: 44.8310\n",
      "Accuracy: 0.8600 / Loss: 44.8308\n",
      "Accuracy: 0.8600 / Loss: 44.8306\n",
      "Accuracy: 0.8600 / Loss: 44.8304\n",
      "Accuracy: 0.8600 / Loss: 44.8302\n",
      "Accuracy: 0.8600 / Loss: 44.8300\n",
      "Accuracy: 0.8600 / Loss: 44.8298\n",
      "Accuracy: 0.8600 / Loss: 44.8296\n",
      "Accuracy: 0.8600 / Loss: 44.8294\n",
      "Accuracy: 0.8600 / Loss: 44.8293\n",
      "Accuracy: 0.8600 / Loss: 44.8291\n",
      "Accuracy: 0.8600 / Loss: 44.8289\n",
      "Accuracy: 0.8600 / Loss: 44.8287\n",
      "Accuracy: 0.8600 / Loss: 44.8285\n",
      "Accuracy: 0.8600 / Loss: 44.8283\n",
      "Accuracy: 0.8600 / Loss: 44.8281\n",
      "Accuracy: 0.8600 / Loss: 44.8280\n",
      "Accuracy: 0.8600 / Loss: 44.8278\n",
      "Accuracy: 0.8600 / Loss: 44.8276\n",
      "Accuracy: 0.8600 / Loss: 44.8274\n",
      "Accuracy: 0.8600 / Loss: 44.8272\n",
      "Accuracy: 0.8600 / Loss: 44.8271\n",
      "Accuracy: 0.8600 / Loss: 44.8269\n",
      "Accuracy: 0.8600 / Loss: 44.8267\n",
      "Accuracy: 0.8600 / Loss: 44.8265\n",
      "Accuracy: 0.8600 / Loss: 44.8263\n",
      "Accuracy: 0.8600 / Loss: 44.8262\n",
      "Accuracy: 0.8600 / Loss: 44.8260\n",
      "Accuracy: 0.8600 / Loss: 44.8258\n",
      "Accuracy: 0.8600 / Loss: 44.8256\n",
      "Accuracy: 0.8600 / Loss: 44.8255\n",
      "Accuracy: 0.8600 / Loss: 44.8253\n",
      "Accuracy: 0.8600 / Loss: 44.8251\n",
      "Accuracy: 0.8600 / Loss: 44.8249\n",
      "Accuracy: 0.8600 / Loss: 44.8248\n",
      "Accuracy: 0.8600 / Loss: 44.8246\n",
      "Accuracy: 0.8600 / Loss: 44.8244\n",
      "Accuracy: 0.8600 / Loss: 44.8242\n",
      "Accuracy: 0.8600 / Loss: 44.8241\n",
      "Accuracy: 0.8600 / Loss: 44.8239\n",
      "Accuracy: 0.8600 / Loss: 44.8237\n",
      "Accuracy: 0.8600 / Loss: 44.8235\n",
      "Accuracy: 0.8600 / Loss: 44.8234\n",
      "Accuracy: 0.8600 / Loss: 44.8232\n",
      "Accuracy: 0.8600 / Loss: 44.8230\n",
      "Accuracy: 0.8600 / Loss: 44.8229\n",
      "Accuracy: 0.8600 / Loss: 44.8227\n",
      "Accuracy: 0.8600 / Loss: 44.8225\n",
      "Accuracy: 0.8600 / Loss: 44.8224\n",
      "Accuracy: 0.8600 / Loss: 44.8222\n",
      "Accuracy: 0.8600 / Loss: 44.8220\n",
      "Accuracy: 0.8600 / Loss: 44.8219\n",
      "Accuracy: 0.8600 / Loss: 44.8217\n",
      "Accuracy: 0.8600 / Loss: 44.8215\n",
      "Accuracy: 0.8600 / Loss: 44.8214\n",
      "Accuracy: 0.8600 / Loss: 44.8212\n",
      "Accuracy: 0.8600 / Loss: 44.8210\n",
      "Accuracy: 0.8600 / Loss: 44.8209\n",
      "Accuracy: 0.8600 / Loss: 44.8207\n",
      "Accuracy: 0.8600 / Loss: 44.8206\n",
      "Accuracy: 0.8600 / Loss: 44.8204\n",
      "Accuracy: 0.8600 / Loss: 44.8202\n",
      "Accuracy: 0.8600 / Loss: 44.8201\n",
      "Accuracy: 0.8600 / Loss: 44.8199\n",
      "Accuracy: 0.8600 / Loss: 44.8198\n",
      "Accuracy: 0.8600 / Loss: 44.8196\n",
      "Accuracy: 0.8600 / Loss: 44.8194\n",
      "Accuracy: 0.8600 / Loss: 44.8193\n",
      "Accuracy: 0.8600 / Loss: 44.8191\n",
      "Accuracy: 0.8600 / Loss: 44.8190\n",
      "Accuracy: 0.8600 / Loss: 44.8188\n",
      "Accuracy: 0.8600 / Loss: 44.8187\n",
      "Accuracy: 0.8600 / Loss: 44.8185\n",
      "Accuracy: 0.8600 / Loss: 44.8183\n",
      "Accuracy: 0.8600 / Loss: 44.8182\n",
      "Accuracy: 0.8600 / Loss: 44.8180\n",
      "Accuracy: 0.8600 / Loss: 44.8179\n",
      "Accuracy: 0.8600 / Loss: 44.8177\n",
      "Accuracy: 0.8600 / Loss: 44.8176\n",
      "Accuracy: 0.8600 / Loss: 44.8174\n",
      "Accuracy: 0.8600 / Loss: 44.8173\n",
      "Accuracy: 0.8600 / Loss: 44.8171\n",
      "Accuracy: 0.8600 / Loss: 44.8170\n",
      "Accuracy: 0.8600 / Loss: 44.8168\n",
      "Accuracy: 0.8600 / Loss: 44.8167\n",
      "Accuracy: 0.8600 / Loss: 44.8165\n",
      "Accuracy: 0.8600 / Loss: 44.8164\n",
      "Accuracy: 0.8600 / Loss: 44.8162\n",
      "Accuracy: 0.8600 / Loss: 44.8161\n",
      "Accuracy: 0.8600 / Loss: 44.8159\n",
      "Accuracy: 0.8600 / Loss: 44.8158\n",
      "Accuracy: 0.8600 / Loss: 44.8156\n",
      "Accuracy: 0.8600 / Loss: 44.8155\n",
      "Accuracy: 0.8600 / Loss: 44.8153\n",
      "Accuracy: 0.8600 / Loss: 44.8152\n",
      "Accuracy: 0.8600 / Loss: 44.8150\n",
      "Accuracy: 0.8600 / Loss: 44.8149\n",
      "Accuracy: 0.8600 / Loss: 44.8148\n",
      "Accuracy: 0.8600 / Loss: 44.8146\n",
      "Accuracy: 0.8600 / Loss: 44.8145\n",
      "Accuracy: 0.8600 / Loss: 44.8143\n",
      "Accuracy: 0.8600 / Loss: 44.8142\n",
      "Accuracy: 0.8600 / Loss: 44.8140\n",
      "Accuracy: 0.8600 / Loss: 44.8139\n",
      "Accuracy: 0.8600 / Loss: 44.8138\n",
      "Accuracy: 0.8600 / Loss: 44.8136\n",
      "Accuracy: 0.8600 / Loss: 44.8135\n",
      "Accuracy: 0.8600 / Loss: 44.8133\n",
      "Accuracy: 0.8600 / Loss: 44.8132\n",
      "Accuracy: 0.8600 / Loss: 44.8131\n",
      "Accuracy: 0.8600 / Loss: 44.8129\n",
      "Accuracy: 0.8600 / Loss: 44.8128\n",
      "Accuracy: 0.8600 / Loss: 44.8126\n",
      "Accuracy: 0.8600 / Loss: 44.8125\n",
      "Accuracy: 0.8600 / Loss: 44.8124\n",
      "Accuracy: 0.8600 / Loss: 44.8122\n",
      "Accuracy: 0.8600 / Loss: 44.8121\n",
      "Accuracy: 0.8600 / Loss: 44.8120\n",
      "Accuracy: 0.8600 / Loss: 44.8118\n",
      "Accuracy: 0.8600 / Loss: 44.8117\n",
      "Accuracy: 0.8600 / Loss: 44.8116\n",
      "Accuracy: 0.8600 / Loss: 44.8114\n",
      "Accuracy: 0.8600 / Loss: 44.8113\n",
      "Accuracy: 0.8600 / Loss: 44.8112\n",
      "Accuracy: 0.8600 / Loss: 44.8110\n",
      "Accuracy: 0.8600 / Loss: 44.8109\n",
      "Accuracy: 0.8600 / Loss: 44.8108\n",
      "Accuracy: 0.8600 / Loss: 44.8106\n",
      "Accuracy: 0.8600 / Loss: 44.8105\n",
      "Accuracy: 0.8600 / Loss: 44.8104\n",
      "Accuracy: 0.8600 / Loss: 44.8102\n",
      "Accuracy: 0.8600 / Loss: 44.8101\n",
      "Accuracy: 0.8600 / Loss: 44.8100\n",
      "Accuracy: 0.8600 / Loss: 44.8098\n",
      "Accuracy: 0.8600 / Loss: 44.8097\n",
      "Accuracy: 0.8600 / Loss: 44.8096\n",
      "Accuracy: 0.8600 / Loss: 44.8095\n",
      "Accuracy: 0.8600 / Loss: 44.8093\n",
      "Accuracy: 0.8600 / Loss: 44.8092\n",
      "Accuracy: 0.8600 / Loss: 44.8091\n",
      "Accuracy: 0.8600 / Loss: 44.8089\n",
      "Accuracy: 0.8600 / Loss: 44.8088\n",
      "Accuracy: 0.8600 / Loss: 44.8087\n",
      "Accuracy: 0.8600 / Loss: 44.8086\n",
      "Accuracy: 0.8600 / Loss: 44.8084\n",
      "Accuracy: 0.8600 / Loss: 44.8083\n",
      "Accuracy: 0.8600 / Loss: 44.8082\n",
      "Accuracy: 0.8600 / Loss: 44.8081\n",
      "Accuracy: 0.8600 / Loss: 44.8080\n",
      "Accuracy: 0.8600 / Loss: 44.8078\n",
      "Accuracy: 0.8600 / Loss: 44.8077\n",
      "Accuracy: 0.8600 / Loss: 44.8076\n",
      "Accuracy: 0.8600 / Loss: 44.8075\n",
      "Accuracy: 0.8600 / Loss: 44.8073\n",
      "Accuracy: 0.8600 / Loss: 44.8072\n",
      "Accuracy: 0.8600 / Loss: 44.8071\n",
      "Accuracy: 0.8600 / Loss: 44.8070\n",
      "Accuracy: 0.8600 / Loss: 44.8069\n",
      "Accuracy: 0.8600 / Loss: 44.8067\n",
      "Accuracy: 0.8600 / Loss: 44.8066\n",
      "Accuracy: 0.8600 / Loss: 44.8065\n",
      "Accuracy: 0.8600 / Loss: 44.8064\n",
      "Accuracy: 0.8600 / Loss: 44.8063\n",
      "Accuracy: 0.8600 / Loss: 44.8061\n",
      "Accuracy: 0.8600 / Loss: 44.8060\n",
      "Accuracy: 0.8600 / Loss: 44.8059\n",
      "Accuracy: 0.8600 / Loss: 44.8058\n",
      "Accuracy: 0.8600 / Loss: 44.8057\n",
      "Accuracy: 0.8600 / Loss: 44.8056\n",
      "Accuracy: 0.8600 / Loss: 44.8055\n",
      "Accuracy: 0.8600 / Loss: 44.8053\n",
      "Accuracy: 0.8600 / Loss: 44.8052\n",
      "Accuracy: 0.8600 / Loss: 44.8051\n",
      "Accuracy: 0.8600 / Loss: 44.8050\n",
      "Accuracy: 0.8600 / Loss: 44.8049\n",
      "Accuracy: 0.8600 / Loss: 44.8048\n",
      "Accuracy: 0.8600 / Loss: 44.8047\n",
      "Accuracy: 0.8600 / Loss: 44.8045\n",
      "Accuracy: 0.8600 / Loss: 44.8044\n",
      "Accuracy: 0.8600 / Loss: 44.8043\n",
      "Accuracy: 0.8600 / Loss: 44.8042\n",
      "Accuracy: 0.8600 / Loss: 44.8041\n",
      "Accuracy: 0.8600 / Loss: 44.8040\n",
      "Accuracy: 0.8600 / Loss: 44.8039\n",
      "Accuracy: 0.8600 / Loss: 44.8038\n",
      "Accuracy: 0.8600 / Loss: 44.8037\n",
      "Accuracy: 0.8600 / Loss: 44.8035\n",
      "Accuracy: 0.8600 / Loss: 44.8034\n",
      "Accuracy: 0.8600 / Loss: 44.8033\n",
      "Accuracy: 0.8600 / Loss: 44.8032\n",
      "Accuracy: 0.8600 / Loss: 44.8031\n",
      "Accuracy: 0.8600 / Loss: 44.8030\n",
      "Accuracy: 0.8600 / Loss: 44.8029\n",
      "Accuracy: 0.8600 / Loss: 44.8028\n",
      "Accuracy: 0.8600 / Loss: 44.8027\n",
      "Accuracy: 0.8600 / Loss: 44.8026\n",
      "Accuracy: 0.8600 / Loss: 44.8025\n",
      "Accuracy: 0.8600 / Loss: 44.8024\n",
      "Accuracy: 0.8600 / Loss: 44.8023\n",
      "Accuracy: 0.8600 / Loss: 44.8021\n",
      "Accuracy: 0.8600 / Loss: 44.8020\n",
      "Accuracy: 0.8600 / Loss: 44.8019\n",
      "Accuracy: 0.8600 / Loss: 44.8018\n",
      "Accuracy: 0.8600 / Loss: 44.8017\n",
      "Accuracy: 0.8600 / Loss: 44.8016\n",
      "Accuracy: 0.8600 / Loss: 44.8015\n",
      "Accuracy: 0.8600 / Loss: 44.8014\n",
      "Accuracy: 0.8600 / Loss: 44.8013\n",
      "Accuracy: 0.8600 / Loss: 44.8012\n",
      "Accuracy: 0.8600 / Loss: 44.8011\n",
      "Accuracy: 0.8600 / Loss: 44.8010\n",
      "Accuracy: 0.8600 / Loss: 44.8009\n",
      "Accuracy: 0.8600 / Loss: 44.8008\n",
      "Accuracy: 0.8600 / Loss: 44.8007\n",
      "Accuracy: 0.8600 / Loss: 44.8006\n",
      "Accuracy: 0.8600 / Loss: 44.8005\n",
      "Accuracy: 0.8600 / Loss: 44.8004\n",
      "Accuracy: 0.8600 / Loss: 44.8003\n",
      "Accuracy: 0.8600 / Loss: 44.8002\n",
      "Accuracy: 0.8600 / Loss: 44.8001\n",
      "Accuracy: 0.8600 / Loss: 44.8000\n",
      "Accuracy: 0.8600 / Loss: 44.7999\n",
      "Accuracy: 0.8600 / Loss: 44.7998\n",
      "Accuracy: 0.8600 / Loss: 44.7997\n",
      "Accuracy: 0.8600 / Loss: 44.7996\n",
      "Accuracy: 0.8600 / Loss: 44.7995\n",
      "Accuracy: 0.8600 / Loss: 44.7994\n",
      "Accuracy: 0.8600 / Loss: 44.7993\n",
      "Accuracy: 0.8600 / Loss: 44.7992\n",
      "Accuracy: 0.8600 / Loss: 44.7991\n",
      "Accuracy: 0.8600 / Loss: 44.7990\n",
      "Accuracy: 0.8600 / Loss: 44.7989\n",
      "Accuracy: 0.8600 / Loss: 44.7988\n",
      "Accuracy: 0.8600 / Loss: 44.7988\n",
      "Accuracy: 0.8600 / Loss: 44.7987\n",
      "Accuracy: 0.8600 / Loss: 44.7986\n",
      "Accuracy: 0.8600 / Loss: 44.7985\n",
      "Accuracy: 0.8600 / Loss: 44.7984\n",
      "Accuracy: 0.8600 / Loss: 44.7983\n",
      "Accuracy: 0.8600 / Loss: 44.7982\n",
      "Accuracy: 0.8600 / Loss: 44.7981\n",
      "Accuracy: 0.8600 / Loss: 44.7980\n",
      "Accuracy: 0.8600 / Loss: 44.7979\n",
      "Accuracy: 0.8600 / Loss: 44.7978\n",
      "Accuracy: 0.8600 / Loss: 44.7977\n",
      "Accuracy: 0.8600 / Loss: 44.7976\n",
      "Accuracy: 0.8600 / Loss: 44.7975\n",
      "Accuracy: 0.8600 / Loss: 44.7974\n",
      "Accuracy: 0.8600 / Loss: 44.7974\n",
      "Accuracy: 0.8600 / Loss: 44.7973\n",
      "Accuracy: 0.8600 / Loss: 44.7972\n",
      "Accuracy: 0.8600 / Loss: 44.7971\n",
      "Accuracy: 0.8600 / Loss: 44.7970\n",
      "Accuracy: 0.8600 / Loss: 44.7969\n",
      "Accuracy: 0.8600 / Loss: 44.7968\n",
      "Accuracy: 0.8600 / Loss: 44.7967\n",
      "Accuracy: 0.8600 / Loss: 44.7966\n",
      "Accuracy: 0.8600 / Loss: 44.7965\n",
      "Accuracy: 0.8600 / Loss: 44.7965\n",
      "Accuracy: 0.8600 / Loss: 44.7964\n",
      "Accuracy: 0.8600 / Loss: 44.7963\n",
      "Accuracy: 0.8600 / Loss: 44.7962\n",
      "Accuracy: 0.8600 / Loss: 44.7961\n",
      "Accuracy: 0.8600 / Loss: 44.7960\n",
      "Accuracy: 0.8600 / Loss: 44.7959\n",
      "Accuracy: 0.8600 / Loss: 44.7958\n",
      "Accuracy: 0.8600 / Loss: 44.7958\n",
      "Accuracy: 0.8600 / Loss: 44.7957\n",
      "Accuracy: 0.8600 / Loss: 44.7956\n",
      "Accuracy: 0.8600 / Loss: 44.7955\n",
      "Accuracy: 0.8600 / Loss: 44.7954\n",
      "Accuracy: 0.8600 / Loss: 44.7953\n",
      "Accuracy: 0.8600 / Loss: 44.7952\n",
      "Accuracy: 0.8600 / Loss: 44.7952\n",
      "Accuracy: 0.8600 / Loss: 44.7951\n",
      "Accuracy: 0.8600 / Loss: 44.7950\n",
      "Accuracy: 0.8600 / Loss: 44.7949\n",
      "Accuracy: 0.8600 / Loss: 44.7948\n",
      "Accuracy: 0.8600 / Loss: 44.7947\n",
      "Accuracy: 0.8600 / Loss: 44.7947\n",
      "Accuracy: 0.8600 / Loss: 44.7946\n",
      "Accuracy: 0.8600 / Loss: 44.7945\n",
      "Accuracy: 0.8600 / Loss: 44.7944\n",
      "Accuracy: 0.8600 / Loss: 44.7943\n",
      "Accuracy: 0.8600 / Loss: 44.7942\n",
      "Accuracy: 0.8600 / Loss: 44.7942\n",
      "Accuracy: 0.8600 / Loss: 44.7941\n",
      "Accuracy: 0.8600 / Loss: 44.7940\n",
      "Accuracy: 0.8600 / Loss: 44.7939\n",
      "Accuracy: 0.8600 / Loss: 44.7938\n",
      "Accuracy: 0.8600 / Loss: 44.7937\n",
      "Accuracy: 0.8600 / Loss: 44.7937\n",
      "Accuracy: 0.8600 / Loss: 44.7936\n",
      "Accuracy: 0.8600 / Loss: 44.7935\n",
      "Accuracy: 0.8600 / Loss: 44.7934\n",
      "Accuracy: 0.8600 / Loss: 44.7933\n",
      "Accuracy: 0.8600 / Loss: 44.7933\n",
      "Accuracy: 0.8600 / Loss: 44.7932\n",
      "Accuracy: 0.8600 / Loss: 44.7931\n",
      "Accuracy: 0.8600 / Loss: 44.7930\n",
      "Accuracy: 0.8600 / Loss: 44.7929\n",
      "Accuracy: 0.8600 / Loss: 44.7929\n",
      "Accuracy: 0.8600 / Loss: 44.7928\n",
      "Accuracy: 0.8600 / Loss: 44.7927\n",
      "Accuracy: 0.8600 / Loss: 44.7926\n",
      "Accuracy: 0.8600 / Loss: 44.7925\n",
      "Accuracy: 0.8600 / Loss: 44.7925\n",
      "Accuracy: 0.8600 / Loss: 44.7924\n",
      "Accuracy: 0.8600 / Loss: 44.7923\n",
      "Accuracy: 0.8600 / Loss: 44.7922\n",
      "Accuracy: 0.8600 / Loss: 44.7922\n",
      "Accuracy: 0.8600 / Loss: 44.7921\n",
      "Accuracy: 0.8600 / Loss: 44.7920\n",
      "Accuracy: 0.8600 / Loss: 44.7919\n",
      "Accuracy: 0.8600 / Loss: 44.7919\n",
      "Accuracy: 0.8600 / Loss: 44.7918\n",
      "Accuracy: 0.8600 / Loss: 44.7917\n",
      "Accuracy: 0.8600 / Loss: 44.7916\n",
      "Accuracy: 0.8600 / Loss: 44.7915\n",
      "Accuracy: 0.8600 / Loss: 44.7915\n",
      "Accuracy: 0.8600 / Loss: 44.7914\n",
      "Accuracy: 0.8600 / Loss: 44.7913\n",
      "Accuracy: 0.8600 / Loss: 44.7912\n",
      "Accuracy: 0.8600 / Loss: 44.7912\n",
      "Accuracy: 0.8600 / Loss: 44.7911\n",
      "Accuracy: 0.8600 / Loss: 44.7910\n",
      "Accuracy: 0.8600 / Loss: 44.7909\n",
      "Accuracy: 0.8600 / Loss: 44.7909\n",
      "Accuracy: 0.8600 / Loss: 44.7908\n",
      "Accuracy: 0.8600 / Loss: 44.7907\n",
      "Accuracy: 0.8600 / Loss: 44.7906\n",
      "Accuracy: 0.8600 / Loss: 44.7906\n",
      "Accuracy: 0.8600 / Loss: 44.7905\n",
      "Accuracy: 0.8600 / Loss: 44.7904\n",
      "Accuracy: 0.8600 / Loss: 44.7904\n",
      "Accuracy: 0.8600 / Loss: 44.7903\n",
      "Accuracy: 0.8600 / Loss: 44.7902\n",
      "Accuracy: 0.8600 / Loss: 44.7901\n",
      "Accuracy: 0.8600 / Loss: 44.7901\n",
      "Accuracy: 0.8600 / Loss: 44.7900\n",
      "Accuracy: 0.8600 / Loss: 44.7899\n",
      "Accuracy: 0.8600 / Loss: 44.7898\n",
      "Accuracy: 0.8600 / Loss: 44.7898\n",
      "Accuracy: 0.8600 / Loss: 44.7897\n",
      "Accuracy: 0.8600 / Loss: 44.7896\n",
      "Accuracy: 0.8600 / Loss: 44.7896\n",
      "Accuracy: 0.8600 / Loss: 44.7895\n",
      "Accuracy: 0.8600 / Loss: 44.7894\n",
      "Accuracy: 0.8600 / Loss: 44.7893\n",
      "Accuracy: 0.8600 / Loss: 44.7893\n",
      "Accuracy: 0.8600 / Loss: 44.7892\n",
      "Accuracy: 0.8600 / Loss: 44.7891\n",
      "Accuracy: 0.8600 / Loss: 44.7891\n",
      "Accuracy: 0.8600 / Loss: 44.7890\n",
      "Accuracy: 0.8600 / Loss: 44.7889\n",
      "Accuracy: 0.8600 / Loss: 44.7888\n",
      "Accuracy: 0.8600 / Loss: 44.7888\n",
      "Accuracy: 0.8600 / Loss: 44.7887\n",
      "Accuracy: 0.8600 / Loss: 44.7886\n"
     ]
    }
   ],
   "source": [
    "import kraus_optimizer\n",
    "\n",
    "reload(kraus_optimizer)\n",
    "\n",
    "N = 16\n",
    "# mps_tpcp = tpcp_mps.MPSTPCP(N=N, K=2, d=2)\n",
    "# data = torch.randn(100, N, 2)\n",
    "# target = torch.randint(0, 2, (100,))\n",
    "optimizer = kraus_optimizer.CayleyAdam(mps_tpcp.kraus_ops, lr=0.3, betas = (0.9, 0.99), q = 0.01)\n",
    "\n",
    "for epoch in range(2000):\n",
    "    acc_tot = 0\n",
    "    loss_tot = 0\n",
    "    optimizer.zero_grad()\n",
    "    outputs = mps_tpcp.forward(data.to(torch.float64))\n",
    "    loss = loss_batch(outputs, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    acc = calculate_accuracy(outputs, target)\n",
    "    acc_tot += acc\n",
    "    loss_tot += loss.item()\n",
    "    # print(acc)\n",
    "    print(f\"Accuracy: {acc:.4f} / Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# K @ K.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5088\n",
      "Loss: 90.9863\n",
      "Accuracy: 0.5060\n",
      "Loss: 92.5615\n",
      "Accuracy: 0.4993\n",
      "Loss: 92.6686\n",
      "Accuracy: 0.5030\n",
      "Loss: 91.1258\n",
      "Accuracy: 0.5018\n",
      "Loss: 91.4921\n",
      "Accuracy: 0.5094\n",
      "Loss: 90.2015\n",
      "Accuracy: 0.5080\n",
      "Loss: 91.6243\n",
      "Accuracy: 0.5038\n",
      "Loss: 90.4939\n",
      "Accuracy: 0.5081\n",
      "Loss: 90.6116\n",
      "Accuracy: 0.5005\n",
      "Loss: 91.6860\n",
      "Accuracy: 0.5039\n",
      "Loss: 90.9248\n",
      "Accuracy: 0.5025\n",
      "Loss: 93.0975\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[394], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m mps_tpcp\u001b[38;5;241m.\u001b[39mforward(data\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat64))\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_batch(outputs, target)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m acc \u001b[38;5;241m=\u001b[39m calculate_accuracy(outputs, target)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import kraus_optimizer\n",
    "\n",
    "reload(kraus_optimizer)\n",
    "\n",
    "mps_tpcp = tpcp_mps.MPSTPCP(N=16 * 16, K=1, d=2)\n",
    "# for i in range(len(umpsm.params)):\n",
    "#     mps_tpcp.kraus_ops[i].data[:] = umpsm.params[i].reshape(4,4).T\n",
    "optimizer = kraus_optimizer.CayleySGDMomentum(mps_tpcp.kraus_ops, lr=0.01, beta=0.9, q=0.5, s=2)\n",
    "# optimizer = kraus_optimizer.CayleyAdam(mps_tpcp.kraus_ops, lr=0.005, betas = (0.9, 0.999), q = 0.5, s = 2)\n",
    "\n",
    "for epoch in range(100):\n",
    "    acc_tot = 0\n",
    "    loss_tot = 0\n",
    "    for data, target in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mps_tpcp.forward(data.to(torch.float64))\n",
    "        loss = loss_batch(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = calculate_accuracy(outputs, target)\n",
    "        acc_tot += acc\n",
    "        loss_tot += loss.item()\n",
    "        # print(acc)\n",
    "        # print(loss.item())\n",
    "        # print(mps.kraus_ops[-1].sum())\n",
    "    acc_tot /= len(trainloader)\n",
    "    loss_tot /= len(trainloader)\n",
    "    print(f\"Accuracy: {acc_tot:.4f}\")\n",
    "    print(f\"Loss: {loss_tot:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, kraus_op in enumerate(mps_tpcp.kraus_ops):\n",
    "    kraus_op.data[:] = umpsm.params[i].reshape(4,4).T\n",
    "    # print(mps_tpcp.forward(data.to(torch.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00,  3.8858e-16,  1.3878e-17, -3.5405e-16],\n",
       "        [ 3.8858e-16,  1.0000e+00, -1.2490e-16,  2.7756e-17],\n",
       "        [ 1.3878e-17, -1.2490e-16,  1.0000e+00, -1.1102e-16],\n",
       "        [-3.5405e-16,  2.7756e-17, -1.1102e-16,  1.0000e+00]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = list(mps_tpcp.kraus_ops.parameters())[0].data[:]\n",
    "\n",
    "U @ U.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.5022, -0.2951],\n",
       "          [ 0.5226,  0.6225]],\n",
       "\n",
       "         [[-0.5633, -0.2183],\n",
       "          [-0.4017,  0.6883]]],\n",
       "\n",
       "\n",
       "        [[[ 0.4872, -0.5938],\n",
       "          [-0.6219, -0.1525]],\n",
       "\n",
       "         [[ 0.4394,  0.7160],\n",
       "          [-0.4228,  0.3399]]]], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "umpsm.params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS unitaries\n",
      "Accuracy: 0.5238\n",
      "loss: 83.79042052731249\n",
      "Accuracy: 0.5281\n",
      "loss: 83.30355859970851\n",
      "Accuracy: 0.6275\n",
      "loss: 50.921918496208235\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[329], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m umpsm_op\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mumpsm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_batch(outputs, target)\n\u001b[1;32m     12\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/umps.py:246\u001b[0m, in \u001b[0;36muMPS.forward\u001b[0;34m(self, X, label)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:, :, :] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_discriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/opt_einsum/contract.py:763\u001b[0m, in \u001b[0;36mContractExpression.__call__\u001b[0;34m(self, *arrays, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backends\u001b[38;5;241m.\u001b[39mhas_backend(backend) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays):\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contract_with_conversion(ops, out, backend, evaluate_constants\u001b[38;5;241m=\u001b[39mevaluate_constants)\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_contract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate_constants\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    766\u001b[0m     original_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(err\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39margs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/opt_einsum/contract.py:693\u001b[0m, in \u001b[0;36mContractExpression._contract\u001b[0;34m(self, arrays, out, backend, evaluate_constants)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The normal, core contraction.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m contraction_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_contraction_list \u001b[38;5;28;01mif\u001b[39;00m evaluate_constants \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontraction_list\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core_contract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcontraction_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mevaluate_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate_constants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/opt_einsum/contract.py:573\u001b[0m, in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m     right_pos\u001b[38;5;241m.\u001b[39mappend(input_right\u001b[38;5;241m.\u001b[39mfind(s))\n\u001b[1;32m    572\u001b[0m \u001b[38;5;66;03m# Contract!\u001b[39;00m\n\u001b[0;32m--> 573\u001b[0m new_view \u001b[38;5;241m=\u001b[39m \u001b[43m_tensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mleft_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mright_pos\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# Build a new view if needed\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (tensor_result \u001b[38;5;241m!=\u001b[39m results_index) \u001b[38;5;129;01mor\u001b[39;00m handle_out:\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/opt_einsum/sharing.py:131\u001b[0m, in \u001b[0;36mtensordot_cache_wrap.<locals>.cached_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(tensordot)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_tensordot\u001b[39m(x, y, axes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m currently_sharing():\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# hash based on the (axes_x,axes_y) form of axes\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     _save_tensors(x, y)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/opt_einsum/contract.py:374\u001b[0m, in \u001b[0;36m_tensordot\u001b[0;34m(x, y, axes, backend)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Base tensordot.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    373\u001b[0m fn \u001b[38;5;241m=\u001b[39m backends\u001b[38;5;241m.\u001b[39mget_func(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensordot\u001b[39m\u001b[38;5;124m'\u001b[39m, backend)\n\u001b[0;32m--> 374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/opt_einsum/backends/torch.py:54\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(x, y, axes)\u001b[0m\n\u001b[1;32m     51\u001b[0m torch, _ \u001b[38;5;241m=\u001b[39m _get_torch_and_device()\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TORCH_HAS_TENSORDOT:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m xnd \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mndimension()\n\u001b[1;32m     57\u001b[0m ynd \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mndimension()\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/functional.py:1213\u001b[0m, in \u001b[0;36mtensordot\u001b[0;34m(a, b, dims, out)\u001b[0m\n\u001b[1;32m   1210\u001b[0m     dims_b \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(dims))\n\u001b[1;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensordot\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims_b\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mtensordot(a, b, dims_a, dims_b, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "umpsm = umps.uMPS(N = 16 * 16, chi = 2, d = 2, l = 2, layers = 1, device = \"cpu\")\n",
    "umpsm_op = unitary_optimizer.Adam(umpsm, lr=0.01)\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    acc = 0\n",
    "    for data, target in trainloader:    \n",
    "        data = data.permute(1, 0, 2)\n",
    "        umpsm_op.zero_grad()\n",
    "        outputs = umpsm(data)\n",
    "        loss = loss_batch(outputs, target)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        umpsm_op.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = calculate_accuracy(outputs, target)\n",
    "        # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        acc += accuracy\n",
    "    acc /= len(trainloader)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
