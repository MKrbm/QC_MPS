{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import umps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "def embedding_pixel(batch, label: int = 0):\n",
    "    pixel_size = batch.shape[-1] * batch.shape[-2]\n",
    "    x = batch.view(*batch.shape[:-2], pixel_size)\n",
    "    # x[:] = 0\n",
    "    x = torch.stack([x, 1-x], dim=-1)\n",
    "    # x = x / torch.sum(x, dim=-1).unsqueeze(-1)\n",
    "    x = x / torch.norm(x, dim=-1).unsqueeze(-1)\n",
    "    return x\n",
    "\n",
    "def embedding_label(labels: torch.Tensor):\n",
    "    emb = torch.zeros(labels.shape[0], 2)\n",
    "    emb[torch.arange(labels.shape[0]), labels] = 1\n",
    "    return emb\n",
    "\n",
    "def filiter_single_channel(batch):\n",
    "    return batch[0, ...]\n",
    "\n",
    "def filter_dataset(dataset, allowed_digits=[0, 1]):\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if label in allowed_digits:\n",
    "            indices.append(i)\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "img_size = 16\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(filiter_single_channel),\n",
    "    transforms.Lambda(embedding_pixel),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.QMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "trainset = filter_dataset(trainset, allowed_digits=[0, 1])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS unitaries\n"
     ]
    }
   ],
   "source": [
    "import unitary_optimizer\n",
    "umpsm = umps.uMPS(N = 16 * 16, chi = 2, d = 2, l = 2, layers = 1, device = \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(outputs, labels):\n",
    "    device = outputs.device\n",
    "    loss = torch.zeros(1, device=device, dtype=torch.float64)\n",
    "\n",
    "    for i in range(len(outputs)):\n",
    "        prob = outputs[i] if labels[i] == 0 else 1 - outputs[i]\n",
    "        loss -= torch.log(prob + 1e-8)\n",
    "    return loss\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    predictions = (outputs < 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    accuracy = correct / labels.numel()\n",
    "    return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7637, 0.9795, 0.8926, 0.0836, 0.8505, 0.9312, 0.0141, 0.9725, 0.1513,\n",
       "        0.9215, 0.0045, 0.0138, 0.0089, 0.0070, 0.6724, 0.9093, 0.0056, 0.0247,\n",
       "        0.0412, 0.0244, 0.0280, 0.0170, 0.8263, 0.0167, 0.0047, 0.0053, 0.0180,\n",
       "        0.9278, 0.9438, 0.0029, 0.0059, 0.0206, 0.0076, 0.8622, 0.0115, 0.2721,\n",
       "        0.8460, 0.0073, 0.0073, 0.9246, 0.0516, 0.0091, 0.9750, 0.9407, 0.8581,\n",
       "        0.0070, 0.9598, 0.0050, 0.6729, 0.9233, 0.0349, 0.0216, 0.9407, 0.0320,\n",
       "        0.8074, 0.0115, 0.8249, 0.8394, 0.8526, 0.0065, 0.0150, 0.1316, 0.9226,\n",
       "        0.9621, 0.0381, 0.0073, 0.0118, 0.9372, 0.8304, 0.9543, 0.9461, 0.0072,\n",
       "        0.0116, 0.7962, 0.9524, 0.0862, 0.9462, 0.3711, 0.9059, 0.9561, 0.9215,\n",
       "        0.0224, 0.0299, 0.0226, 0.0125, 0.0085, 0.0345, 0.0175, 0.0083, 0.0250,\n",
       "        0.0045, 0.8125, 0.0155, 0.9516, 0.9834, 0.1243, 0.0123, 0.9089, 0.0109,\n",
       "        0.9692, 0.0099, 0.9248, 0.8843, 0.9015, 0.0178, 0.0118, 0.0097, 0.0158,\n",
       "        0.9392, 0.7801, 0.0062, 0.0061, 0.9339, 0.9147, 0.0150, 0.0037, 0.8426,\n",
       "        0.0138, 0.0204, 0.9329, 0.8716, 0.8493, 0.0159, 0.0221, 0.7231, 0.0244,\n",
       "        0.9322, 0.0266], dtype=torch.float64, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS unitaries\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n",
      "Accuracy: 0.5391\n",
      "loss: 88.5568615402778\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m umpsm(data)\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_batch(outputs, target)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m umpsm_op\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "umpsm_op = unitary_optimizer.Adam(umpsm, lr=0.01)\n",
    "umpsm = umps.uMPS(N = 16 * 16, chi = 2, d = 2, l = 2, layers = 1, device = \"cpu\")\n",
    "\n",
    "data, target = next(iter(trainloader))\n",
    "data = data.permute(1, 0, 2)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    acc = 0\n",
    "    # for data, target in trainloader:    \n",
    "    #     data = data.permute(1, 0, 2)\n",
    "    umpsm_op.zero_grad()\n",
    "    outputs = umpsm(data)\n",
    "    loss = loss_batch(outputs, target)\n",
    "    loss.backward()\n",
    "    umpsm_op.step()\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = calculate_accuracy(outputs, target)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.62225885873355\n",
      "88.400779478013\n",
      "88.60617995000045\n",
      "88.87919172283672\n",
      "88.8166169799111\n",
      "88.9230582490079\n",
      "88.89673588191654\n",
      "88.30784067465723\n",
      "88.58697651412227\n",
      "88.89301631927417\n",
      "92.62701037763468\n",
      "89.68329130806143\n",
      "87.97742245863435\n",
      "88.02188398893549\n",
      "88.6090710640717\n",
      "88.13267963698254\n",
      "88.48040841418795\n",
      "88.10945438398502\n",
      "88.9771047835331\n",
      "88.11855071648897\n",
      "88.87948038653596\n",
      "89.16618582039055\n",
      "87.84996391477794\n",
      "88.13714208482457\n",
      "89.55143128984768\n",
      "88.48058798526598\n",
      "88.98256957168863\n",
      "87.17232935990128\n",
      "87.804648284048\n",
      "90.08918941731231\n",
      "88.79090859634914\n",
      "88.4849981176141\n",
      "88.63517737951305\n",
      "89.06399800213697\n",
      "88.3416708962494\n",
      "88.59250719697431\n",
      "88.2635701363373\n",
      "88.07611812424696\n",
      "87.83689866820531\n",
      "87.96770101142275\n",
      "87.87484267284829\n",
      "89.0732690084382\n",
      "88.36076242587293\n",
      "87.53418404821701\n",
      "87.50203539425091\n",
      "88.21176195451984\n",
      "87.20474369363372\n",
      "85.81242761084728\n",
      "89.88397296528669\n",
      "90.54964370692309\n",
      "90.33466888457934\n",
      "88.95224481754441\n",
      "88.16064475455956\n",
      "88.33229514799883\n",
      "89.57185240803437\n",
      "88.41370311437367\n",
      "88.64603520644765\n",
      "88.80338334087787\n",
      "88.66499214953708\n",
      "88.78193019362439\n",
      "88.4844731723897\n",
      "88.7535097233405\n",
      "88.79345680755655\n",
      "89.17606039015645\n",
      "87.78369569113767\n",
      "88.34522238877635\n",
      "87.77356154373666\n",
      "89.30791572813835\n",
      "87.99728945395731\n",
      "88.33921498191785\n",
      "87.60903548756293\n",
      "87.1772819972219\n",
      "87.07737463294875\n",
      "88.7189444137891\n",
      "87.4565816204966\n",
      "88.30777216967795\n",
      "88.3341974379788\n",
      "87.16168067675135\n",
      "87.74840575529232\n",
      "90.66007144489275\n",
      "88.50765163142516\n",
      "89.09179109731696\n",
      "89.25686746709916\n",
      "89.11663517998494\n",
      "88.72806272032267\n",
      "88.73310731791679\n",
      "88.80524854100555\n",
      "88.94782323625797\n",
      "88.10838561058887\n",
      "89.58447928715312\n",
      "90.00809747972116\n",
      "89.53974777077936\n",
      "89.52522997534378\n",
      "89.2485393367331\n",
      "88.73951885959796\n",
      "88.82836086389112\n",
      "88.19584551602657\n",
      "86.99633593594402\n",
      "84.0662393725971\n",
      "Accuracy: 0.5254\n",
      "86.824458440155\n",
      "90.30159724040179\n",
      "89.97364998130463\n",
      "90.32812992342275\n",
      "90.7240981209797\n",
      "90.49277750531407\n",
      "87.78714617684825\n",
      "88.73524209267006\n",
      "88.35976827166776\n",
      "88.9147998144538\n",
      "88.78036060911163\n",
      "89.55307142596655\n",
      "88.91500753936432\n",
      "88.88162170224459\n",
      "88.76893389580513\n",
      "88.70748103790794\n",
      "89.06756311201657\n",
      "88.59033058294557\n",
      "88.41519327928964\n",
      "88.38066657434462\n",
      "88.3525957944372\n",
      "88.05555085344824\n",
      "88.65995334642938\n",
      "89.77952101039916\n",
      "89.20990998543853\n",
      "88.48899807369749\n",
      "88.05773045016122\n",
      "87.18401139879106\n",
      "87.34369832281301\n",
      "88.16464799471596\n",
      "89.34016105002635\n",
      "91.05659074899172\n",
      "89.62413519088304\n",
      "88.04456630044886\n",
      "88.24438518516585\n",
      "88.30011575781022\n",
      "88.17801579908559\n",
      "88.17524373509409\n",
      "88.58203261047159\n",
      "87.95232061351889\n",
      "89.11189963076559\n",
      "87.51680192771315\n",
      "87.3735847669432\n",
      "89.9007934935878\n",
      "87.14630702526046\n",
      "89.06741935710323\n",
      "87.0444302848219\n",
      "89.89607863723808\n",
      "87.49044173495528\n",
      "89.11287772873385\n",
      "85.79465766041179\n",
      "90.20966791389095\n",
      "89.43355438226116\n",
      "87.97665547876723\n",
      "88.17143879586703\n",
      "88.76143298513495\n",
      "88.47298369935959\n",
      "87.98161070877805\n",
      "88.48669874556575\n",
      "88.95454722184543\n",
      "88.74864029899378\n",
      "88.24862455987908\n",
      "88.66098022831179\n",
      "88.52131848906555\n",
      "88.7364350281595\n",
      "87.61520236209634\n",
      "89.00811645941575\n",
      "89.58991999143767\n",
      "87.50982655322976\n",
      "87.9331869475618\n",
      "87.07637390781431\n",
      "88.54870890301781\n",
      "89.29503098926243\n",
      "88.66557404422805\n",
      "90.58321450723976\n",
      "88.17083732318457\n",
      "88.54339942750923\n",
      "87.8342310791282\n",
      "89.2286521577734\n",
      "87.74630528954356\n",
      "90.00884660239787\n",
      "88.45355993775651\n",
      "88.63014147609309\n",
      "88.64833904056051\n",
      "88.69498622520534\n",
      "88.66099892139212\n",
      "88.53979103420251\n",
      "88.71991649580886\n",
      "88.66911819627246\n",
      "88.26005724955456\n",
      "89.54828877635612\n",
      "88.75324257008596\n",
      "87.92511810135862\n",
      "88.76714455698503\n",
      "87.4659187069376\n",
      "87.05660207100911\n",
      "89.11240520316778\n",
      "87.12941783492613\n",
      "85.49615894723637\n",
      "Accuracy: 0.5277\n",
      "87.9574618522871\n",
      "89.09926310263906\n",
      "86.88341492824684\n",
      "89.87963967458954\n",
      "89.31767269541844\n",
      "88.65269954896313\n",
      "88.09390871177732\n",
      "88.27665647815081\n",
      "88.5828444227703\n",
      "87.49059875171884\n",
      "88.29323312185421\n",
      "87.9778362940361\n",
      "88.3316973738087\n",
      "88.53538211999165\n",
      "89.01165977517964\n",
      "86.85097603020559\n",
      "87.7216997518858\n",
      "91.36542640104258\n",
      "89.43424894831503\n",
      "87.52495667195937\n",
      "86.80300874809524\n",
      "88.72897779889954\n",
      "89.05719399785602\n",
      "89.92333836823248\n",
      "87.88110714939981\n",
      "88.30152398698601\n",
      "89.20850437729108\n",
      "88.89865870296802\n",
      "88.61544575457748\n",
      "88.72272352535207\n",
      "88.80662063414077\n",
      "88.65255032389274\n",
      "88.17345695666991\n",
      "87.98916201173704\n",
      "89.74286181234083\n",
      "88.72617121867181\n",
      "89.85730607905762\n",
      "89.43888131793214\n",
      "89.14720435658153\n",
      "87.47307956021398\n",
      "88.01148968432354\n",
      "87.71879018894\n",
      "88.64395519367302\n",
      "89.25759166315972\n",
      "88.89422583006726\n",
      "88.23643299900186\n",
      "89.0013423273457\n",
      "88.91258329347693\n",
      "88.82320031781877\n",
      "88.68579302100378\n",
      "88.7495432255189\n",
      "88.7288297823559\n",
      "88.68541056071268\n",
      "88.77009534969217\n",
      "88.90307558821188\n",
      "88.84548431961007\n",
      "88.69327814820238\n",
      "88.75520602271466\n",
      "88.18585864133361\n",
      "88.4725949333894\n",
      "88.98350330827213\n",
      "88.34829696579783\n",
      "89.21787159264663\n",
      "88.82117631127687\n",
      "89.66116444216132\n",
      "89.89263904217287\n",
      "87.71781172915502\n",
      "88.98303686212596\n",
      "88.86742816006598\n",
      "88.01083212505567\n",
      "88.3012721386705\n",
      "88.7825991782791\n",
      "88.00530980612689\n",
      "88.76499100553289\n",
      "88.58648791945146\n",
      "87.58654136670165\n",
      "88.80148033931046\n",
      "89.93071114877715\n",
      "88.16273240899187\n",
      "89.19259060307063\n",
      "87.07313740500749\n",
      "89.63228440614962\n",
      "88.48145225156009\n",
      "87.93492257111906\n",
      "88.34252181908515\n",
      "87.95922173575822\n",
      "89.28594310452871\n",
      "87.96342874490199\n",
      "87.69913404048069\n",
      "88.76535081083394\n",
      "89.23883320621103\n",
      "87.90006143077903\n",
      "87.45465875612926\n",
      "87.68269404713423\n",
      "87.973551784665\n",
      "87.95745041346416\n",
      "89.34107760537674\n",
      "90.5505785667658\n",
      "83.59877522986959\n",
      "Accuracy: 0.5274\n",
      "88.72917698918702\n",
      "88.97760933433257\n",
      "87.84112865047308\n",
      "89.00828868404373\n",
      "88.19746913315724\n",
      "88.46578120318328\n",
      "88.8397771582499\n",
      "88.12750212592587\n",
      "88.33741598241565\n",
      "88.0421482531981\n",
      "88.34529710001276\n",
      "88.65649838537566\n",
      "86.26848632288102\n",
      "90.0842592703525\n",
      "89.51610690068128\n",
      "87.72092217868402\n",
      "89.28021624022563\n",
      "86.98053445875142\n",
      "86.5116336204373\n",
      "88.71970831656466\n",
      "88.46416361083173\n",
      "91.7948906644268\n",
      "91.46516440141099\n",
      "87.3131878878151\n",
      "88.2487950503835\n",
      "88.90218431185956\n",
      "88.71894542517477\n",
      "88.90820202427729\n",
      "88.71694042678732\n",
      "88.71853292205509\n",
      "88.58244251701157\n",
      "88.2607645535164\n",
      "88.67954563241008\n",
      "89.02443012146051\n",
      "88.98776330252238\n",
      "87.98260782535577\n",
      "89.51971482843776\n",
      "88.15971848328358\n",
      "89.30480640531444\n",
      "88.86492108880107\n",
      "89.25696252922675\n",
      "88.47273291996483\n",
      "89.13795365669118\n",
      "88.55473428562696\n",
      "88.70813690061168\n",
      "88.76268367816166\n",
      "88.77949504466477\n",
      "88.81717572439963\n",
      "88.52945868125506\n",
      "88.67773319896776\n",
      "88.71402990637229\n",
      "89.72657485999008\n",
      "88.47403922020683\n",
      "88.73391116904922\n",
      "88.10755202189401\n",
      "87.50731268900893\n",
      "86.96113917398365\n",
      "86.75315235131477\n",
      "86.12436640769633\n",
      "88.63022941682331\n",
      "85.79457211020197\n",
      "90.00916420914758\n",
      "89.83478859993025\n",
      "88.16736136136932\n",
      "93.44528896632069\n",
      "86.10360674615234\n",
      "89.39717092969336\n",
      "90.67646988965804\n",
      "88.55435907496543\n",
      "88.72811881532563\n",
      "89.15047669763717\n",
      "88.66501274500233\n",
      "88.3068280730758\n",
      "90.50541411301117\n",
      "89.40204073753554\n",
      "87.95651170326771\n",
      "91.9050147467903\n",
      "90.68441118956743\n",
      "89.26885005408998\n",
      "88.33944200255313\n",
      "89.57786954215577\n",
      "88.9068258326839\n",
      "88.62560751800115\n",
      "88.47708558908803\n",
      "89.52118053077477\n",
      "87.95729204037873\n",
      "87.9662552951302\n",
      "87.45351139902843\n",
      "88.98743189646632\n",
      "86.19726794479192\n",
      "92.89557713994543\n",
      "86.20507704760949\n",
      "87.76301387380119\n",
      "90.34040977633074\n",
      "86.70407264051299\n",
      "89.80605554534277\n",
      "90.86227996509332\n",
      "88.34298844187201\n",
      "83.31870150347518\n",
      "Accuracy: 0.5240\n",
      "88.93751320480271\n",
      "88.81224151490655\n",
      "88.73147982764038\n",
      "88.92554086656169\n",
      "88.67190996356157\n",
      "89.10767422092998\n",
      "88.72609539881536\n",
      "88.71125592997485\n",
      "88.39654913686866\n",
      "88.5198235582242\n",
      "89.60070454368035\n",
      "88.34238415621608\n",
      "88.03545531204331\n",
      "89.0120340338087\n",
      "89.05639057524131\n",
      "87.44801251120735\n",
      "90.02186646315235\n",
      "87.8167865034797\n",
      "88.16690057187138\n",
      "88.49367323615036\n",
      "89.57155416968823\n",
      "88.8717528529107\n",
      "87.84159598260243\n",
      "88.86907956489041\n",
      "88.18509435358632\n",
      "89.20264994467243\n",
      "88.36490466917327\n",
      "88.07417994787328\n",
      "88.66140417391041\n",
      "87.71493307944526\n",
      "88.83747894516848\n",
      "88.33311883589221\n",
      "89.01743640148585\n",
      "88.70815980864673\n",
      "89.09877944876509\n",
      "88.52429784380918\n",
      "88.85609977861553\n",
      "88.17365579442678\n",
      "86.8707072418367\n",
      "88.33170259629556\n",
      "87.50078968643957\n",
      "87.23562364543302\n",
      "89.01567133059493\n",
      "88.1826128443714\n",
      "87.96028529436175\n",
      "86.98472717004398\n",
      "89.01884817911021\n",
      "88.7670900265674\n",
      "90.76755559927656\n",
      "87.51137531143803\n",
      "88.55585570201366\n",
      "87.6524470995488\n",
      "88.48553971708293\n",
      "88.47468383857559\n",
      "89.43706534916642\n",
      "88.67162357047901\n",
      "88.35879523021616\n",
      "88.26733223340979\n",
      "88.41496141068112\n",
      "88.85701703802881\n",
      "88.07374306072384\n",
      "87.80981235209019\n",
      "88.20649538586152\n",
      "88.84665737864084\n",
      "88.9477598187744\n",
      "88.80264642052731\n",
      "89.25720738299451\n",
      "88.37603234899727\n",
      "88.36048332134202\n",
      "86.66287597324015\n",
      "90.66869999343339\n",
      "88.5058883678587\n",
      "89.07302462454308\n",
      "86.94431571091377\n",
      "88.82657847040942\n",
      "89.03572050150503\n",
      "88.09442662556302\n",
      "88.67659963736676\n",
      "89.30020413222437\n",
      "88.38428354586596\n",
      "88.83577562116496\n",
      "88.70922996901271\n",
      "88.59497148246066\n",
      "88.61520981004642\n",
      "88.65194932432712\n",
      "88.38300760597544\n",
      "88.35071672675083\n",
      "88.48647999900915\n",
      "88.72855069674154\n",
      "88.63742967034035\n",
      "87.82145792140373\n",
      "87.96664683435196\n",
      "87.95669017839606\n",
      "90.30371807120376\n",
      "88.87796165183386\n",
      "90.99583068842934\n",
      "88.8671002048272\n",
      "89.6628521541825\n",
      "82.9616968819483\n",
      "Accuracy: 0.5284\n",
      "88.62177506279798\n",
      "88.64666132291683\n",
      "88.73014193655233\n",
      "88.81622292896493\n",
      "88.70814534396534\n",
      "88.32903708637465\n",
      "89.10193678414176\n",
      "88.71600361096706\n",
      "89.22893640934538\n",
      "88.81979208929222\n",
      "86.32077259991274\n",
      "89.53551747200062\n",
      "88.78702365244105\n",
      "87.32848473857142\n",
      "88.82954021073523\n",
      "88.61056592980047\n",
      "86.8962337352494\n",
      "89.2591345525808\n",
      "88.37646188170983\n",
      "90.15456217069952\n",
      "87.49079264662491\n",
      "89.23568603685648\n",
      "86.63727980322606\n",
      "88.984154018064\n",
      "88.59720895307579\n",
      "88.8320100559385\n",
      "88.14840030545486\n",
      "88.58492292653287\n",
      "87.74767563406958\n",
      "88.94441417957505\n",
      "89.4481292230946\n",
      "87.71010391517297\n",
      "88.81549979238483\n",
      "89.70757106622843\n",
      "88.58242172127785\n",
      "88.72912348632266\n",
      "88.5213974645619\n",
      "88.27803586328616\n",
      "88.61341849200018\n",
      "88.20710813719246\n",
      "88.85998902929433\n",
      "89.25745465676125\n",
      "88.82027738386809\n",
      "88.92763590573541\n",
      "88.99876706729191\n",
      "88.15070744352658\n",
      "88.33968188569914\n",
      "88.41521201609066\n",
      "88.76461917854647\n",
      "89.05039880658084\n",
      "87.38517708414886\n",
      "88.69261615292683\n",
      "89.34359867460729\n",
      "87.9714884520545\n",
      "88.20538272755128\n",
      "87.02074392029691\n",
      "90.44437308673113\n",
      "90.80600915220322\n",
      "89.06803133566727\n",
      "88.14270440769337\n",
      "88.75447265743746\n",
      "88.24484014011767\n",
      "88.46208894323125\n",
      "88.45919299751161\n",
      "88.66027045877264\n",
      "88.86801233728517\n",
      "88.94243312472068\n",
      "88.91135731406517\n",
      "88.56800960710406\n",
      "88.43242925516401\n",
      "88.664790816427\n",
      "88.29593330980009\n",
      "88.66205470745989\n",
      "88.76523229796535\n",
      "88.79178486417428\n",
      "87.48147346597291\n",
      "88.33856046096243\n",
      "88.49182503928618\n",
      "88.6989117983357\n",
      "90.07329397495225\n",
      "86.1675046834866\n",
      "88.55046128079272\n",
      "87.35040433474111\n",
      "89.47927516189988\n",
      "88.60473436188666\n",
      "88.79452166318357\n",
      "88.35517305044898\n",
      "88.69985328233312\n",
      "88.01108790069732\n",
      "89.21424248009663\n",
      "87.73538571432401\n",
      "87.7914342956744\n",
      "88.82984523662668\n",
      "88.7087121829965\n",
      "87.68499713376815\n",
      "87.1102453419066\n",
      "90.36681871788444\n",
      "87.21826204468982\n",
      "82.84378316200858\n",
      "Accuracy: 0.5306\n",
      "88.75181408599897\n",
      "89.42547358238326\n",
      "85.88567607136612\n",
      "88.63572522392289\n",
      "88.67183348021177\n",
      "89.63728053028836\n",
      "90.4368080015657\n",
      "89.67904980946662\n",
      "88.90099384054632\n",
      "87.94826071539364\n",
      "88.2444315363708\n",
      "88.52682306839796\n",
      "88.50365590987656\n",
      "88.75675142427356\n",
      "88.04303053777707\n",
      "89.27661896081186\n",
      "89.43743001464755\n",
      "88.22113014088193\n",
      "88.06834500945364\n",
      "88.58222574192999\n",
      "89.11047724631034\n",
      "89.3387301095623\n",
      "88.67668872640563\n",
      "88.0997516247545\n",
      "88.58445460095975\n",
      "87.58971031388414\n",
      "88.80424810613118\n",
      "89.87963891938604\n",
      "87.25156444253285\n",
      "88.6123875285268\n",
      "87.59070604101845\n",
      "88.67218646590806\n",
      "89.25605172295006\n",
      "89.62449513548604\n",
      "89.33795751029464\n",
      "88.75692455111758\n",
      "88.14293777890833\n",
      "87.66697127249851\n",
      "88.29846103093307\n",
      "89.6163307491205\n",
      "88.31149252319612\n",
      "89.09228295696627\n",
      "88.79745292187216\n",
      "88.66306540927519\n",
      "88.64188629634765\n",
      "88.4476935420699\n",
      "88.41241706965181\n",
      "88.92591874809064\n",
      "88.41427137366995\n",
      "88.79550444604527\n",
      "88.95202736384444\n",
      "87.84911494895036\n",
      "88.89384363756731\n",
      "89.2067779841604\n",
      "88.19675535127301\n",
      "87.65116872192417\n",
      "88.03973046736182\n",
      "88.17186795651499\n",
      "86.94617615961184\n",
      "86.75173313743544\n",
      "90.60242021085088\n",
      "91.22920497312388\n",
      "87.72616628592169\n",
      "86.87848372794339\n",
      "88.1681715941172\n",
      "90.0241238146652\n",
      "87.43001195464566\n",
      "89.35012243159895\n",
      "89.77807576079965\n",
      "88.070693828542\n",
      "89.10891568227926\n",
      "88.67123370697412\n",
      "88.72647410740787\n",
      "88.9039243393459\n",
      "88.79438243237527\n",
      "89.78284033077696\n",
      "88.75638389954959\n",
      "88.61929553659098\n",
      "88.72471656855656\n",
      "88.78345583244771\n",
      "87.70307171158221\n",
      "87.83695777706134\n",
      "91.63557014584954\n",
      "87.75650208744656\n",
      "87.3394213348378\n",
      "88.38690016933086\n",
      "89.93619010627465\n",
      "87.34071538975711\n",
      "89.76283837002089\n",
      "87.44878481154201\n",
      "86.83167806026368\n",
      "87.11708943454005\n",
      "89.12884081890499\n",
      "86.75514084283179\n",
      "88.17631821538978\n",
      "87.72357387535575\n",
      "89.98431144276445\n",
      "88.21425514394926\n",
      "82.94009851105034\n",
      "Accuracy: 0.5278\n",
      "89.58511687559792\n",
      "87.5341061572645\n",
      "88.3580968258199\n",
      "89.61544658113641\n",
      "87.72252670195857\n",
      "89.27821815144318\n",
      "88.16399335115877\n",
      "89.3584434679294\n",
      "88.6103497804482\n",
      "88.73145728923018\n",
      "88.76602195042447\n",
      "88.74126513366427\n",
      "88.68461015364123\n",
      "88.40894727235639\n",
      "89.20359261203295\n",
      "87.96479092982744\n",
      "89.92369648579421\n",
      "88.21937153341143\n",
      "88.33865200793286\n",
      "86.59251959482341\n",
      "89.58699132950636\n",
      "87.19593634839038\n",
      "88.38687101983741\n",
      "88.65495938080548\n",
      "91.33038244678903\n",
      "88.39282205438619\n",
      "89.91474510794549\n",
      "87.24426121987764\n",
      "87.53757707711327\n",
      "89.11076486750935\n",
      "88.81147767353963\n",
      "89.23274099676182\n",
      "88.16427684630163\n",
      "88.16302112659578\n",
      "89.07786125208364\n",
      "88.88017698143649\n",
      "88.32220491916237\n",
      "88.95893918638185\n",
      "88.75377323334133\n",
      "88.85384055781542\n",
      "88.55728890024702\n",
      "88.48302132953323\n",
      "88.36109251821952\n",
      "88.04432246987456\n",
      "88.96360444796609\n",
      "90.04599903017794\n",
      "89.64402077126798\n",
      "88.33175748350233\n",
      "87.89222502211844\n",
      "88.47966748766228\n",
      "88.47841562094493\n",
      "88.47678027056864\n",
      "89.16388467005719\n",
      "88.47257701946221\n",
      "87.47416711748548\n",
      "87.64256123193506\n",
      "88.19675415023899\n",
      "88.33205247196484\n",
      "86.53283316464042\n",
      "87.08926691907836\n",
      "88.74884053451554\n",
      "89.16397725766087\n",
      "89.54480417143832\n",
      "88.63575049310249\n",
      "85.75186475492497\n",
      "89.16314703771687\n",
      "89.09969507761926\n",
      "91.2638767383255\n",
      "89.40213478256749\n",
      "88.63448864837461\n",
      "88.19002002590585\n",
      "88.71847112397424\n",
      "88.56788894461116\n",
      "88.71964762332831\n",
      "88.71636463305806\n",
      "88.77410643057814\n",
      "88.71045333262539\n",
      "88.98472465196998\n",
      "88.71526762250912\n",
      "88.56997835673275\n",
      "88.4650692244937\n",
      "88.38535079482311\n",
      "88.87017617463513\n",
      "88.01900022242295\n",
      "89.25199178285813\n",
      "88.35351449479465\n",
      "86.54810438235354\n",
      "87.73178799666285\n",
      "90.17956666458879\n",
      "86.98251982513315\n",
      "87.72010606558328\n",
      "88.23694858440464\n",
      "88.49732615496289\n",
      "91.76789474060138\n",
      "87.30767695820322\n",
      "88.91508840021314\n",
      "88.95838148790185\n",
      "89.0719858593719\n",
      "83.30468029997411\n",
      "Accuracy: 0.5273\n",
      "89.1207235513187\n",
      "88.6403371002195\n",
      "88.73549460947748\n",
      "88.81748068648277\n",
      "88.8122382614549\n",
      "88.99215488717678\n",
      "88.97308230979785\n",
      "88.6560625623252\n",
      "88.77853519894722\n",
      "89.22925736529011\n",
      "88.69461622321597\n",
      "87.62657126165497\n",
      "86.87775152157933\n",
      "89.85137580638\n",
      "89.41753750239988\n",
      "90.09683130951927\n",
      "84.36520436192107\n",
      "87.95575917794675\n",
      "89.91804300801665\n",
      "87.47478434316824\n",
      "87.96973287542818\n",
      "88.4736581439758\n",
      "87.47365753985865\n",
      "92.1310154654919\n",
      "87.95736838017645\n",
      "88.33840026392933\n",
      "88.62759078038681\n",
      "88.9420942786828\n",
      "88.66678784683641\n",
      "88.35816358244266\n",
      "88.63906533893598\n",
      "88.7537253475914\n",
      "88.6131407302966\n",
      "88.80685617388437\n",
      "88.5697720773695\n",
      "88.36369453847989\n",
      "88.6604171303311\n",
      "88.77166916686623\n",
      "86.95763302070293\n",
      "88.33208840251747\n",
      "87.35461368234455\n",
      "89.19169001390114\n",
      "88.00031511719057\n",
      "89.81179566731542\n",
      "87.45629754336564\n",
      "85.35802482088525\n",
      "87.78249471047974\n",
      "90.38998620861024\n",
      "87.77738955738634\n",
      "88.0508750278518\n",
      "86.33399300109512\n",
      "86.91111328628965\n",
      "89.6106828875874\n",
      "88.72970888715777\n",
      "87.0396942413146\n",
      "90.5445930255212\n",
      "88.88288611890522\n",
      "90.17980128622108\n",
      "88.66705497719539\n",
      "88.79481224503343\n",
      "88.78248373040316\n",
      "89.54897060233618\n",
      "89.07790442998342\n",
      "89.50972391794554\n",
      "89.05467430440483\n",
      "89.04592924907817\n",
      "88.71805459179433\n",
      "88.48494596055804\n",
      "88.94084678856687\n",
      "87.24771673756155\n",
      "89.53298183409157\n",
      "87.96117811165833\n",
      "87.95683991580486\n",
      "90.25195340003724\n",
      "89.7690429698278\n",
      "86.01905837621919\n",
      "89.68334720887462\n",
      "87.02831101715104\n",
      "88.41198427644085\n",
      "88.17469043439596\n",
      "87.9571633763659\n",
      "90.1777105226921\n",
      "88.86080778069557\n",
      "88.04690260409525\n",
      "89.19539732511325\n",
      "87.60915149119066\n",
      "89.14997218614418\n",
      "88.5987543607427\n",
      "88.61850129371841\n",
      "88.67228455083803\n",
      "88.65393775709845\n",
      "88.75467515913029\n",
      "88.52416789695616\n",
      "88.90289573612883\n",
      "88.70735837661262\n",
      "88.82103555017609\n",
      "88.67334543110023\n",
      "88.89903466821809\n",
      "83.90477475462147\n",
      "Accuracy: 0.5227\n",
      "88.72642994956637\n",
      "88.70013322211777\n",
      "88.82713097766018\n",
      "88.89941672778207\n",
      "88.56350421002882\n",
      "88.70862146355671\n",
      "88.93577703700146\n",
      "88.82347630467838\n",
      "88.86641125926008\n",
      "88.74645029808809\n",
      "88.60919946935095\n",
      "88.54241836785256\n",
      "87.83071049521487\n",
      "89.1785711611029\n",
      "90.57162203987384\n",
      "87.98732197185305\n",
      "89.96103585536328\n",
      "88.66590837786302\n",
      "87.13529223393375\n",
      "87.88198745923575\n",
      "87.70532092562537\n",
      "89.5243266504165\n",
      "88.50174539338232\n",
      "87.02348372038576\n",
      "88.33673144971849\n",
      "88.34279263429242\n",
      "88.15939575145683\n",
      "87.77074187179494\n",
      "88.77196568503153\n",
      "88.36732043495833\n",
      "87.15352919211928\n",
      "87.7486862242609\n",
      "88.6080240251942\n",
      "87.51608035530339\n",
      "88.63288918339177\n",
      "91.10281255081303\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      7\u001b[0m umpsm_op\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mumpsm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_batch(outputs, target)\n\u001b[1;32m     10\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/umps.py:246\u001b[0m, in \u001b[0;36muMPS.forward\u001b[0;34m(self, X, label)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    245\u001b[0m     batch[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:, :, :] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_discriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/opt_einsum/contract.py:763\u001b[0m, in \u001b[0;36mContractExpression.__call__\u001b[0;34m(self, *arrays, **kwargs)\u001b[0m\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backends\u001b[38;5;241m.\u001b[39mhas_backend(backend) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays):\n\u001b[1;32m    761\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_contract_with_conversion(ops, out, backend, evaluate_constants\u001b[38;5;241m=\u001b[39mevaluate_constants)\n\u001b[0;32m--> 763\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_contract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate_constants\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    766\u001b[0m     original_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(err\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;28;01mif\u001b[39;00m err\u001b[38;5;241m.\u001b[39margs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/opt_einsum/contract.py:693\u001b[0m, in \u001b[0;36mContractExpression._contract\u001b[0;34m(self, arrays, out, backend, evaluate_constants)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The normal, core contraction.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m contraction_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_full_contraction_list \u001b[38;5;28;01mif\u001b[39;00m evaluate_constants \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontraction_list\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_core_contract\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcontraction_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mevaluate_constants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluate_constants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/opt_einsum/contract.py:591\u001b[0m, in \u001b[0;36m_core_contract\u001b[0;34m(operands, contraction_list, backend, evaluate_constants, **einsum_kwargs)\u001b[0m\n\u001b[1;32m    588\u001b[0m         einsum_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m out_array\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;66;03m# Do the contraction\u001b[39;00m\n\u001b[0;32m--> 591\u001b[0m     new_view \u001b[38;5;241m=\u001b[39m \u001b[43m_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43meinsum_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtmp_operands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meinsum_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;66;03m# Append new items and dereference what we can\u001b[39;00m\n\u001b[1;32m    594\u001b[0m operands\u001b[38;5;241m.\u001b[39mappend(new_view)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "umpsm_op = unitary_optimizer.Adam(umpsm, lr=0.01)\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(100):\n",
    "    acc = 0\n",
    "    for data, target in trainloader:    \n",
    "        data = data.permute(1, 0, 2)\n",
    "        umpsm_op.zero_grad()\n",
    "        outputs = umpsm(data)\n",
    "        loss = loss_batch(outputs, target)\n",
    "        loss.backward()\n",
    "        loss_list.append(loss.item())\n",
    "        umpsm_op.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = calculate_accuracy(outputs, target)\n",
    "        # print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        acc += accuracy\n",
    "    acc /= len(trainloader)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    # print(\"grad\", umpsm.params[-1].grad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0039, 1.0000],\n",
       "         [0.1673, 0.9859],\n",
       "         [0.7474, 0.6644],\n",
       "         [0.7474, 0.6644],\n",
       "         [0.1470, 0.9891],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0079, 1.0000],\n",
       "         [0.2150, 0.9766],\n",
       "         [0.9151, 0.4033],\n",
       "         [0.9990, 0.0450],\n",
       "         [0.9936, 0.1128],\n",
       "         [0.7043, 0.7099],\n",
       "         [0.0624, 0.9981],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0039, 1.0000],\n",
       "         [0.3297, 0.9441],\n",
       "         [0.9281, 0.3723],\n",
       "         [0.9988, 0.0493],\n",
       "         [0.9990, 0.0450],\n",
       "         [0.8709, 0.4915],\n",
       "         [0.9766, 0.2150],\n",
       "         [0.4599, 0.8880],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0039, 1.0000],\n",
       "         [0.1470, 0.9891],\n",
       "         [0.9558, 0.2941],\n",
       "         [0.9941, 0.1081],\n",
       "         [0.8483, 0.5295],\n",
       "         [0.9420, 0.3357],\n",
       "         [0.4599, 0.8880],\n",
       "         [0.9006, 0.4346],\n",
       "         [0.6874, 0.7262],\n",
       "         [0.0119, 0.9999],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.1176, 0.9931],\n",
       "         [0.8522, 0.5232],\n",
       "         [0.9811, 0.1935],\n",
       "         [0.6817, 0.7316],\n",
       "         [0.0987, 0.9951],\n",
       "         [0.0987, 0.9951],\n",
       "         [0.0079, 1.0000],\n",
       "         [0.8104, 0.5859],\n",
       "         [0.9610, 0.2766],\n",
       "         [0.1224, 0.9925],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0282, 0.9996],\n",
       "         [0.6347, 0.7727],\n",
       "         [0.9715, 0.2370],\n",
       "         [0.2824, 0.9593],\n",
       "         [0.0580, 0.9983],\n",
       "         [0.0079, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.8104, 0.5859],\n",
       "         [0.9884, 0.1521],\n",
       "         [0.1777, 0.9841],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.1829, 0.9831],\n",
       "         [0.9593, 0.2824],\n",
       "         [0.7577, 0.6526],\n",
       "         [0.0079, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0039, 1.0000],\n",
       "         [0.8192, 0.5735],\n",
       "         [0.9789, 0.2041],\n",
       "         [0.1521, 0.9884],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.3478, 0.9376],\n",
       "         [0.9821, 0.1882],\n",
       "         [0.3177, 0.9482],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0039, 1.0000],\n",
       "         [0.2314, 0.9729],\n",
       "         [0.9420, 0.3357],\n",
       "         [0.6644, 0.7474],\n",
       "         [0.0282, 0.9996],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.3539, 0.9353],\n",
       "         [0.9558, 0.2941],\n",
       "         [0.1673, 0.9859],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0450, 0.9990],\n",
       "         [0.4220, 0.9066],\n",
       "         [0.9095, 0.4158],\n",
       "         [0.5421, 0.8403],\n",
       "         [0.0712, 0.9975],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.3539, 0.9353],\n",
       "         [0.9868, 0.1622],\n",
       "         [0.6227, 0.7825],\n",
       "         [0.2314, 0.9729],\n",
       "         [0.5232, 0.8522],\n",
       "         [0.8321, 0.5547],\n",
       "         [0.9576, 0.2882],\n",
       "         [0.5547, 0.8321],\n",
       "         [0.0282, 0.9996],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.2652, 0.9642],\n",
       "         [0.9800, 0.1988],\n",
       "         [0.9960, 0.0894],\n",
       "         [0.9821, 0.1882],\n",
       "         [0.9891, 0.1470],\n",
       "         [0.8912, 0.4535],\n",
       "         [0.3908, 0.9205],\n",
       "         [0.0408, 0.9992],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0366, 0.9993],\n",
       "         [0.3908, 0.9205],\n",
       "         [0.8912, 0.4535],\n",
       "         [0.8976, 0.4409],\n",
       "         [0.4662, 0.8847],\n",
       "         [0.0940, 0.9956],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000],\n",
       "         [0.0000, 1.0000]]),\n",
       " 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.0000, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.9540, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7446, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.7160, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.4835, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2753, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2266, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.2112, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.1061, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0592, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0522, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0518, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0326, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0233, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0119, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0034, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0031, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0020, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0014, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0008, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0004, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0002, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(0.0001, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(9.6392e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.9003e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.8620e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7271e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.9768e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3536e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.3430e-05, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(6.7274e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.5438e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(3.0788e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.8691e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.6596e-06, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.7729e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(8.6364e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(7.7402e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(4.2230e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.7570e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(2.6929e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.9952e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n",
      "tensor(1.8739e-07, dtype=torch.float64, grad_fn=<TraceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import tpcp_mps\n",
    "from importlib import reload\n",
    "reload(tpcp_mps)\n",
    "\n",
    "mps = tpcp_mps.MPSTPCP(N=16*16, K=2, d=2)\n",
    "\n",
    "rho = mps.forward(data.to(torch.float64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
