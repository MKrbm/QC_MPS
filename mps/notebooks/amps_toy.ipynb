{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geoopt\n",
    "\n",
    "# If your mps/ package is local, ensure itâ€™s on sys.path or installed in editable mode:\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from mps.simple_mps import SimpleMPS\n",
    "from mps.tpcp_mps import MPSTPCP, ManifoldType\n",
    "from mps.StiefelOptimizers import StiefelAdam, StiefelSGD\n",
    "from mps.radam import RiemannianAdam\n",
    "\n",
    "# Enable inline plotting in Jupyter\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Each sample is an n-dimensional vector where the first element is either 0 or 1,\n",
    "    and the remaining n-1 entries are 0. The label is the first element.\n",
    "    We embed each scalar x to a 2-d vector [x, 1-x] and then L2-normalize.\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int, num_samples: int = 10000, seed: int | None = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n: Dimension of the vector.\n",
    "            num_samples: Number of samples in the dataset.\n",
    "            seed: Random seed (optional).\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.num_samples = num_samples\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        # Randomly assign labels (0 or 1) for each sample\n",
    "        self.labels = np.random.randint(0, 2, size=num_samples).astype(np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # The label is either 0 or 1\n",
    "        l = self.labels[index]\n",
    "        # Create a vector of length n with first element l and rest zeros\n",
    "        x = torch.zeros(self.n, dtype=torch.float64)\n",
    "        x[0] = float(l)\n",
    "        # Embed each scalar: map x -> [x, 1-x]\n",
    "        x_embedded = torch.stack([x, 1 - x], dim=-1)  # shape: [n, 2]\n",
    "        # Normalize each site so that the two entries sum to 1\n",
    "        x_embedded = x_embedded / (x_embedded.sum(dim=-1, keepdim=True).clamp(min=1e-8))\n",
    "        # The target label is l\n",
    "        return x_embedded, torch.tensor(l, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(outputs, labels):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy style loss for outputs in [0, 1].\n",
    "    For label=0 => use outputs[i], for label=1 => use 1 - outputs[i].\n",
    "    \"\"\"\n",
    "    device = outputs.device\n",
    "    loss_val = torch.zeros(1, device=device, dtype=torch.float64)\n",
    "    for i in range(len(outputs)):\n",
    "        prob = outputs[i] if labels[i] == 0 else (1 - outputs[i])\n",
    "        loss_val -= torch.log(prob + 1e-8)\n",
    "        if torch.isnan(loss_val):\n",
    "            print(f\"NaN in loss at index {i}, prob={prob}, output={outputs[i]}, label={labels[i]}\")\n",
    "    return loss_val\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Threshold outputs at 0.5 to assign label 0 or 1 and compare to true labels.\n",
    "    \"\"\"\n",
    "    predictions = (outputs < 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    return correct / labels.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared:\n",
      "Number of samples: 1000\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Configuration for debugging\n",
    "n = 100          # Use a smaller input dimension for faster runs\n",
    "num_data = 1000 # Smaller dataset for quick debugging\n",
    "batch_size = 64\n",
    "\n",
    "synthetic_dataset = SyntheticDataset(n=n, num_samples=num_data, seed=42)\n",
    "dataloader = torch.utils.data.DataLoader(synthetic_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Dataset prepared:\")\n",
    "print(\"Number of samples:\", len(synthetic_dataset))\n",
    "print(\"Batch size:\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS with random matrices\n",
      "\n",
      "=== Training SimpleMPS for 3 epochs (debug mode)... ===\n",
      "[SimpleMPS] Epoch 1 | Loss: 0.693003 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 2 | Loss: 0.688807 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 3 | Loss: 0.622389 | Accuracy: 54.60%\n",
      "[SimpleMPS] Epoch 4 | Loss: 0.444156 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 5 | Loss: 0.421245 | Accuracy: 54.50%\n",
      "[SimpleMPS] Epoch 6 | Loss: 0.389004 | Accuracy: 54.10%\n",
      "[SimpleMPS] Epoch 7 | Loss: 0.355695 | Accuracy: 72.50%\n",
      "[SimpleMPS] Epoch 8 | Loss: 0.507542 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 9 | Loss: 0.416707 | Accuracy: 57.50%\n",
      "[SimpleMPS] Epoch 10 | Loss: 0.484801 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 11 | Loss: 0.400196 | Accuracy: 67.90%\n",
      "[SimpleMPS] Epoch 12 | Loss: 0.352454 | Accuracy: 77.20%\n",
      "[SimpleMPS] Epoch 13 | Loss: 0.394807 | Accuracy: 69.70%\n",
      "[SimpleMPS] Epoch 14 | Loss: 0.426074 | Accuracy: 58.90%\n",
      "[SimpleMPS] Epoch 15 | Loss: 0.330534 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 16 | Loss: 0.202745 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 17 | Loss: 0.088731 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 18 | Loss: 0.012066 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 19 | Loss: 0.001677 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 20 | Loss: 0.000598 | Accuracy: 100.00%\n",
      "SimpleMPS training complete.\n"
     ]
    }
   ],
   "source": [
    "# Training a SimpleMPS model (demo)\n",
    "N = n\n",
    "d = l = 2  \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "smps = SimpleMPS(\n",
    "    N,\n",
    "    2,\n",
    "    d,\n",
    "    l,\n",
    "    layers=2,\n",
    "    device=device,\n",
    "    dtype=torch.float64,\n",
    "    optimize=\"greedy\",\n",
    ")\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "nnloss = torch.nn.NLLLoss(reduction=\"mean\")\n",
    "opt_smps = torch.optim.Adam(smps.parameters(), lr=0.001)\n",
    "\n",
    "smps_losses = []\n",
    "smps.train()\n",
    "print(\"\\n=== Training SimpleMPS for 3 epochs (debug mode)... ===\")\n",
    "for epoch in range(20):  # Use fewer epochs for debugging\n",
    "    total_loss_smps = 0.0\n",
    "    total_samples_smps = 0\n",
    "    total_correct_smps = 0\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.permute(1, 0, 2)\n",
    "        opt_smps.zero_grad()\n",
    "        outputs = smps(data)  # shape: [batch_size, 2]\n",
    "        outputs = logsoftmax(outputs)\n",
    "        loss = nnloss(outputs, target)\n",
    "        loss.backward()\n",
    "        opt_smps.step()\n",
    "\n",
    "        bs = target.size(0)\n",
    "        total_loss_smps += loss.item() * bs\n",
    "        total_samples_smps += bs\n",
    "\n",
    "        preds = outputs.argmax(dim=-1)\n",
    "        total_correct_smps += (preds == target).float().sum().item()\n",
    "\n",
    "    epoch_loss = total_loss_smps / total_samples_smps\n",
    "    epoch_accuracy = total_correct_smps / total_samples_smps\n",
    "    smps_losses.append(epoch_loss)\n",
    "    print(f\"[SimpleMPS] Epoch {epoch+1} | Loss: {epoch_loss:.6f} | Accuracy: {epoch_accuracy:.2%}\")\n",
    "\n",
    "print(\"SimpleMPS training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPCP model built and canonical form set from SimpleMPS.\n"
     ]
    }
   ],
   "source": [
    "from mps import tpcp_mps\n",
    "from importlib import reload\n",
    "\n",
    "reload(tpcp_mps)\n",
    "\n",
    "# Build TPCP model with original manifold\n",
    "tpcp = tpcp_mps.MPSTPCP(N, K=1, d=2, enable_r=False, with_identity=True, manifold=tpcp_mps.ManifoldType.EXACT)\n",
    "tpcp.to(device)\n",
    "tpcp.train()\n",
    "tpcp.set_canonical_mps(smps, set_r=True)\n",
    "print(\"TPCP model built and canonical form set from SimpleMPS.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.zeros(tpcp.L, 2, dtype=torch.float64)\n",
    "W[:, 0] = 1\n",
    "W[:, 1] = 0\n",
    "tpcp.initialize_W(W)\n",
    "\n",
    "def accuracy(outputs, target):\n",
    "    correct = (outputs < 0).float() == target.float()\n",
    "    return correct.float().sum() / target.numel()\n",
    "\n",
    "def to_probs(outputs):\n",
    "    outputs = outputs / outputs.sum(dim=-1).unsqueeze(-1)\n",
    "    return outputs\n",
    "\n",
    "data, target = next(iter(dataloader))\n",
    "\n",
    "out = tpcp(data)\n",
    "probs = to_probs(out)\n",
    "\n",
    "calculate_accuracy(probs[:, 0], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPCP optimizer set up.\n",
      "5.155473232473777\n",
      "4.724784274544622\n",
      "4.591041582921726\n",
      "4.526179760070529\n",
      "3.8995202302191365\n",
      "3.3278915083438947\n",
      "3.800269678659858\n",
      "2.691488289578952\n",
      "2.403651388201233\n",
      "2.303956247691111\n",
      "2.9145804003831604\n",
      "2.3961578013323557\n",
      "2.041894984066699\n",
      "1.8357755966114473\n",
      "1.7653843644664011\n",
      "1.0273811616522768\n",
      "1.450180272384733\n",
      "1.2482996680995344\n",
      "1.1306110142522283\n",
      "0.984785758995399\n",
      "0.8516335623559306\n",
      "0.6962392026979123\n",
      "0.5677048977170822\n",
      "0.4905377766475981\n",
      "0.3809207337269796\n",
      "0.36258990845748046\n",
      "0.24437056766554538\n",
      "0.18757677835590994\n",
      "0.13844433501470338\n",
      "0.0960065865886699\n",
      "0.0708515003298373\n",
      "0.03818102214943235\n",
      "0.04439861015175338\n",
      "0.01946653828444713\n",
      "0.013646094569727346\n",
      "0.0013998030173011519\n",
      "0.002127642819655853\n",
      "0.009398766801611265\n",
      "0.016071112722805318\n",
      "0.018868202614813204\n",
      "0.014586907740646343\n",
      "0.02000499283673006\n",
      "0.028012750563233185\n",
      "0.029286196435079407\n",
      "0.03979123217385742\n",
      "0.035943604261882346\n",
      "0.03705610978262247\n",
      "0.028442432042053795\n",
      "0.04707733640574422\n",
      "0.04451189991615381\n",
      "0.03884920623072517\n",
      "0.0349367614553528\n",
      "0.031215521575019597\n",
      "0.04152925901558561\n",
      "0.03373336243579532\n",
      "0.025916626774533824\n",
      "0.02147184174580693\n",
      "0.023028739668095575\n",
      "0.01730249993152738\n",
      "0.013866213160233192\n",
      "0.01008153758098639\n",
      "0.007571486492380859\n",
      "0.007564776887375086\n",
      "0.004391872193317779\n",
      "0.004962710660305537\n",
      "0.002047547688007455\n",
      "0.0012203116307002626\n",
      "0.0017230607316348673\n",
      "0.0014953260587050085\n",
      "0.0010439659995435961\n",
      "0.00010888428785441559\n",
      "0.0002077250727346036\n",
      "0.0008507547341693085\n",
      "0.00133406157647181\n",
      "0.0012458542872047784\n",
      "0.0007702889883963496\n",
      "0.0009696695421890808\n",
      "0.001671267463388706\n",
      "0.0022584070077038463\n",
      "0.001007598719129306\n"
     ]
    }
   ],
   "source": [
    "# Choose an optimizer for TPCP\n",
    "opt_tpcp = RiemannianAdam(tpcp.kraus_ops.parameters(), lr=0.0001)\n",
    "print(\"TPCP optimizer set up.\")\n",
    "\n",
    "max_epochs = 5\n",
    "for i in range(max_epochs):\n",
    "  for data, target in dataloader:\n",
    "    opt_tpcp.zero_grad()\n",
    "\n",
    "    out = tpcp(data)\n",
    "    probs = to_probs(out)\n",
    "    loss = loss_batch(probs[:, 0], target)\n",
    "\n",
    "    loss.backward()\n",
    "    opt_tpcp.step()\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for TPCP training (using fewer epochs and a subset of weight values for debugging)\n",
    "max_epochs = 5\n",
    "min_epochs = 2\n",
    "conv_threshold = 1e-4\n",
    "log_steps = 5\n",
    "\n",
    "w_values = [0, 0.5, 1]  # Use a few weight rates for debugging\n",
    "\n",
    "for w_ in w_values:\n",
    "    # Re-initialize weight parameter W for current w_\n",
    "    W = torch.zeros(tpcp.L, 2, dtype=torch.float64, device=device)\n",
    "    W[:, 0] = 1\n",
    "    W[:, 1] = w_\n",
    "    tpcp.initialize_W(W)\n",
    "    \n",
    "    print(f\"\\n=== Training TPCP with initial weight rate w={w_:.1f} (max_epochs={max_epochs}) ===\")\n",
    "    epoch = 0\n",
    "    prev_epoch_loss = None\n",
    "\n",
    "    while epoch < max_epochs:\n",
    "        epoch_loss_sum = 0.0\n",
    "        total_samples = 0\n",
    "        epoch_acc_sum = 0.0\n",
    "        total_acc_samples = 0\n",
    "        t0 = time.time()\n",
    "        for step, (data, target) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            bs = target.size(0)\n",
    "            \n",
    "            opt_tpcp.zero_grad()\n",
    "            outputs = tpcp(data)\n",
    "            if torch.isnan(outputs).any():\n",
    "                print(f\"NaN detected in outputs at step {step}, skipping batch.\")\n",
    "                continue\n",
    "            \n",
    "            loss_val = loss_batch(outputs, target)\n",
    "            if torch.isnan(loss_val):\n",
    "                print(\"NaN detected in batch loss, skipping batch.\")\n",
    "                continue\n",
    "            \n",
    "            loss_val.backward()\n",
    "            opt_tpcp.step()\n",
    "            tpcp.proj_stiefel(check_on_manifold=True, print_log=False, rtol=1e-3)\n",
    "            \n",
    "            epoch_loss_sum += loss_val.item() * bs\n",
    "            total_samples += bs\n",
    "            batch_acc = calculate_accuracy(outputs.detach(), target)\n",
    "            epoch_acc_sum += batch_acc.item() * bs\n",
    "            total_acc_samples += bs\n",
    "            \n",
    "            if (step + 1) % log_steps == 0:\n",
    "                print(f\"[TPCP::w={w_:.1f}] Epoch {epoch+1}, Step {step+1}/{len(dataloader)} | Batch Loss: {loss_val.item():.6f} | Batch Accuracy: {batch_acc.item():.2%}\")\n",
    "\n",
    "        if total_samples == 0:\n",
    "            print(\"No samples processed in this epoch, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        avg_loss = epoch_loss_sum / total_samples\n",
    "        avg_acc = epoch_acc_sum / total_acc_samples\n",
    "        current_weight_rate = (tpcp.W[:, 1] / torch.sum(tpcp.W, dim=1)).mean().item() if hasattr(tpcp, \"W\") else w_\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[TPCP::w={w_:.1f}] Epoch {epoch+1} | Avg Loss: {avg_loss:.6f}, Avg Acc: {avg_acc:.2%}, Weight Rate: {current_weight_rate:.4f} | Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        if epoch >= min_epochs and prev_epoch_loss is not None:\n",
    "            if abs(avg_loss - prev_epoch_loss) < conv_threshold:\n",
    "                print(f\"Convergence reached: Loss change below threshold for epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        prev_epoch_loss = avg_loss\n",
    "        epoch += 1\n",
    "\n",
    "print(\"\\nTPCP training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
