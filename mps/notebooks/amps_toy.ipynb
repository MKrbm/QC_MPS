{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import geoopt\n",
    "\n",
    "# If your mps/ package is local, ensure itâ€™s on sys.path or installed in editable mode:\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from mps.simple_mps import SimpleMPS\n",
    "from mps.tpcp_mps import MPSTPCP, ManifoldType\n",
    "from mps.StiefelOptimizers import StiefelAdam, StiefelSGD\n",
    "from mps.radam import RiemannianAdam\n",
    "\n",
    "# Enable inline plotting in Jupyter\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Each sample is an n-dimensional vector where the first element is either 0 or 1,\n",
    "    and the remaining n-1 entries are 0. The label is the first element.\n",
    "    We embed each scalar x to a 2-d vector [x, 1-x] and then L2-normalize.\n",
    "    \"\"\"\n",
    "    def __init__(self, n: int, num_samples: int = 10000, seed: int | None = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n: Dimension of the vector.\n",
    "            num_samples: Number of samples in the dataset.\n",
    "            seed: Random seed (optional).\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.num_samples = num_samples\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        # Randomly assign labels (0 or 1) for each sample\n",
    "        self.labels = np.random.randint(0, 2, size=num_samples).astype(np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # The label is either 0 or 1\n",
    "        l = self.labels[index]\n",
    "        # Create a vector of length n with first element l and rest zeros\n",
    "        x = torch.zeros(self.n, dtype=torch.float64)\n",
    "        x[0] = float(l)\n",
    "        # Embed each scalar: map x -> [x, 1-x]\n",
    "        x_embedded = torch.stack([x, 1 - x], dim=-1)  # shape: [n, 2]\n",
    "        # Normalize each site so that the two entries sum to 1\n",
    "        x_embedded = x_embedded / (x_embedded.sum(dim=-1, keepdim=True).clamp(min=1e-8))\n",
    "        # The target label is l\n",
    "        return x_embedded, torch.tensor(l, dtype=torch.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(outputs, labels):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy style loss for outputs in [0, 1].\n",
    "    For label=0 => use outputs[i], for label=1 => use 1 - outputs[i].\n",
    "    \"\"\"\n",
    "    device = outputs.device\n",
    "    loss_val = torch.zeros(1, device=device, dtype=torch.float64)\n",
    "    for i in range(len(outputs)):\n",
    "        prob = outputs[i] if labels[i] == 0 else (1 - outputs[i])\n",
    "        loss_val -= torch.log(prob + 1e-8)\n",
    "        if torch.isnan(loss_val):\n",
    "            print(f\"NaN in loss at index {i}, prob={prob}, output={outputs[i]}, label={labels[i]}\")\n",
    "    return loss_val\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Threshold outputs at 0.5 to assign label 0 or 1 and compare to true labels.\n",
    "    \"\"\"\n",
    "    predictions = (outputs < 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    return correct / labels.numel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared:\n",
      "Number of samples: 1000\n",
      "Batch size: 64\n"
     ]
    }
   ],
   "source": [
    "# Configuration for debugging\n",
    "n = 100          # Use a smaller input dimension for faster runs\n",
    "num_data = 1000 # Smaller dataset for quick debugging\n",
    "batch_size = 64\n",
    "\n",
    "synthetic_dataset = SyntheticDataset(n=n, num_samples=num_data, seed=42)\n",
    "dataloader = torch.utils.data.DataLoader(synthetic_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(\"Dataset prepared:\")\n",
    "print(\"Number of samples:\", len(synthetic_dataset))\n",
    "print(\"Batch size:\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS with random matrices\n",
      "\n",
      "=== Training SimpleMPS for 3 epochs (debug mode)... ===\n",
      "[SimpleMPS] Epoch 1 | Loss: 0.693003 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 2 | Loss: 0.688807 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 3 | Loss: 0.622389 | Accuracy: 54.60%\n",
      "[SimpleMPS] Epoch 4 | Loss: 0.444156 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 5 | Loss: 0.421245 | Accuracy: 54.50%\n",
      "[SimpleMPS] Epoch 6 | Loss: 0.389004 | Accuracy: 54.10%\n",
      "[SimpleMPS] Epoch 7 | Loss: 0.355695 | Accuracy: 72.50%\n",
      "[SimpleMPS] Epoch 8 | Loss: 0.507542 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 9 | Loss: 0.416707 | Accuracy: 57.50%\n",
      "[SimpleMPS] Epoch 10 | Loss: 0.484801 | Accuracy: 51.00%\n",
      "[SimpleMPS] Epoch 11 | Loss: 0.400196 | Accuracy: 67.90%\n",
      "[SimpleMPS] Epoch 12 | Loss: 0.352454 | Accuracy: 77.20%\n",
      "[SimpleMPS] Epoch 13 | Loss: 0.394807 | Accuracy: 69.70%\n",
      "[SimpleMPS] Epoch 14 | Loss: 0.426074 | Accuracy: 58.90%\n",
      "[SimpleMPS] Epoch 15 | Loss: 0.330534 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 16 | Loss: 0.202745 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 17 | Loss: 0.088731 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 18 | Loss: 0.012066 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 19 | Loss: 0.001677 | Accuracy: 100.00%\n",
      "[SimpleMPS] Epoch 20 | Loss: 0.000598 | Accuracy: 100.00%\n",
      "SimpleMPS training complete.\n"
     ]
    }
   ],
   "source": [
    "# Training a SimpleMPS model (demo)\n",
    "N = n\n",
    "d = l = 2  \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "smps = SimpleMPS(\n",
    "    N,\n",
    "    2,\n",
    "    d,\n",
    "    l,\n",
    "    layers=2,\n",
    "    device=device,\n",
    "    dtype=torch.float64,\n",
    "    optimize=\"greedy\",\n",
    ")\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "nnloss = torch.nn.NLLLoss(reduction=\"mean\")\n",
    "opt_smps = torch.optim.Adam(smps.parameters(), lr=0.001)\n",
    "\n",
    "smps_losses = []\n",
    "smps.train()\n",
    "print(\"\\n=== Training SimpleMPS for 3 epochs (debug mode)... ===\")\n",
    "for epoch in range(20):  # Use fewer epochs for debugging\n",
    "    total_loss_smps = 0.0\n",
    "    total_samples_smps = 0\n",
    "    total_correct_smps = 0\n",
    "    for batch_idx, (data, target) in enumerate(dataloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.permute(1, 0, 2)\n",
    "        opt_smps.zero_grad()\n",
    "        outputs = smps(data)  # shape: [batch_size, 2]\n",
    "        outputs = logsoftmax(outputs)\n",
    "        loss = nnloss(outputs, target)\n",
    "        loss.backward()\n",
    "        opt_smps.step()\n",
    "\n",
    "        bs = target.size(0)\n",
    "        total_loss_smps += loss.item() * bs\n",
    "        total_samples_smps += bs\n",
    "\n",
    "        preds = outputs.argmax(dim=-1)\n",
    "        total_correct_smps += (preds == target).float().sum().item()\n",
    "\n",
    "    epoch_loss = total_loss_smps / total_samples_smps\n",
    "    epoch_accuracy = total_correct_smps / total_samples_smps\n",
    "    smps_losses.append(epoch_loss)\n",
    "    print(f\"[SimpleMPS] Epoch {epoch+1} | Loss: {epoch_loss:.6f} | Accuracy: {epoch_accuracy:.2%}\")\n",
    "\n",
    "print(\"SimpleMPS training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPCP model built and canonical form set from SimpleMPS.\n",
      "TPCP optimizer set up.\n"
     ]
    }
   ],
   "source": [
    "from mps import tpcp_mps\n",
    "from importlib import reload\n",
    "\n",
    "reload(tpcp_mps)\n",
    "\n",
    "# Build TPCP model with original manifold\n",
    "tpcp = tpcp_mps.MPSTPCP(N, K=1, d=2, with_pros=False, with_identity=True, manifold=tpcp_mps.ManifoldType.EXACT)\n",
    "tpcp.to(device)\n",
    "tpcp.train()\n",
    "tpcp.set_canonical_mps(smps, set_r=True)\n",
    "print(\"TPCP model built and canonical form set from SimpleMPS.\")\n",
    "\n",
    "# Choose an optimizer for TPCP\n",
    "opt_tpcp = RiemannianAdam(tpcp.kraus_ops.parameters(), lr=0.01)\n",
    "print(\"TPCP optimizer set up.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.zeros(tpcp.L, 2, dtype=torch.float64)\n",
    "W[:, 0] = 1\n",
    "W[:, 1] = 0\n",
    "tpcp.initialize_W(W)\n",
    "\n",
    "def accuracy(outputs, target):\n",
    "    correct = (outputs < 0).float() == target.float()\n",
    "    return correct.float().sum() / target.numel()\n",
    "\n",
    "def to_probs(outputs):\n",
    "    outputs = outputs / outputs.sum(dim=-1).unsqueeze(-1)\n",
    "    return outputs\n",
    "\n",
    "data, target = next(iter(dataloader))\n",
    "\n",
    "out = tpcp(data)\n",
    "probs = to_probs(out)\n",
    "\n",
    "calculate_accuracy(probs[:, 0], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 5\n",
    "for i in range(max_epochs):\n",
    "  out = tpcp(data)\n",
    "  probs = to_probs(out)\n",
    "  loss = loss_batch(probs[:, 0], target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for TPCP training (using fewer epochs and a subset of weight values for debugging)\n",
    "max_epochs = 5\n",
    "min_epochs = 2\n",
    "conv_threshold = 1e-4\n",
    "log_steps = 5\n",
    "\n",
    "w_values = [0, 0.5, 1]  # Use a few weight rates for debugging\n",
    "\n",
    "for w_ in w_values:\n",
    "    # Re-initialize weight parameter W for current w_\n",
    "    W = torch.zeros(tpcp.L, 2, dtype=torch.float64, device=device)\n",
    "    W[:, 0] = 1\n",
    "    W[:, 1] = w_\n",
    "    tpcp.initialize_W(W)\n",
    "    \n",
    "    print(f\"\\n=== Training TPCP with initial weight rate w={w_:.1f} (max_epochs={max_epochs}) ===\")\n",
    "    epoch = 0\n",
    "    prev_epoch_loss = None\n",
    "\n",
    "    while epoch < max_epochs:\n",
    "        epoch_loss_sum = 0.0\n",
    "        total_samples = 0\n",
    "        epoch_acc_sum = 0.0\n",
    "        total_acc_samples = 0\n",
    "        t0 = time.time()\n",
    "        for step, (data, target) in enumerate(dataloader):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            bs = target.size(0)\n",
    "            \n",
    "            opt_tpcp.zero_grad()\n",
    "            outputs = tpcp(data)\n",
    "            if torch.isnan(outputs).any():\n",
    "                print(f\"NaN detected in outputs at step {step}, skipping batch.\")\n",
    "                continue\n",
    "            \n",
    "            loss_val = loss_batch(outputs, target)\n",
    "            if torch.isnan(loss_val):\n",
    "                print(\"NaN detected in batch loss, skipping batch.\")\n",
    "                continue\n",
    "            \n",
    "            loss_val.backward()\n",
    "            opt_tpcp.step()\n",
    "            tpcp.proj_stiefel(check_on_manifold=True, print_log=False, rtol=1e-3)\n",
    "            \n",
    "            epoch_loss_sum += loss_val.item() * bs\n",
    "            total_samples += bs\n",
    "            batch_acc = calculate_accuracy(outputs.detach(), target)\n",
    "            epoch_acc_sum += batch_acc.item() * bs\n",
    "            total_acc_samples += bs\n",
    "            \n",
    "            if (step + 1) % log_steps == 0:\n",
    "                print(f\"[TPCP::w={w_:.1f}] Epoch {epoch+1}, Step {step+1}/{len(dataloader)} | Batch Loss: {loss_val.item():.6f} | Batch Accuracy: {batch_acc.item():.2%}\")\n",
    "\n",
    "        if total_samples == 0:\n",
    "            print(\"No samples processed in this epoch, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        avg_loss = epoch_loss_sum / total_samples\n",
    "        avg_acc = epoch_acc_sum / total_acc_samples\n",
    "        current_weight_rate = (tpcp.W[:, 1] / torch.sum(tpcp.W, dim=1)).mean().item() if hasattr(tpcp, \"W\") else w_\n",
    "        \n",
    "        elapsed = time.time() - t0\n",
    "        print(f\"[TPCP::w={w_:.1f}] Epoch {epoch+1} | Avg Loss: {avg_loss:.6f}, Avg Acc: {avg_acc:.2%}, Weight Rate: {current_weight_rate:.4f} | Time: {elapsed:.2f}s\")\n",
    "        \n",
    "        if epoch >= min_epochs and prev_epoch_loss is not None:\n",
    "            if abs(avg_loss - prev_epoch_loss) < conv_threshold:\n",
    "                print(f\"Convergence reached: Loss change below threshold for epoch {epoch+1}.\")\n",
    "                break\n",
    "\n",
    "        prev_epoch_loss = avg_loss\n",
    "        epoch += 1\n",
    "\n",
    "print(\"\\nTPCP training complete.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
