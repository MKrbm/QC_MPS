{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opt_einsum as oe\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mps.simple_mps' from '/Users/keisuke/Documents/presentation/QC_MPS/mps/notebooks/../../mps/simple_mps.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from mps import simple_mps, tpcp_mps\n",
    "reload(simple_mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_digits(dataset, allowed_digits=[0, 1]):\n",
    "    \"\"\"Return a subset of MNIST dataset containing only allowed_digits (0 or 1).\"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if label in allowed_digits:\n",
    "            indices.append(i)\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "\n",
    "def filiter_single_channel(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    MNIST is loaded as shape [C, H, W].\n",
    "    Take only the first channel => shape [H, W].\n",
    "    \"\"\"\n",
    "    return img[0, ...]\n",
    "\n",
    "\n",
    "def embedding_pixel(batch, label: int = 0):\n",
    "    \"\"\"\n",
    "    Flatten each image from shape [H, W] => [H*W],\n",
    "    then embed x => [x, 1-x], and L2-normalize along last dim.\n",
    "    \"\"\"\n",
    "    pixel_size = batch.shape[-1] * batch.shape[-2]\n",
    "    x = batch.view(*batch.shape[:-2], pixel_size)\n",
    "    x = torch.stack([x, 1 - x], dim=-1)\n",
    "    x = x / torch.sum(x, dim=-1).unsqueeze(-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Loss & Accuracy\n",
    "###############################################################################\n",
    "def loss_batch(outputs, labels):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy style loss for outputs in [0, 1].\n",
    "    For label=0 => prob=outputs[i], else => 1 - outputs[i].\n",
    "    \"\"\"\n",
    "    device = outputs.device\n",
    "    loss = torch.zeros(1, device=device, dtype=torch.float64)\n",
    "    for i in range(len(outputs)):\n",
    "        prob = outputs[i] if labels[i] == 0 else (1 - outputs[i])\n",
    "        loss -= torch.log(prob + 1e-8)\n",
    "        # Start of Selection\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Loss is NaN at i={i}\")\n",
    "            print(prob, outputs[i], labels[i])\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Threshold 0.5 => label 0 or 1. Compare to true labels.\n",
    "    \"\"\"\n",
    "    predictions = (outputs < 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    return correct / labels.numel()\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "img_size = 16\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(filiter_single_channel),\n",
    "        transforms.Lambda(embedding_pixel),\n",
    "        transforms.Lambda(lambda x: x.to(torch.float64)),  # double precision\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "# Filter digits 0,1 only\n",
    "trainset = filter_digits(trainset, allowed_digits=[0, 1])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS with random matrices\n"
     ]
    }
   ],
   "source": [
    "# ---------- Build MPS model ----------\n",
    "N = img_size * img_size\n",
    "d = l = 2 #data input dimension and class label dimension \n",
    "chi_umps = 2\n",
    "chi_max = 2\n",
    "reload(simple_mps)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "smps = simple_mps.SimpleMPS(\n",
    "    N, \n",
    "    2,\n",
    "    d, \n",
    "    l, \n",
    "    layers=2,\n",
    "    device=device, \n",
    "    dtype=torch.float64, \n",
    "    optimize=\"greedy\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/12665 (0%)]\tLoss: 0.693380 Accuracy: 42.97%\n",
      "Train Epoch: 0 [128/12665 (1%)]\tLoss: 0.693088 Accuracy: 47.66%\n",
      "Train Epoch: 0 [256/12665 (2%)]\tLoss: 0.692907 Accuracy: 53.91%\n",
      "Train Epoch: 0 [384/12665 (3%)]\tLoss: 0.692651 Accuracy: 53.12%\n",
      "Train Epoch: 0 [512/12665 (4%)]\tLoss: 0.692105 Accuracy: 53.91%\n",
      "Train Epoch: 0 [640/12665 (5%)]\tLoss: 0.691262 Accuracy: 51.56%\n",
      "Train Epoch: 0 [768/12665 (6%)]\tLoss: 0.688629 Accuracy: 55.47%\n",
      "Train Epoch: 0 [896/12665 (7%)]\tLoss: 0.686424 Accuracy: 52.34%\n",
      "Train Epoch: 0 [1024/12665 (8%)]\tLoss: 0.683018 Accuracy: 49.22%\n",
      "Train Epoch: 0 [1152/12665 (9%)]\tLoss: 0.671369 Accuracy: 53.12%\n",
      "Train Epoch: 0 [1280/12665 (10%)]\tLoss: 0.652862 Accuracy: 54.69%\n",
      "Train Epoch: 0 [1408/12665 (11%)]\tLoss: 0.643309 Accuracy: 50.78%\n",
      "Train Epoch: 0 [1536/12665 (12%)]\tLoss: 0.600847 Accuracy: 57.03%\n",
      "Train Epoch: 0 [1664/12665 (13%)]\tLoss: 0.607812 Accuracy: 45.31%\n",
      "Train Epoch: 0 [1792/12665 (14%)]\tLoss: 0.448316 Accuracy: 62.50%\n",
      "Train Epoch: 0 [1920/12665 (15%)]\tLoss: 0.556803 Accuracy: 52.34%\n",
      "Train Epoch: 0 [2048/12665 (16%)]\tLoss: 0.619577 Accuracy: 45.31%\n",
      "Train Epoch: 0 [2176/12665 (17%)]\tLoss: 0.493634 Accuracy: 53.12%\n",
      "Train Epoch: 0 [2304/12665 (18%)]\tLoss: 0.467783 Accuracy: 55.47%\n",
      "Train Epoch: 0 [2432/12665 (19%)]\tLoss: 0.525273 Accuracy: 46.88%\n",
      "Train Epoch: 0 [2560/12665 (20%)]\tLoss: 0.464525 Accuracy: 56.25%\n",
      "Train Epoch: 0 [2688/12665 (21%)]\tLoss: 0.498498 Accuracy: 49.22%\n",
      "Train Epoch: 0 [2816/12665 (22%)]\tLoss: 0.447078 Accuracy: 54.69%\n",
      "Train Epoch: 0 [2944/12665 (23%)]\tLoss: 0.464165 Accuracy: 53.12%\n",
      "Train Epoch: 0 [3072/12665 (24%)]\tLoss: 0.399482 Accuracy: 58.59%\n",
      "Train Epoch: 0 [3200/12665 (25%)]\tLoss: 0.438300 Accuracy: 52.34%\n",
      "Train Epoch: 0 [3328/12665 (26%)]\tLoss: 0.430640 Accuracy: 52.34%\n",
      "Train Epoch: 0 [3456/12665 (27%)]\tLoss: 0.367207 Accuracy: 57.81%\n",
      "Train Epoch: 0 [3584/12665 (28%)]\tLoss: 0.367783 Accuracy: 58.59%\n",
      "Train Epoch: 0 [3712/12665 (29%)]\tLoss: 0.408274 Accuracy: 57.81%\n",
      "Train Epoch: 0 [3840/12665 (30%)]\tLoss: 0.357102 Accuracy: 59.38%\n",
      "Train Epoch: 0 [3968/12665 (31%)]\tLoss: 0.365995 Accuracy: 61.72%\n",
      "Train Epoch: 0 [4096/12665 (32%)]\tLoss: 0.375240 Accuracy: 72.66%\n",
      "Train Epoch: 0 [4224/12665 (33%)]\tLoss: 0.347924 Accuracy: 86.72%\n",
      "Train Epoch: 0 [4352/12665 (34%)]\tLoss: 0.367512 Accuracy: 79.69%\n",
      "Train Epoch: 0 [4480/12665 (35%)]\tLoss: 0.321474 Accuracy: 92.19%\n",
      "Train Epoch: 0 [4608/12665 (36%)]\tLoss: 0.343170 Accuracy: 81.25%\n",
      "Train Epoch: 0 [4736/12665 (37%)]\tLoss: 0.378336 Accuracy: 75.00%\n",
      "Train Epoch: 0 [4864/12665 (38%)]\tLoss: 0.317848 Accuracy: 86.72%\n",
      "Train Epoch: 0 [4992/12665 (39%)]\tLoss: 0.283388 Accuracy: 97.66%\n",
      "Train Epoch: 0 [5120/12665 (40%)]\tLoss: 0.264976 Accuracy: 96.88%\n",
      "Train Epoch: 0 [5248/12665 (41%)]\tLoss: 0.287292 Accuracy: 93.75%\n",
      "Train Epoch: 0 [5376/12665 (42%)]\tLoss: 0.264116 Accuracy: 96.09%\n",
      "Train Epoch: 0 [5504/12665 (43%)]\tLoss: 0.299542 Accuracy: 96.09%\n",
      "Train Epoch: 0 [5632/12665 (44%)]\tLoss: 0.211937 Accuracy: 99.22%\n",
      "Train Epoch: 0 [5760/12665 (45%)]\tLoss: 0.222654 Accuracy: 100.00%\n",
      "Train Epoch: 0 [5888/12665 (46%)]\tLoss: 0.185217 Accuracy: 99.22%\n",
      "Train Epoch: 0 [6016/12665 (47%)]\tLoss: 0.162523 Accuracy: 99.22%\n",
      "Train Epoch: 0 [6144/12665 (48%)]\tLoss: 0.098302 Accuracy: 98.44%\n",
      "Train Epoch: 0 [6272/12665 (49%)]\tLoss: 0.039280 Accuracy: 100.00%\n",
      "Train Epoch: 0 [6400/12665 (51%)]\tLoss: 0.047489 Accuracy: 98.44%\n",
      "Train Epoch: 0 [6528/12665 (52%)]\tLoss: 0.111772 Accuracy: 97.66%\n",
      "Train Epoch: 0 [6656/12665 (53%)]\tLoss: 0.041544 Accuracy: 98.44%\n",
      "Train Epoch: 0 [6784/12665 (54%)]\tLoss: 0.042638 Accuracy: 99.22%\n",
      "Train Epoch: 0 [6912/12665 (55%)]\tLoss: 0.030871 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7040/12665 (56%)]\tLoss: 0.012337 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7168/12665 (57%)]\tLoss: 0.012033 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7296/12665 (58%)]\tLoss: 0.001145 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7424/12665 (59%)]\tLoss: 0.000006 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7552/12665 (60%)]\tLoss: 0.021539 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7680/12665 (61%)]\tLoss: 0.000002 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7808/12665 (62%)]\tLoss: 0.000003 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7936/12665 (63%)]\tLoss: 0.173194 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8064/12665 (64%)]\tLoss: 0.166657 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8192/12665 (65%)]\tLoss: 0.000001 Accuracy: 100.00%\n",
      "Train Epoch: 0 [8320/12665 (66%)]\tLoss: 0.073696 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8448/12665 (67%)]\tLoss: 0.090236 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8576/12665 (68%)]\tLoss: 0.009253 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8704/12665 (69%)]\tLoss: 0.007290 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8832/12665 (70%)]\tLoss: 0.003033 Accuracy: 100.00%\n",
      "Train Epoch: 0 [8960/12665 (71%)]\tLoss: 0.000552 Accuracy: 100.00%\n",
      "Train Epoch: 0 [9088/12665 (72%)]\tLoss: 0.013449 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9216/12665 (73%)]\tLoss: 0.081734 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9344/12665 (74%)]\tLoss: 0.229856 Accuracy: 89.06%\n",
      "Train Epoch: 0 [9472/12665 (75%)]\tLoss: 0.024697 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9600/12665 (76%)]\tLoss: 0.011546 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9728/12665 (77%)]\tLoss: 0.017467 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9856/12665 (78%)]\tLoss: 0.006282 Accuracy: 100.00%\n",
      "Train Epoch: 0 [9984/12665 (79%)]\tLoss: 0.010869 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10112/12665 (80%)]\tLoss: 0.007901 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10240/12665 (81%)]\tLoss: 0.011657 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10368/12665 (82%)]\tLoss: 0.017381 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10496/12665 (83%)]\tLoss: 0.016126 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10624/12665 (84%)]\tLoss: 0.041070 Accuracy: 98.44%\n",
      "Train Epoch: 0 [10752/12665 (85%)]\tLoss: 0.011674 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10880/12665 (86%)]\tLoss: 0.009482 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11008/12665 (87%)]\tLoss: 0.016412 Accuracy: 99.22%\n",
      "Train Epoch: 0 [11136/12665 (88%)]\tLoss: 0.007578 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11264/12665 (89%)]\tLoss: 0.002160 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11392/12665 (90%)]\tLoss: 0.006941 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11520/12665 (91%)]\tLoss: 0.010545 Accuracy: 99.22%\n",
      "Train Epoch: 0 [11648/12665 (92%)]\tLoss: 0.000797 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11776/12665 (93%)]\tLoss: 0.000316 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11904/12665 (94%)]\tLoss: 0.001165 Accuracy: 100.00%\n",
      "Train Epoch: 0 [12032/12665 (95%)]\tLoss: 0.001121 Accuracy: 100.00%\n",
      "Train Epoch: 0 [12160/12665 (96%)]\tLoss: 0.001547 Accuracy: 100.00%\n",
      "Train Epoch: 0 [12288/12665 (97%)]\tLoss: 0.095685 Accuracy: 98.44%\n",
      "Train Epoch: 0 [12416/12665 (98%)]\tLoss: 0.000020 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11858/12665 (99%)]\tLoss: 0.000086 Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(outputs, target):\n",
    "    return (outputs.argmax(dim=-1) == target).float().mean()\n",
    "losses = []\n",
    "running_loss = 0\n",
    "running_accuracy = 0\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "nnloss = torch.nn.NLLLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(smps.parameters(), lr=0.001)\n",
    "n_samples = 0\n",
    "for epoch in range(1):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        target = target.to(device).to(torch.int64)\n",
    "        data = data.to(device).permute(1, 0, 2)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = smps(data)\n",
    "        outputs = logsoftmax(outputs)\n",
    "        loss = nnloss(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data_size = data.shape[1]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        # print(torch.exp(outputs[:10]), target[:10])\n",
    "        \n",
    "        running_loss += loss.item() * data_size\n",
    "        n_samples += data_size\n",
    "        \n",
    "        if batch_idx % 1 == 0:\n",
    "            avg_loss = running_loss / n_samples\n",
    "            avg_accuracy = accuracy(outputs, target)\n",
    "            losses.append(avg_loss)\n",
    "            running_loss = 0\n",
    "            running_accuracy = 0\n",
    "            n_samples = 0\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Accuracy: {:.2f}%'.format(\n",
    "                epoch, batch_idx * data_size, len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), avg_loss, avg_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mps import tpcp_mps  \n",
    "from mps.trainer import utils\n",
    "\n",
    "reload(tpcp_mps)\n",
    "\n",
    "tpcp = tpcp_mps.MPSTPCP(N, K=1, d=2, enable_r=False, with_identity=True, manifold=tpcp_mps.ManifoldType.EXACT)\n",
    "\n",
    "W = torch.zeros(tpcp.L, 2, dtype=torch.float64)\n",
    "W[:, 0] = 1\n",
    "W[:, 1] = 0\n",
    "tpcp.initialize_W(W)\n",
    "\n",
    "tpcp.set_canonical_mps(smps)\n",
    "\n",
    "\n",
    "def accuracy(outputs, target):\n",
    "    correct = (outputs < 0).float() == target.float()\n",
    "    return correct.float().sum() / target.numel()\n",
    "\n",
    "data, target = next(iter(trainloader))\n",
    "out = tpcp(data)\n",
    "out = utils.to_probs(out)[:, 0]\n",
    "\n",
    "calculate_accuracy(out, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  22.68030837581526 Reg Weight:  0.6931471805599453 Loss with Weight:  22.749623093871257 Accuracy:  1.0\n",
      "Loss:  64.90553578842453 Reg Weight:  0.6931471805599453 Loss with Weight:  64.97485050648052 Accuracy:  0.8203125\n",
      "Loss:  25.002029693919006 Reg Weight:  0.6931471805599453 Loss with Weight:  25.071344411975 Accuracy:  0.9921875\n",
      "Loss:  26.361382362243358 Reg Weight:  0.6931471805599453 Loss with Weight:  26.430697080299353 Accuracy:  1.0\n",
      "Loss:  31.46013168467533 Reg Weight:  0.6931471805599453 Loss with Weight:  31.529446402731324 Accuracy:  1.0\n",
      "Loss:  36.44273384145295 Reg Weight:  0.6931471805599453 Loss with Weight:  36.51204855950894 Accuracy:  1.0\n",
      "Loss:  39.59856423755742 Reg Weight:  0.6931471805599453 Loss with Weight:  39.66787895561341 Accuracy:  0.9921875\n",
      "Loss:  40.386162122988196 Reg Weight:  0.6931471805599453 Loss with Weight:  40.45547684104419 Accuracy:  1.0\n",
      "Loss:  42.14323843626131 Reg Weight:  0.6931471805599453 Loss with Weight:  42.212553154317305 Accuracy:  1.0\n",
      "Loss:  41.148013876465605 Reg Weight:  0.6931471805599453 Loss with Weight:  41.2173285945216 Accuracy:  0.9921875\n",
      "Loss:  39.06034364938381 Reg Weight:  0.6931471805599453 Loss with Weight:  39.1296583674398 Accuracy:  1.0\n",
      "Loss:  40.0477333352025 Reg Weight:  0.6931471805599453 Loss with Weight:  40.11704805325849 Accuracy:  1.0\n",
      "Loss:  37.95405955888815 Reg Weight:  0.6931471805599453 Loss with Weight:  38.02337427694414 Accuracy:  1.0\n",
      "Loss:  37.54914208223324 Reg Weight:  0.6931471805599453 Loss with Weight:  37.61845680028923 Accuracy:  1.0\n",
      "Loss:  32.911825001250435 Reg Weight:  0.6931471805599453 Loss with Weight:  32.98113971930643 Accuracy:  1.0\n",
      "Loss:  30.370421560827747 Reg Weight:  0.6931471805599453 Loss with Weight:  30.439736278883743 Accuracy:  1.0\n",
      "Loss:  30.227674150008095 Reg Weight:  0.6931471805599453 Loss with Weight:  30.29698886806409 Accuracy:  1.0\n",
      "Loss:  25.280270355048962 Reg Weight:  0.6931471805599453 Loss with Weight:  25.349585073104958 Accuracy:  1.0\n",
      "Loss:  22.464317126593187 Reg Weight:  0.6931471805599453 Loss with Weight:  22.533631844649182 Accuracy:  0.9921875\n",
      "Loss:  24.062194984120953 Reg Weight:  0.6931471805599453 Loss with Weight:  24.13150970217695 Accuracy:  1.0\n",
      "Loss:  21.39288346935221 Reg Weight:  0.6931471805599453 Loss with Weight:  21.462198187408205 Accuracy:  1.0\n",
      "Loss:  29.84107149680126 Reg Weight:  0.6931471805599453 Loss with Weight:  29.910386214857255 Accuracy:  0.9921875\n",
      "Loss:  26.461907697290233 Reg Weight:  0.6931471805599453 Loss with Weight:  26.531222415346228 Accuracy:  1.0\n",
      "Loss:  27.703393775357988 Reg Weight:  0.6931471805599453 Loss with Weight:  27.772708493413983 Accuracy:  1.0\n",
      "Loss:  21.281698235050534 Reg Weight:  0.6931471805599453 Loss with Weight:  21.35101295310653 Accuracy:  1.0\n",
      "Loss:  22.526259984475576 Reg Weight:  0.6931471805599453 Loss with Weight:  22.59557470253157 Accuracy:  0.9921875\n",
      "Loss:  23.483306909020758 Reg Weight:  0.6931471805599453 Loss with Weight:  23.552621627076753 Accuracy:  0.9921875\n",
      "Loss:  20.80095109530128 Reg Weight:  0.6931471805599453 Loss with Weight:  20.870265813357275 Accuracy:  1.0\n",
      "Loss:  23.349259791072697 Reg Weight:  0.6931471805599453 Loss with Weight:  23.418574509128693 Accuracy:  0.9921875\n",
      "Loss:  23.891056985591334 Reg Weight:  0.6931471805599453 Loss with Weight:  23.96037170364733 Accuracy:  1.0\n",
      "Loss:  22.947968582104696 Reg Weight:  0.6931471805599453 Loss with Weight:  23.017283300160692 Accuracy:  1.0\n",
      "Loss:  23.053053944529683 Reg Weight:  0.6931471805599453 Loss with Weight:  23.12236866258568 Accuracy:  1.0\n",
      "Loss:  23.746557139532104 Reg Weight:  0.6931471805599453 Loss with Weight:  23.8158718575881 Accuracy:  1.0\n",
      "Loss:  23.55056039310918 Reg Weight:  0.6931471805599453 Loss with Weight:  23.619875111165175 Accuracy:  0.9921875\n",
      "Loss:  21.80934518545889 Reg Weight:  0.6931471805599453 Loss with Weight:  21.878659903514887 Accuracy:  1.0\n",
      "Loss:  23.63147900870335 Reg Weight:  0.6931471805599453 Loss with Weight:  23.700793726759347 Accuracy:  0.984375\n",
      "Loss:  22.133361667252636 Reg Weight:  0.6931471805599453 Loss with Weight:  22.20267638530863 Accuracy:  0.9921875\n",
      "Loss:  22.088237273640434 Reg Weight:  0.6931471805599453 Loss with Weight:  22.15755199169643 Accuracy:  1.0\n",
      "Loss:  21.479652166556463 Reg Weight:  0.6931471805599453 Loss with Weight:  21.548966884612458 Accuracy:  1.0\n",
      "Loss:  19.10860142416907 Reg Weight:  0.6931471805599453 Loss with Weight:  19.177916142225065 Accuracy:  1.0\n",
      "Loss:  18.061042369092284 Reg Weight:  0.6931471805599453 Loss with Weight:  18.13035708714828 Accuracy:  1.0\n",
      "Loss:  19.73925037163706 Reg Weight:  0.6931471805599453 Loss with Weight:  19.808565089693055 Accuracy:  0.9921875\n",
      "Loss:  19.025375229313187 Reg Weight:  0.6931471805599453 Loss with Weight:  19.094689947369183 Accuracy:  1.0\n",
      "Loss:  21.87853442957472 Reg Weight:  0.6931471805599453 Loss with Weight:  21.947849147630716 Accuracy:  0.984375\n",
      "Loss:  17.94512954948766 Reg Weight:  0.6931471805599453 Loss with Weight:  18.014444267543656 Accuracy:  1.0\n",
      "Loss:  21.153316882184914 Reg Weight:  0.6931471805599453 Loss with Weight:  21.22263160024091 Accuracy:  1.0\n",
      "Loss:  20.67041344632565 Reg Weight:  0.6931471805599453 Loss with Weight:  20.739728164381646 Accuracy:  0.9921875\n",
      "Loss:  21.946030116831142 Reg Weight:  0.6931471805599453 Loss with Weight:  22.015344834887138 Accuracy:  0.9921875\n",
      "Loss:  21.4405089657193 Reg Weight:  0.6931471805599453 Loss with Weight:  21.509823683775295 Accuracy:  1.0\n",
      "Loss:  22.574667224610632 Reg Weight:  0.6931471805599453 Loss with Weight:  22.643981942666628 Accuracy:  0.9921875\n",
      "Loss:  19.336049937615435 Reg Weight:  0.6931471805599453 Loss with Weight:  19.40536465567143 Accuracy:  0.9921875\n",
      "Loss:  19.875440084475475 Reg Weight:  0.6931471805599453 Loss with Weight:  19.94475480253147 Accuracy:  1.0\n",
      "Loss:  17.78819264869343 Reg Weight:  0.6931471805599453 Loss with Weight:  17.857507366749424 Accuracy:  0.9921875\n",
      "Loss:  15.636319612594894 Reg Weight:  0.6931471805599453 Loss with Weight:  15.705634330650888 Accuracy:  1.0\n",
      "Loss:  19.489258893087992 Reg Weight:  0.6931471805599453 Loss with Weight:  19.558573611143988 Accuracy:  1.0\n",
      "Loss:  17.56252525462712 Reg Weight:  0.6931471805599453 Loss with Weight:  17.631839972683114 Accuracy:  1.0\n",
      "Loss:  18.260089478357 Reg Weight:  0.6931471805599453 Loss with Weight:  18.329404196412995 Accuracy:  1.0\n",
      "Loss:  20.704532964992783 Reg Weight:  0.6931471805599453 Loss with Weight:  20.77384768304878 Accuracy:  0.9921875\n",
      "Loss:  16.57020859316336 Reg Weight:  0.6931471805599453 Loss with Weight:  16.639523311219357 Accuracy:  1.0\n",
      "Loss:  21.17996560715818 Reg Weight:  0.6931471805599453 Loss with Weight:  21.249280325214176 Accuracy:  0.9921875\n",
      "Loss:  19.67320716833462 Reg Weight:  0.6931471805599453 Loss with Weight:  19.742521886390616 Accuracy:  1.0\n",
      "Loss:  17.528594680697243 Reg Weight:  0.6931471805599453 Loss with Weight:  17.597909398753238 Accuracy:  1.0\n",
      "Loss:  21.433913475581125 Reg Weight:  0.6931471805599453 Loss with Weight:  21.50322819363712 Accuracy:  0.9921875\n",
      "Loss:  20.743015250953913 Reg Weight:  0.6931471805599453 Loss with Weight:  20.81232996900991 Accuracy:  0.9921875\n",
      "Loss:  16.84524579101108 Reg Weight:  0.6931471805599453 Loss with Weight:  16.914560509067076 Accuracy:  1.0\n",
      "Loss:  20.580905715564658 Reg Weight:  0.6931471805599453 Loss with Weight:  20.650220433620653 Accuracy:  0.984375\n",
      "Loss:  21.79464116443372 Reg Weight:  0.6931471805599453 Loss with Weight:  21.863955882489716 Accuracy:  0.9921875\n",
      "Loss:  17.827423579458603 Reg Weight:  0.6931471805599453 Loss with Weight:  17.8967382975146 Accuracy:  0.9921875\n",
      "Loss:  20.460916476334262 Reg Weight:  0.6931471805599453 Loss with Weight:  20.530231194390257 Accuracy:  1.0\n",
      "Loss:  16.45542828377315 Reg Weight:  0.6931471805599453 Loss with Weight:  16.524743001829144 Accuracy:  1.0\n",
      "Loss:  17.63773525863026 Reg Weight:  0.6931471805599453 Loss with Weight:  17.707049976686257 Accuracy:  0.9921875\n",
      "Loss:  17.946771155138762 Reg Weight:  0.6931471805599453 Loss with Weight:  18.016085873194758 Accuracy:  1.0\n",
      "Loss:  16.627416928795693 Reg Weight:  0.6931471805599453 Loss with Weight:  16.69673164685169 Accuracy:  0.9921875\n",
      "Loss:  16.954864373939717 Reg Weight:  0.6931471805599453 Loss with Weight:  17.024179091995713 Accuracy:  1.0\n",
      "Loss:  18.504458665080662 Reg Weight:  0.6931471805599453 Loss with Weight:  18.573773383136658 Accuracy:  1.0\n",
      "Loss:  19.472266397867084 Reg Weight:  0.6931471805599453 Loss with Weight:  19.54158111592308 Accuracy:  1.0\n",
      "Loss:  14.550834668742176 Reg Weight:  0.6931471805599453 Loss with Weight:  14.62014938679817 Accuracy:  1.0\n",
      "Loss:  20.147788971660184 Reg Weight:  0.6931471805599453 Loss with Weight:  20.21710368971618 Accuracy:  1.0\n",
      "Loss:  18.121365899435155 Reg Weight:  0.6931471805599453 Loss with Weight:  18.19068061749115 Accuracy:  1.0\n",
      "Loss:  16.687197842083542 Reg Weight:  0.6931471805599453 Loss with Weight:  16.756512560139537 Accuracy:  1.0\n",
      "Loss:  20.062861425016024 Reg Weight:  0.6931471805599453 Loss with Weight:  20.13217614307202 Accuracy:  1.0\n",
      "Loss:  17.576172989411884 Reg Weight:  0.6931471805599453 Loss with Weight:  17.64548770746788 Accuracy:  0.9921875\n",
      "Loss:  17.74054240051663 Reg Weight:  0.6931471805599453 Loss with Weight:  17.809857118572626 Accuracy:  1.0\n",
      "Loss:  19.995252463519694 Reg Weight:  0.6931471805599453 Loss with Weight:  20.06456718157569 Accuracy:  0.984375\n",
      "Loss:  16.37568308380482 Reg Weight:  0.6931471805599453 Loss with Weight:  16.444997801860815 Accuracy:  1.0\n",
      "Loss:  18.654041498234502 Reg Weight:  0.6931471805599453 Loss with Weight:  18.723356216290497 Accuracy:  1.0\n",
      "Loss:  16.792996472041082 Reg Weight:  0.6931471805599453 Loss with Weight:  16.862311190097078 Accuracy:  0.9921875\n",
      "Loss:  15.61680299988385 Reg Weight:  0.6931471805599453 Loss with Weight:  15.686117717939844 Accuracy:  1.0\n",
      "Loss:  15.954893187658488 Reg Weight:  0.6931471805599453 Loss with Weight:  16.024207905714484 Accuracy:  1.0\n",
      "Loss:  16.87495035978806 Reg Weight:  0.6931471805599453 Loss with Weight:  16.944265077844054 Accuracy:  1.0\n",
      "Loss:  17.803773813567503 Reg Weight:  0.6931471805599453 Loss with Weight:  17.873088531623498 Accuracy:  0.9921875\n",
      "Loss:  17.24356293049873 Reg Weight:  0.6931471805599453 Loss with Weight:  17.312877648554725 Accuracy:  1.0\n",
      "Loss:  15.750494932637372 Reg Weight:  0.6931471805599453 Loss with Weight:  15.819809650693365 Accuracy:  1.0\n",
      "Loss:  18.151909629499308 Reg Weight:  0.6931471805599453 Loss with Weight:  18.221224347555303 Accuracy:  1.0\n",
      "Loss:  16.221944153644728 Reg Weight:  0.6931471805599453 Loss with Weight:  16.291258871700723 Accuracy:  1.0\n",
      "Loss:  17.11620341143966 Reg Weight:  0.6931471805599453 Loss with Weight:  17.185518129495655 Accuracy:  1.0\n",
      "Loss:  17.611584890724625 Reg Weight:  0.6931471805599453 Loss with Weight:  17.68089960878062 Accuracy:  0.9765625\n",
      "Loss:  16.273140642838843 Reg Weight:  0.6931471805599453 Loss with Weight:  16.34245536089484 Accuracy:  1.0\n",
      "Loss:  15.604862955292269 Reg Weight:  0.6931471805599453 Loss with Weight:  15.674177673348263 Accuracy:  1.0\n",
      "Epoch 0 / 100 / Loss: 22.652416223179877 / Accuracy: 0.9951862373737373\n",
      "Loss:  14.952539142696024 Reg Weight:  0.6931471805599453 Loss with Weight:  15.021853860752017 Accuracy:  1.0\n",
      "Loss:  15.565119773442444 Reg Weight:  0.6931471805599453 Loss with Weight:  15.634434491498437 Accuracy:  1.0\n",
      "Loss:  17.444298127887123 Reg Weight:  0.6931471805599453 Loss with Weight:  17.51361284594312 Accuracy:  0.9921875\n",
      "Loss:  14.970260983515544 Reg Weight:  0.6931471805599453 Loss with Weight:  15.039575701571538 Accuracy:  1.0\n",
      "Loss:  13.673140538311065 Reg Weight:  0.6931471805599453 Loss with Weight:  13.742455256367059 Accuracy:  1.0\n",
      "Loss:  15.187342963771066 Reg Weight:  0.6931471805599453 Loss with Weight:  15.25665768182706 Accuracy:  1.0\n",
      "Loss:  15.53146266630818 Reg Weight:  0.6931471805599453 Loss with Weight:  15.600777384364173 Accuracy:  1.0\n",
      "Loss:  15.46672215348075 Reg Weight:  0.6931471805599453 Loss with Weight:  15.536036871536744 Accuracy:  1.0\n",
      "Loss:  17.09312694412121 Reg Weight:  0.6931471805599453 Loss with Weight:  17.162441662177205 Accuracy:  1.0\n",
      "Loss:  14.735106968468635 Reg Weight:  0.6931471805599453 Loss with Weight:  14.804421686524629 Accuracy:  0.9921875\n",
      "Loss:  13.12330216620176 Reg Weight:  0.6931471805599453 Loss with Weight:  13.192616884257754 Accuracy:  1.0\n",
      "Loss:  15.382911023772554 Reg Weight:  0.6931471805599453 Loss with Weight:  15.452225741828547 Accuracy:  1.0\n",
      "Loss:  12.483843445281858 Reg Weight:  0.6931471805599453 Loss with Weight:  12.553158163337852 Accuracy:  1.0\n",
      "Loss:  16.25299209230091 Reg Weight:  0.6931471805599453 Loss with Weight:  16.322306810356906 Accuracy:  1.0\n",
      "Loss:  12.66744374502768 Reg Weight:  0.6931471805599453 Loss with Weight:  12.736758463083673 Accuracy:  0.9921875\n",
      "Loss:  14.136513709228472 Reg Weight:  0.6931471805599453 Loss with Weight:  14.205828427284466 Accuracy:  1.0\n",
      "Loss:  16.16898089285639 Reg Weight:  0.6931471805599453 Loss with Weight:  16.238295610912385 Accuracy:  1.0\n",
      "Loss:  14.041572905475055 Reg Weight:  0.6931471805599453 Loss with Weight:  14.110887623531049 Accuracy:  1.0\n",
      "Loss:  13.841063965814463 Reg Weight:  0.6931471805599453 Loss with Weight:  13.910378683870457 Accuracy:  0.9921875\n",
      "Loss:  14.302066575978655 Reg Weight:  0.6931471805599453 Loss with Weight:  14.371381294034649 Accuracy:  1.0\n",
      "Loss:  11.167024551133522 Reg Weight:  0.6931471805599453 Loss with Weight:  11.236339269189516 Accuracy:  1.0\n",
      "Loss:  15.803723937871567 Reg Weight:  0.6931471805599453 Loss with Weight:  15.87303865592756 Accuracy:  0.9921875\n",
      "Loss:  13.247746511179923 Reg Weight:  0.6931471805599453 Loss with Weight:  13.317061229235916 Accuracy:  1.0\n",
      "Loss:  13.389675727565816 Reg Weight:  0.6931471805599453 Loss with Weight:  13.45899044562181 Accuracy:  1.0\n",
      "Loss:  12.542260730117732 Reg Weight:  0.6931471805599453 Loss with Weight:  12.611575448173726 Accuracy:  1.0\n",
      "Loss:  13.57796367850682 Reg Weight:  0.6931471805599453 Loss with Weight:  13.647278396562813 Accuracy:  0.9921875\n",
      "Loss:  13.642150300057022 Reg Weight:  0.6931471805599453 Loss with Weight:  13.711465018113016 Accuracy:  0.9921875\n",
      "Loss:  11.26938354470897 Reg Weight:  0.6931471805599453 Loss with Weight:  11.338698262764964 Accuracy:  1.0\n",
      "Loss:  11.504511527114513 Reg Weight:  0.6931471805599453 Loss with Weight:  11.573826245170507 Accuracy:  0.9921875\n",
      "Loss:  11.484774215308258 Reg Weight:  0.6931471805599453 Loss with Weight:  11.554088933364252 Accuracy:  1.0\n",
      "Loss:  11.395461799394663 Reg Weight:  0.6931471805599453 Loss with Weight:  11.464776517450657 Accuracy:  1.0\n",
      "Loss:  11.099633912550296 Reg Weight:  0.6931471805599453 Loss with Weight:  11.16894863060629 Accuracy:  1.0\n",
      "Loss:  12.976796338406734 Reg Weight:  0.6931471805599453 Loss with Weight:  13.046111056462728 Accuracy:  1.0\n",
      "Loss:  14.92159762439183 Reg Weight:  0.6931471805599453 Loss with Weight:  14.990912342447823 Accuracy:  0.9921875\n",
      "Loss:  11.649586853419937 Reg Weight:  0.6931471805599453 Loss with Weight:  11.71890157147593 Accuracy:  1.0\n",
      "Loss:  17.09961008939328 Reg Weight:  0.6931471805599453 Loss with Weight:  17.168924807449276 Accuracy:  0.984375\n",
      "Loss:  13.668754040342606 Reg Weight:  0.6931471805599453 Loss with Weight:  13.7380687583986 Accuracy:  0.9921875\n",
      "Loss:  12.001559964308868 Reg Weight:  0.6931471805599453 Loss with Weight:  12.070874682364861 Accuracy:  1.0\n",
      "Loss:  12.03825331240209 Reg Weight:  0.6931471805599453 Loss with Weight:  12.107568030458083 Accuracy:  1.0\n",
      "Loss:  9.335449275948367 Reg Weight:  0.6931471805599453 Loss with Weight:  9.40476399400436 Accuracy:  1.0\n",
      "Loss:  8.755033704709142 Reg Weight:  0.6931471805599453 Loss with Weight:  8.824348422765135 Accuracy:  1.0\n",
      "Loss:  11.26636513512406 Reg Weight:  0.6931471805599453 Loss with Weight:  11.335679853180054 Accuracy:  0.9921875\n",
      "Loss:  11.260867774428995 Reg Weight:  0.6931471805599453 Loss with Weight:  11.330182492484989 Accuracy:  1.0\n",
      "Loss:  12.447222503343294 Reg Weight:  0.6931471805599453 Loss with Weight:  12.516537221399288 Accuracy:  0.984375\n",
      "Loss:  9.684429815769175 Reg Weight:  0.6931471805599453 Loss with Weight:  9.753744533825168 Accuracy:  1.0\n",
      "Loss:  11.073801935357892 Reg Weight:  0.6931471805599453 Loss with Weight:  11.143116653413886 Accuracy:  1.0\n",
      "Loss:  12.428976016617419 Reg Weight:  0.6931471805599453 Loss with Weight:  12.498290734673413 Accuracy:  0.9921875\n",
      "Loss:  12.854111144602737 Reg Weight:  0.6931471805599453 Loss with Weight:  12.92342586265873 Accuracy:  0.9921875\n",
      "Loss:  10.73076276805852 Reg Weight:  0.6931471805599453 Loss with Weight:  10.800077486114514 Accuracy:  1.0\n",
      "Loss:  12.863244793402483 Reg Weight:  0.6931471805599453 Loss with Weight:  12.932559511458477 Accuracy:  0.9921875\n",
      "Loss:  10.127341619091624 Reg Weight:  0.6931471805599453 Loss with Weight:  10.196656337147617 Accuracy:  0.9921875\n",
      "Loss:  9.608639501821468 Reg Weight:  0.6931471805599453 Loss with Weight:  9.677954219877462 Accuracy:  1.0\n",
      "Loss:  9.338511090801603 Reg Weight:  0.6931471805599453 Loss with Weight:  9.407825808857597 Accuracy:  0.9921875\n",
      "Loss:  6.852347293612842 Reg Weight:  0.6931471805599453 Loss with Weight:  6.921662011668836 Accuracy:  1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m reg_weight \u001b[38;5;241m=\u001b[39m tpcp_mps\u001b[38;5;241m.\u001b[39mregularize_weight(tpcp\u001b[38;5;241m.\u001b[39mW)\n\u001b[1;32m     17\u001b[0m loss_with_weight \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m+\u001b[39m reg_weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss_with_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m optimizer_weight\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mps.StiefelOptimizers import StiefelAdam\n",
    "from mps.radam import RiemannianAdam\n",
    "optimizer = RiemannianAdam(tpcp.kraus_ops.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "optimizer_weight = torch.optim.Adam([tpcp.W, tpcp.r], lr=0.0001)\n",
    "# optimzier = StiefelAdam(tpcp.parameters(), lr=0.0001, expm_method=\"ForwardEuler\")\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    acc_tot = 0\n",
    "    loss_tot = 0\n",
    "    for data, target in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_weight.zero_grad()\n",
    "        outputs = tpcp(data)\n",
    "        outputs = utils.to_probs(outputs)[:, 0]\n",
    "        loss = loss_batch(outputs, target)\n",
    "        reg_weight = tpcp_mps.regularize_weight(tpcp.W)\n",
    "        loss_with_weight = loss + reg_weight * 0.1\n",
    "        loss_with_weight.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_weight.step()\n",
    "        tpcp.normalize_w_and_r()\n",
    "        acc = calculate_accuracy(outputs, target)\n",
    "        acc_tot += acc\n",
    "        loss_tot += loss.item()\n",
    "        print(\"Loss: \", loss.item(), \"Reg Weight: \", reg_weight.item(), \"Loss with Weight: \", loss_with_weight.item(), \"Accuracy: \", acc.item())\n",
    "\n",
    "    print(f\"Epoch {epoch} / {epochs} / Loss: {loss_tot / len(trainloader)} / Accuracy: {acc_tot.item() / len(trainloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Number of qubits must match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43msmps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m out \u001b[38;5;241m=\u001b[39m logsoftmax(out)\n\u001b[1;32m      3\u001b[0m out\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/notebooks/../../mps/simple_mps.py:252\u001b[0m, in \u001b[0;36mSimpleMPS.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    251\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of qubits must match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN):\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mops[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_qubit_inds[i]] \u001b[38;5;241m=\u001b[39m X[i]\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of qubits must match"
     ]
    }
   ],
   "source": [
    "out = smps(data)\n",
    "out = logsoftmax(out)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 1.], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tpcp(data.permute(1, 0, 2))\n",
    "\n",
    "# loss_batch(out, target)\n",
    "(torch.sgn(out) + 1) / 2 + target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
