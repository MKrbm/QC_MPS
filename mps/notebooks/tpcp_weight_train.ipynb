{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opt_einsum as oe\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mps.simple_mps' from '/Users/keisuke/Documents/presentation/QC_MPS/mps/notebooks/../../mps/simple_mps.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from mps import simple_mps, tpcp_mps\n",
    "reload(simple_mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_digits(dataset, allowed_digits=[0, 1]):\n",
    "    \"\"\"Return a subset of MNIST dataset containing only allowed_digits (0 or 1).\"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if label in allowed_digits:\n",
    "            indices.append(i)\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "\n",
    "def filiter_single_channel(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    MNIST is loaded as shape [C, H, W].\n",
    "    Take only the first channel => shape [H, W].\n",
    "    \"\"\"\n",
    "    return img[0, ...]\n",
    "\n",
    "\n",
    "def embedding_pixel(batch, label: int = 0):\n",
    "    \"\"\"\n",
    "    Flatten each image from shape [H, W] => [H*W],\n",
    "    then embed x => [x, 1-x], and L2-normalize along last dim.\n",
    "    \"\"\"\n",
    "    pixel_size = batch.shape[-1] * batch.shape[-2]\n",
    "    x = batch.view(*batch.shape[:-2], pixel_size)\n",
    "    x = torch.stack([x, 1 - x], dim=-1)\n",
    "    x = x / torch.sum(x, dim=-1).unsqueeze(-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Loss & Accuracy\n",
    "###############################################################################\n",
    "def loss_batch(outputs, labels):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy style loss for outputs in [0, 1].\n",
    "    For label=0 => prob=outputs[i], else => 1 - outputs[i].\n",
    "    \"\"\"\n",
    "    device = outputs.device\n",
    "    loss = torch.zeros(1, device=device, dtype=torch.float64)\n",
    "    for i in range(len(outputs)):\n",
    "        prob = outputs[i] if labels[i] == 0 else (1 - outputs[i])\n",
    "        loss -= torch.log(prob + 1e-8)\n",
    "        # Start of Selection\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Loss is NaN at i={i}\")\n",
    "            print(prob, outputs[i], labels[i])\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Threshold 0.5 => label 0 or 1. Compare to true labels.\n",
    "    \"\"\"\n",
    "    predictions = (outputs < 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    return correct / labels.numel()\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "img_size = 16\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(filiter_single_channel),\n",
    "        transforms.Lambda(embedding_pixel),\n",
    "        transforms.Lambda(lambda x: x.to(torch.float64)),  # double precision\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "# Filter digits 0,1 only\n",
    "trainset = filter_digits(trainset, allowed_digits=[0, 1])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS with random matrices\n"
     ]
    }
   ],
   "source": [
    "# ---------- Build MPS model ----------\n",
    "N = img_size * img_size\n",
    "d = l = 2 #data input dimension and class label dimension \n",
    "chi_umps = 2\n",
    "chi_max = 2\n",
    "reload(simple_mps)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "smps = simple_mps.SimpleMPS(\n",
    "    N, \n",
    "    2,\n",
    "    d, \n",
    "    l, \n",
    "    layers=2,\n",
    "    device=device, \n",
    "    dtype=torch.float64, \n",
    "    optimize=\"greedy\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/12665 (0%)]\tLoss: 0.693380 Accuracy: 42.97%\n",
      "Train Epoch: 0 [128/12665 (1%)]\tLoss: 0.693088 Accuracy: 47.66%\n",
      "Train Epoch: 0 [256/12665 (2%)]\tLoss: 0.692907 Accuracy: 53.91%\n",
      "Train Epoch: 0 [384/12665 (3%)]\tLoss: 0.692651 Accuracy: 53.12%\n",
      "Train Epoch: 0 [512/12665 (4%)]\tLoss: 0.692105 Accuracy: 53.91%\n",
      "Train Epoch: 0 [640/12665 (5%)]\tLoss: 0.691262 Accuracy: 51.56%\n",
      "Train Epoch: 0 [768/12665 (6%)]\tLoss: 0.688629 Accuracy: 55.47%\n",
      "Train Epoch: 0 [896/12665 (7%)]\tLoss: 0.686424 Accuracy: 52.34%\n",
      "Train Epoch: 0 [1024/12665 (8%)]\tLoss: 0.683018 Accuracy: 49.22%\n",
      "Train Epoch: 0 [1152/12665 (9%)]\tLoss: 0.671369 Accuracy: 53.12%\n",
      "Train Epoch: 0 [1280/12665 (10%)]\tLoss: 0.652862 Accuracy: 54.69%\n",
      "Train Epoch: 0 [1408/12665 (11%)]\tLoss: 0.643309 Accuracy: 50.78%\n",
      "Train Epoch: 0 [1536/12665 (12%)]\tLoss: 0.600847 Accuracy: 57.03%\n",
      "Train Epoch: 0 [1664/12665 (13%)]\tLoss: 0.607812 Accuracy: 45.31%\n",
      "Train Epoch: 0 [1792/12665 (14%)]\tLoss: 0.448316 Accuracy: 62.50%\n",
      "Train Epoch: 0 [1920/12665 (15%)]\tLoss: 0.556803 Accuracy: 52.34%\n",
      "Train Epoch: 0 [2048/12665 (16%)]\tLoss: 0.619577 Accuracy: 45.31%\n",
      "Train Epoch: 0 [2176/12665 (17%)]\tLoss: 0.493634 Accuracy: 53.12%\n",
      "Train Epoch: 0 [2304/12665 (18%)]\tLoss: 0.467783 Accuracy: 55.47%\n",
      "Train Epoch: 0 [2432/12665 (19%)]\tLoss: 0.525273 Accuracy: 46.88%\n",
      "Train Epoch: 0 [2560/12665 (20%)]\tLoss: 0.464525 Accuracy: 56.25%\n",
      "Train Epoch: 0 [2688/12665 (21%)]\tLoss: 0.498498 Accuracy: 49.22%\n",
      "Train Epoch: 0 [2816/12665 (22%)]\tLoss: 0.447078 Accuracy: 54.69%\n",
      "Train Epoch: 0 [2944/12665 (23%)]\tLoss: 0.464165 Accuracy: 53.12%\n",
      "Train Epoch: 0 [3072/12665 (24%)]\tLoss: 0.399482 Accuracy: 58.59%\n",
      "Train Epoch: 0 [3200/12665 (25%)]\tLoss: 0.438300 Accuracy: 52.34%\n",
      "Train Epoch: 0 [3328/12665 (26%)]\tLoss: 0.430640 Accuracy: 52.34%\n",
      "Train Epoch: 0 [3456/12665 (27%)]\tLoss: 0.367207 Accuracy: 57.81%\n",
      "Train Epoch: 0 [3584/12665 (28%)]\tLoss: 0.367783 Accuracy: 58.59%\n",
      "Train Epoch: 0 [3712/12665 (29%)]\tLoss: 0.408274 Accuracy: 57.81%\n",
      "Train Epoch: 0 [3840/12665 (30%)]\tLoss: 0.357102 Accuracy: 59.38%\n",
      "Train Epoch: 0 [3968/12665 (31%)]\tLoss: 0.365995 Accuracy: 61.72%\n",
      "Train Epoch: 0 [4096/12665 (32%)]\tLoss: 0.375240 Accuracy: 72.66%\n",
      "Train Epoch: 0 [4224/12665 (33%)]\tLoss: 0.347924 Accuracy: 86.72%\n",
      "Train Epoch: 0 [4352/12665 (34%)]\tLoss: 0.367512 Accuracy: 79.69%\n",
      "Train Epoch: 0 [4480/12665 (35%)]\tLoss: 0.321474 Accuracy: 92.19%\n",
      "Train Epoch: 0 [4608/12665 (36%)]\tLoss: 0.343170 Accuracy: 81.25%\n",
      "Train Epoch: 0 [4736/12665 (37%)]\tLoss: 0.378336 Accuracy: 75.00%\n",
      "Train Epoch: 0 [4864/12665 (38%)]\tLoss: 0.317848 Accuracy: 86.72%\n",
      "Train Epoch: 0 [4992/12665 (39%)]\tLoss: 0.283388 Accuracy: 97.66%\n",
      "Train Epoch: 0 [5120/12665 (40%)]\tLoss: 0.264976 Accuracy: 96.88%\n",
      "Train Epoch: 0 [5248/12665 (41%)]\tLoss: 0.287292 Accuracy: 93.75%\n",
      "Train Epoch: 0 [5376/12665 (42%)]\tLoss: 0.264116 Accuracy: 96.09%\n",
      "Train Epoch: 0 [5504/12665 (43%)]\tLoss: 0.299542 Accuracy: 96.09%\n",
      "Train Epoch: 0 [5632/12665 (44%)]\tLoss: 0.211937 Accuracy: 99.22%\n",
      "Train Epoch: 0 [5760/12665 (45%)]\tLoss: 0.222654 Accuracy: 100.00%\n",
      "Train Epoch: 0 [5888/12665 (46%)]\tLoss: 0.185217 Accuracy: 99.22%\n",
      "Train Epoch: 0 [6016/12665 (47%)]\tLoss: 0.162523 Accuracy: 99.22%\n",
      "Train Epoch: 0 [6144/12665 (48%)]\tLoss: 0.098302 Accuracy: 98.44%\n",
      "Train Epoch: 0 [6272/12665 (49%)]\tLoss: 0.039280 Accuracy: 100.00%\n",
      "Train Epoch: 0 [6400/12665 (51%)]\tLoss: 0.047489 Accuracy: 98.44%\n",
      "Train Epoch: 0 [6528/12665 (52%)]\tLoss: 0.111772 Accuracy: 97.66%\n",
      "Train Epoch: 0 [6656/12665 (53%)]\tLoss: 0.041544 Accuracy: 98.44%\n",
      "Train Epoch: 0 [6784/12665 (54%)]\tLoss: 0.042638 Accuracy: 99.22%\n",
      "Train Epoch: 0 [6912/12665 (55%)]\tLoss: 0.030871 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7040/12665 (56%)]\tLoss: 0.012337 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7168/12665 (57%)]\tLoss: 0.012033 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7296/12665 (58%)]\tLoss: 0.001145 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7424/12665 (59%)]\tLoss: 0.000006 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7552/12665 (60%)]\tLoss: 0.021539 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7680/12665 (61%)]\tLoss: 0.000002 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7808/12665 (62%)]\tLoss: 0.000003 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7936/12665 (63%)]\tLoss: 0.173194 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8064/12665 (64%)]\tLoss: 0.166657 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8192/12665 (65%)]\tLoss: 0.000001 Accuracy: 100.00%\n",
      "Train Epoch: 0 [8320/12665 (66%)]\tLoss: 0.073696 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8448/12665 (67%)]\tLoss: 0.090236 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8576/12665 (68%)]\tLoss: 0.009253 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8704/12665 (69%)]\tLoss: 0.007290 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8832/12665 (70%)]\tLoss: 0.003033 Accuracy: 100.00%\n",
      "Train Epoch: 0 [8960/12665 (71%)]\tLoss: 0.000552 Accuracy: 100.00%\n",
      "Train Epoch: 0 [9088/12665 (72%)]\tLoss: 0.013449 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9216/12665 (73%)]\tLoss: 0.081734 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9344/12665 (74%)]\tLoss: 0.229856 Accuracy: 89.06%\n",
      "Train Epoch: 0 [9472/12665 (75%)]\tLoss: 0.024697 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9600/12665 (76%)]\tLoss: 0.011546 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9728/12665 (77%)]\tLoss: 0.017467 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9856/12665 (78%)]\tLoss: 0.006282 Accuracy: 100.00%\n",
      "Train Epoch: 0 [9984/12665 (79%)]\tLoss: 0.010869 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10112/12665 (80%)]\tLoss: 0.007901 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10240/12665 (81%)]\tLoss: 0.011657 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10368/12665 (82%)]\tLoss: 0.017381 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10496/12665 (83%)]\tLoss: 0.016126 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10624/12665 (84%)]\tLoss: 0.041070 Accuracy: 98.44%\n",
      "Train Epoch: 0 [10752/12665 (85%)]\tLoss: 0.011674 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10880/12665 (86%)]\tLoss: 0.009482 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11008/12665 (87%)]\tLoss: 0.016412 Accuracy: 99.22%\n",
      "Train Epoch: 0 [11136/12665 (88%)]\tLoss: 0.007578 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11264/12665 (89%)]\tLoss: 0.002160 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11392/12665 (90%)]\tLoss: 0.006941 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11520/12665 (91%)]\tLoss: 0.010545 Accuracy: 99.22%\n",
      "Train Epoch: 0 [11648/12665 (92%)]\tLoss: 0.000797 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11776/12665 (93%)]\tLoss: 0.000316 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11904/12665 (94%)]\tLoss: 0.001165 Accuracy: 100.00%\n",
      "Train Epoch: 0 [12032/12665 (95%)]\tLoss: 0.001121 Accuracy: 100.00%\n",
      "Train Epoch: 0 [12160/12665 (96%)]\tLoss: 0.001547 Accuracy: 100.00%\n",
      "Train Epoch: 0 [12288/12665 (97%)]\tLoss: 0.095685 Accuracy: 98.44%\n",
      "Train Epoch: 0 [12416/12665 (98%)]\tLoss: 0.000020 Accuracy: 100.00%\n",
      "Train Epoch: 0 [11858/12665 (99%)]\tLoss: 0.000086 Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def accuracy(outputs, target):\n",
    "    return (outputs.argmax(dim=-1) == target).float().mean()\n",
    "losses = []\n",
    "running_loss = 0\n",
    "running_accuracy = 0\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "nnloss = torch.nn.NLLLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(smps.parameters(), lr=0.001)\n",
    "n_samples = 0\n",
    "for epoch in range(1):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        target = target.to(device).to(torch.int64)\n",
    "        data = data.to(device).permute(1, 0, 2)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = smps(data)\n",
    "        outputs = logsoftmax(outputs)\n",
    "        loss = nnloss(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data_size = data.shape[1]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        # print(torch.exp(outputs[:10]), target[:10])\n",
    "        \n",
    "        running_loss += loss.item() * data_size\n",
    "        n_samples += data_size\n",
    "        \n",
    "        if batch_idx % 1 == 0:\n",
    "            avg_loss = running_loss / n_samples\n",
    "            avg_accuracy = accuracy(outputs, target)\n",
    "            losses.append(avg_loss)\n",
    "            running_loss = 0\n",
    "            running_accuracy = 0\n",
    "            n_samples = 0\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Accuracy: {:.2f}%'.format(\n",
    "                epoch, batch_idx * data_size, len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), avg_loss, avg_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mps import tpcp_mps  \n",
    "from mps.trainer import utils\n",
    "\n",
    "reload(tpcp_mps)\n",
    "\n",
    "tpcp = tpcp_mps.MPSTPCP(N, K=1, d=2, with_probs=False, with_identity=True, manifold=tpcp_mps.ManifoldType.EXACT)\n",
    "\n",
    "W = torch.zeros(tpcp.L, 2, dtype=torch.float64)\n",
    "W[:, 0] = 1\n",
    "W[:, 1] = 0\n",
    "tpcp.initialize_W(W, set_r=True)\n",
    "\n",
    "tpcp.set_canonical_mps(smps)\n",
    "\n",
    "\n",
    "def accuracy(outputs, target):\n",
    "    correct = (outputs < 0).float() == target.float()\n",
    "    return correct.float().sum() / target.numel()\n",
    "\n",
    "data, target = next(iter(trainloader))\n",
    "out = tpcp(data)\n",
    "# out = utils.to_probs(out)\n",
    "\n",
    "# calculate_accuracy(out, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "        1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "        0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1156e-02, 9.6884e-01],\n",
       "        [6.0471e-04, 9.9940e-01],\n",
       "        [2.8713e-04, 9.9971e-01],\n",
       "        [7.0440e-04, 9.9930e-01],\n",
       "        [7.1283e-04, 9.9929e-01],\n",
       "        [4.4731e-02, 9.5527e-01],\n",
       "        [6.2026e-04, 9.9938e-01],\n",
       "        [3.8995e-03, 9.9610e-01],\n",
       "        [4.2654e-02, 9.5735e-01],\n",
       "        [2.6984e-02, 9.7302e-01],\n",
       "        [5.7392e-04, 9.9943e-01],\n",
       "        [8.6022e-02, 9.1398e-01],\n",
       "        [7.8337e-02, 9.2166e-01],\n",
       "        [6.1946e-04, 9.9938e-01],\n",
       "        [7.3164e-02, 9.2684e-01],\n",
       "        [6.2860e-04, 9.9937e-01],\n",
       "        [2.1797e-01, 7.8203e-01],\n",
       "        [5.4114e-02, 9.4589e-01],\n",
       "        [1.1017e-03, 9.9890e-01],\n",
       "        [8.1063e-04, 9.9919e-01],\n",
       "        [4.4000e-02, 9.5600e-01],\n",
       "        [6.3333e-04, 9.9937e-01],\n",
       "        [5.8965e-04, 9.9941e-01],\n",
       "        [3.8646e-02, 9.6135e-01],\n",
       "        [5.4203e-02, 9.4580e-01],\n",
       "        [3.3859e-02, 9.6614e-01],\n",
       "        [7.6338e-04, 9.9924e-01],\n",
       "        [8.0460e-04, 9.9920e-01],\n",
       "        [7.5971e-04, 9.9924e-01],\n",
       "        [6.1745e-04, 9.9938e-01],\n",
       "        [9.3156e-02, 9.0684e-01],\n",
       "        [6.8200e-04, 9.9932e-01],\n",
       "        [4.3689e-04, 9.9956e-01],\n",
       "        [5.2385e-02, 9.4762e-01],\n",
       "        [3.9159e-02, 9.6084e-01],\n",
       "        [4.8836e-02, 9.5116e-01],\n",
       "        [9.2039e-02, 9.0796e-01],\n",
       "        [8.7986e-04, 9.9912e-01],\n",
       "        [3.4219e-04, 9.9966e-01],\n",
       "        [1.0126e-03, 9.9899e-01],\n",
       "        [6.8010e-04, 9.9932e-01],\n",
       "        [4.2316e-02, 9.5768e-01],\n",
       "        [3.8427e-02, 9.6157e-01],\n",
       "        [7.0943e-04, 9.9929e-01],\n",
       "        [4.7940e-04, 9.9952e-01],\n",
       "        [5.9805e-04, 9.9940e-01],\n",
       "        [2.3932e-02, 9.7607e-01],\n",
       "        [5.5349e-04, 9.9945e-01],\n",
       "        [6.4392e-04, 9.9936e-01],\n",
       "        [5.9754e-04, 9.9940e-01],\n",
       "        [1.0637e-01, 8.9363e-01],\n",
       "        [5.2421e-04, 9.9948e-01],\n",
       "        [8.7969e-02, 9.1203e-01],\n",
       "        [8.4900e-02, 9.1510e-01],\n",
       "        [5.0768e-04, 9.9949e-01],\n",
       "        [3.8045e-02, 9.6195e-01],\n",
       "        [7.8245e-03, 9.9218e-01],\n",
       "        [1.1887e-01, 8.8113e-01],\n",
       "        [5.9897e-04, 9.9940e-01],\n",
       "        [7.8220e-02, 9.2178e-01],\n",
       "        [3.4120e-02, 9.6588e-01],\n",
       "        [5.2050e-01, 4.7950e-01],\n",
       "        [1.8605e-04, 9.9981e-01],\n",
       "        [1.0152e-01, 8.9848e-01],\n",
       "        [4.8824e-04, 9.9951e-01],\n",
       "        [1.2136e-01, 8.7864e-01],\n",
       "        [6.5735e-04, 9.9934e-01],\n",
       "        [1.1119e-03, 9.9889e-01],\n",
       "        [8.2353e-04, 9.9918e-01],\n",
       "        [7.6981e-02, 9.2302e-01],\n",
       "        [6.5301e-04, 9.9935e-01],\n",
       "        [5.8094e-02, 9.4191e-01],\n",
       "        [2.6874e-02, 9.7313e-01],\n",
       "        [5.3435e-02, 9.4656e-01],\n",
       "        [6.8642e-04, 9.9931e-01],\n",
       "        [6.4771e-04, 9.9935e-01],\n",
       "        [5.7404e-04, 9.9943e-01],\n",
       "        [3.9806e-02, 9.6019e-01],\n",
       "        [2.5469e-02, 9.7453e-01],\n",
       "        [5.8605e-04, 9.9941e-01],\n",
       "        [6.0211e-04, 9.9940e-01],\n",
       "        [4.9324e-04, 9.9951e-01],\n",
       "        [5.3414e-04, 9.9947e-01],\n",
       "        [6.9671e-04, 9.9930e-01],\n",
       "        [5.2211e-02, 9.4779e-01],\n",
       "        [7.9306e-04, 9.9921e-01],\n",
       "        [5.7985e-04, 9.9942e-01],\n",
       "        [8.1382e-04, 9.9919e-01],\n",
       "        [5.6913e-04, 9.9943e-01],\n",
       "        [7.3782e-04, 9.9926e-01],\n",
       "        [4.8045e-02, 9.5196e-01],\n",
       "        [4.7472e-04, 9.9953e-01],\n",
       "        [6.9064e-04, 9.9931e-01],\n",
       "        [5.3938e-04, 9.9946e-01],\n",
       "        [5.2535e-02, 9.4746e-01],\n",
       "        [8.0758e-02, 9.1924e-01],\n",
       "        [4.6399e-02, 9.5360e-01],\n",
       "        [1.3125e-01, 8.6875e-01],\n",
       "        [6.2233e-04, 9.9938e-01],\n",
       "        [6.1383e-04, 9.9939e-01],\n",
       "        [6.5645e-04, 9.9934e-01],\n",
       "        [3.0651e-02, 9.6935e-01],\n",
       "        [3.6086e-02, 9.6391e-01],\n",
       "        [7.9192e-04, 9.9921e-01],\n",
       "        [5.6438e-04, 9.9944e-01],\n",
       "        [7.7663e-02, 9.2234e-01],\n",
       "        [6.8318e-02, 9.3168e-01],\n",
       "        [9.7774e-02, 9.0223e-01],\n",
       "        [8.8098e-04, 9.9912e-01],\n",
       "        [1.8361e-03, 9.9816e-01],\n",
       "        [5.6338e-02, 9.4366e-01],\n",
       "        [6.6446e-04, 9.9934e-01],\n",
       "        [8.1265e-04, 9.9919e-01],\n",
       "        [2.1824e-02, 9.7818e-01],\n",
       "        [8.6592e-04, 9.9913e-01],\n",
       "        [6.4512e-04, 9.9935e-01],\n",
       "        [6.1440e-04, 9.9939e-01],\n",
       "        [6.7497e-04, 9.9933e-01],\n",
       "        [8.8240e-04, 9.9912e-01],\n",
       "        [4.2000e-02, 9.5800e-01],\n",
       "        [6.2238e-02, 9.3776e-01],\n",
       "        [1.1804e-01, 8.8196e-01],\n",
       "        [1.9884e-02, 9.8012e-01],\n",
       "        [9.0917e-04, 9.9909e-01],\n",
       "        [7.8069e-04, 9.9922e-01],\n",
       "        [8.0025e-04, 9.9920e-01],\n",
       "        [3.4430e-02, 9.6557e-01],\n",
       "        [6.8953e-04, 9.9931e-01]], dtype=torch.float64,\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.2443e-02, 9.6756e-01],\n",
       "        [6.2676e-04, 9.9937e-01],\n",
       "        [3.2211e-04, 9.9968e-01],\n",
       "        [6.9675e-04, 9.9930e-01],\n",
       "        [7.0968e-04, 9.9929e-01],\n",
       "        [4.7081e-02, 9.5292e-01],\n",
       "        [6.4005e-04, 9.9936e-01],\n",
       "        [2.9855e-03, 9.9701e-01],\n",
       "        [5.1718e-02, 9.4828e-01],\n",
       "        [2.7667e-02, 9.7233e-01],\n",
       "        [5.9253e-04, 9.9941e-01],\n",
       "        [7.7886e-02, 9.2211e-01],\n",
       "        [7.2428e-02, 9.2757e-01],\n",
       "        [6.3998e-04, 9.9936e-01],\n",
       "        [7.0679e-02, 9.2932e-01],\n",
       "        [6.4622e-04, 9.9935e-01],\n",
       "        [4.0094e-01, 5.9906e-01],\n",
       "        [5.1513e-02, 9.4849e-01],\n",
       "        [1.0216e-03, 9.9898e-01],\n",
       "        [7.9721e-04, 9.9920e-01],\n",
       "        [4.4820e-02, 9.5518e-01],\n",
       "        [6.4958e-04, 9.9935e-01],\n",
       "        [6.0966e-04, 9.9939e-01],\n",
       "        [3.7886e-02, 9.6211e-01],\n",
       "        [5.3592e-02, 9.4641e-01],\n",
       "        [3.2767e-02, 9.6723e-01],\n",
       "        [7.5833e-04, 9.9924e-01],\n",
       "        [7.8824e-04, 9.9921e-01],\n",
       "        [7.3497e-04, 9.9927e-01],\n",
       "        [6.3708e-04, 9.9936e-01],\n",
       "        [1.1964e-01, 8.8036e-01],\n",
       "        [6.8423e-04, 9.9932e-01],\n",
       "        [4.7597e-04, 9.9952e-01],\n",
       "        [5.0659e-02, 9.4934e-01],\n",
       "        [4.5229e-02, 9.5477e-01],\n",
       "        [4.7474e-02, 9.5253e-01],\n",
       "        [8.6884e-02, 9.1312e-01],\n",
       "        [8.5595e-04, 9.9914e-01],\n",
       "        [3.7976e-04, 9.9962e-01],\n",
       "        [9.0367e-04, 9.9910e-01],\n",
       "        [6.8761e-04, 9.9931e-01],\n",
       "        [4.7840e-02, 9.5216e-01],\n",
       "        [3.8468e-02, 9.6153e-01],\n",
       "        [7.1036e-04, 9.9929e-01],\n",
       "        [5.1287e-04, 9.9949e-01],\n",
       "        [6.0757e-04, 9.9939e-01],\n",
       "        [2.4267e-02, 9.7573e-01],\n",
       "        [5.6230e-04, 9.9944e-01],\n",
       "        [6.6118e-04, 9.9934e-01],\n",
       "        [6.1851e-04, 9.9938e-01],\n",
       "        [1.0403e-01, 8.9597e-01],\n",
       "        [5.5028e-04, 9.9945e-01],\n",
       "        [8.6177e-02, 9.1382e-01],\n",
       "        [1.6529e-01, 8.3471e-01],\n",
       "        [5.3787e-04, 9.9946e-01],\n",
       "        [4.0892e-02, 9.5911e-01],\n",
       "        [5.6140e-03, 9.9439e-01],\n",
       "        [1.1460e-01, 8.8540e-01],\n",
       "        [6.1955e-04, 9.9938e-01],\n",
       "        [2.3758e-01, 7.6242e-01],\n",
       "        [3.8047e-02, 9.6195e-01],\n",
       "        [9.6905e-01, 3.0948e-02],\n",
       "        [2.2497e-04, 9.9978e-01],\n",
       "        [1.0507e-01, 8.9493e-01],\n",
       "        [5.1924e-04, 9.9948e-01],\n",
       "        [4.8950e-01, 5.1050e-01],\n",
       "        [6.7123e-04, 9.9933e-01],\n",
       "        [1.0438e-03, 9.9896e-01],\n",
       "        [8.0814e-04, 9.9919e-01],\n",
       "        [7.3210e-02, 9.2679e-01],\n",
       "        [6.6632e-04, 9.9933e-01],\n",
       "        [5.4722e-02, 9.4528e-01],\n",
       "        [2.7903e-02, 9.7210e-01],\n",
       "        [5.0707e-02, 9.4929e-01],\n",
       "        [6.9605e-04, 9.9930e-01],\n",
       "        [6.5390e-04, 9.9935e-01],\n",
       "        [5.9926e-04, 9.9940e-01],\n",
       "        [4.0378e-02, 9.5962e-01],\n",
       "        [2.6341e-02, 9.7366e-01],\n",
       "        [6.0901e-04, 9.9939e-01],\n",
       "        [6.2474e-04, 9.9938e-01],\n",
       "        [5.2803e-04, 9.9947e-01],\n",
       "        [5.6245e-04, 9.9944e-01],\n",
       "        [6.8980e-04, 9.9931e-01],\n",
       "        [5.1179e-02, 9.4882e-01],\n",
       "        [7.8202e-04, 9.9922e-01],\n",
       "        [5.9547e-04, 9.9940e-01],\n",
       "        [7.7037e-04, 9.9923e-01],\n",
       "        [5.9417e-04, 9.9941e-01],\n",
       "        [7.2672e-04, 9.9927e-01],\n",
       "        [4.7063e-02, 9.5294e-01],\n",
       "        [4.8583e-04, 9.9951e-01],\n",
       "        [6.9370e-04, 9.9931e-01],\n",
       "        [5.6397e-04, 9.9944e-01],\n",
       "        [5.0653e-02, 9.4935e-01],\n",
       "        [7.6011e-02, 9.2399e-01],\n",
       "        [4.6450e-02, 9.5355e-01],\n",
       "        [1.2797e-01, 8.7203e-01],\n",
       "        [6.4008e-04, 9.9936e-01],\n",
       "        [6.3379e-04, 9.9937e-01],\n",
       "        [6.6487e-04, 9.9934e-01],\n",
       "        [3.1415e-02, 9.6859e-01],\n",
       "        [3.8123e-02, 9.6188e-01],\n",
       "        [7.7689e-04, 9.9922e-01],\n",
       "        [5.8923e-04, 9.9941e-01],\n",
       "        [7.1328e-02, 9.2867e-01],\n",
       "        [6.6173e-02, 9.3383e-01],\n",
       "        [9.7229e-02, 9.0277e-01],\n",
       "        [8.5913e-04, 9.9914e-01],\n",
       "        [1.5280e-03, 9.9847e-01],\n",
       "        [6.0385e-02, 9.3961e-01],\n",
       "        [6.7518e-04, 9.9932e-01],\n",
       "        [7.8939e-04, 9.9921e-01],\n",
       "        [2.7116e-02, 9.7288e-01],\n",
       "        [8.4935e-04, 9.9915e-01],\n",
       "        [6.6159e-04, 9.9934e-01],\n",
       "        [6.3202e-04, 9.9937e-01],\n",
       "        [6.8647e-04, 9.9931e-01],\n",
       "        [8.6025e-04, 9.9914e-01],\n",
       "        [4.3049e-02, 9.5695e-01],\n",
       "        [6.4469e-02, 9.3553e-01],\n",
       "        [1.1104e-01, 8.8896e-01],\n",
       "        [2.6185e-02, 9.7382e-01],\n",
       "        [8.8899e-04, 9.9911e-01],\n",
       "        [7.8490e-04, 9.9922e-01],\n",
       "        [7.9229e-04, 9.9921e-01],\n",
       "        [3.6012e-02, 9.6399e-01],\n",
       "        [6.8210e-04, 9.9932e-01]], dtype=torch.float64, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is NaN at i=0\n",
      "tensor(-0.9925, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9925, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=1\n",
      "tensor(1.9957, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9957, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=2\n",
      "tensor(1.9967, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9967, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=3\n",
      "tensor(1.9956, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9956, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=4\n",
      "tensor(1.9952, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=5\n",
      "tensor(-0.9902, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9902, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=6\n",
      "tensor(1.9957, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9957, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=7\n",
      "tensor(1.9954, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9954, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=8\n",
      "tensor(-0.9896, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9896, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=9\n",
      "tensor(-0.9942, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9942, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=10\n",
      "tensor(1.9952, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=11\n",
      "tensor(-0.9754, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9754, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=12\n",
      "tensor(-0.9763, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9763, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=13\n",
      "tensor(1.9955, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9955, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=14\n",
      "tensor(-0.9830, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9830, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=15\n",
      "tensor(1.9952, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=16\n",
      "tensor(-0.9394, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9394, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=17\n",
      "tensor(-0.9848, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9848, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=18\n",
      "tensor(1.9966, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9966, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=19\n",
      "tensor(1.9948, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9948, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=20\n",
      "tensor(-0.9863, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9863, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=21\n",
      "tensor(1.9954, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9954, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=22\n",
      "tensor(1.9948, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9948, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=23\n",
      "tensor(-0.9871, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9871, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=24\n",
      "tensor(-0.9830, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9830, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=25\n",
      "tensor(-0.9904, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9904, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=26\n",
      "tensor(1.9952, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=27\n",
      "tensor(1.9948, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9948, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=28\n",
      "tensor(1.9956, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9956, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=29\n",
      "tensor(1.9946, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9946, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=30\n",
      "tensor(-0.9823, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9823, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=31\n",
      "tensor(1.9951, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9951, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=32\n",
      "tensor(1.9966, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9966, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=33\n",
      "tensor(-0.9901, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9901, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=34\n",
      "tensor(-0.9853, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9853, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=35\n",
      "tensor(-0.9908, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9908, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=36\n",
      "tensor(-0.9827, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9827, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=37\n",
      "tensor(1.9949, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9949, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=38\n",
      "tensor(1.9955, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9955, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=39\n",
      "tensor(1.9983, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9983, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=40\n",
      "tensor(1.9947, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9947, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=41\n",
      "tensor(-0.9931, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9931, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=42\n",
      "tensor(-0.9899, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9899, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=43\n",
      "tensor(1.9950, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9950, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=44\n",
      "tensor(1.9953, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9953, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=45\n",
      "tensor(1.9955, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9955, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=46\n",
      "tensor(-0.9949, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9949, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=47\n",
      "tensor(1.9956, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9956, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=48\n",
      "tensor(1.9955, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9955, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=49\n",
      "tensor(1.9961, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9961, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=50\n",
      "tensor(-0.9752, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9752, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=51\n",
      "tensor(1.9947, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9947, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=52\n",
      "tensor(-0.9834, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9834, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=53\n",
      "tensor(-0.7799, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.7799, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=54\n",
      "tensor(1.9948, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9948, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=55\n",
      "tensor(-0.9832, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9832, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=56\n",
      "tensor(1.9930, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9930, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=57\n",
      "tensor(-0.9771, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9771, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=58\n",
      "tensor(1.9946, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9946, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=59\n",
      "tensor(-0.9307, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9307, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=60\n",
      "tensor(-0.9861, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9861, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=61\n",
      "tensor(-0.4644, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.4644, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=62\n",
      "tensor(1.9974, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9974, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=63\n",
      "tensor(-0.9820, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9820, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=64\n",
      "tensor(1.9948, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9948, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=65\n",
      "tensor(-0.9788, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9788, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=66\n",
      "tensor(1.9950, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9950, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=67\n",
      "tensor(1.9943, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9943, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=68\n",
      "tensor(1.9947, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9947, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=69\n",
      "tensor(-0.9866, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9866, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=70\n",
      "tensor(1.9946, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9946, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=71\n",
      "tensor(-0.9877, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9877, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=72\n",
      "tensor(-0.9944, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9944, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=73\n",
      "tensor(-0.9893, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9893, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=74\n",
      "tensor(1.9952, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=75\n",
      "tensor(1.9953, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9953, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=76\n",
      "tensor(1.9952, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=77\n",
      "tensor(-0.9892, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9892, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=78\n",
      "tensor(-0.9903, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9903, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=79\n",
      "tensor(1.9951, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9951, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=80\n",
      "tensor(1.9950, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9950, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=81\n",
      "tensor(1.9963, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9963, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=82\n",
      "tensor(1.9949, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9949, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=83\n",
      "tensor(1.9953, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9953, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=84\n",
      "tensor(-0.9873, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9873, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=85\n",
      "tensor(1.9949, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9949, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=86\n",
      "tensor(1.9948, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9948, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=87\n",
      "tensor(1.9952, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=88\n",
      "tensor(1.9960, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9960, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=89\n",
      "tensor(1.9953, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9953, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=90\n",
      "tensor(-0.9918, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9918, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=91\n",
      "tensor(1.9963, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9963, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=92\n",
      "tensor(1.9952, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=93\n",
      "tensor(1.9946, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9946, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=94\n",
      "tensor(-0.9908, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9908, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=95\n",
      "tensor(-0.9846, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9846, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=96\n",
      "tensor(-0.9920, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9920, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=97\n",
      "tensor(-0.9816, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9816, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=98\n",
      "tensor(1.9958, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9958, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=99\n",
      "tensor(1.9958, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9958, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=100\n",
      "tensor(1.9947, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9947, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=101\n",
      "tensor(-0.9934, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9934, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=102\n",
      "tensor(-0.9945, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9945, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=103\n",
      "tensor(1.9952, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9952, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=104\n",
      "tensor(1.9959, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9959, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=105\n",
      "tensor(-0.9780, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9780, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=106\n",
      "tensor(-0.9891, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9891, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=107\n",
      "tensor(-0.9850, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9850, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=108\n",
      "tensor(1.9946, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9946, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=109\n",
      "tensor(1.9929, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9929, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=110\n",
      "tensor(-0.9935, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9935, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=111\n",
      "tensor(1.9962, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9962, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=112\n",
      "tensor(1.9951, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9951, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=113\n",
      "tensor(-0.9964, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9964, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=114\n",
      "tensor(1.9970, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9970, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=115\n",
      "tensor(1.9944, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9944, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=116\n",
      "tensor(1.9962, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9962, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=117\n",
      "tensor(1.9957, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9957, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=118\n",
      "tensor(1.9943, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9943, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=119\n",
      "tensor(-0.9943, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9943, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=120\n",
      "tensor(-0.9837, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9837, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=121\n",
      "tensor(-0.9803, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9803, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=122\n",
      "tensor(-0.9957, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9957, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=123\n",
      "tensor(1.9964, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9964, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=124\n",
      "tensor(1.9969, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9969, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=125\n",
      "tensor(1.9946, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9946, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss is NaN at i=126\n",
      "tensor(-0.9924, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(-0.9924, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(0)\n",
      "Loss is NaN at i=127\n",
      "tensor(1.9963, dtype=torch.float64, grad_fn=<RsubBackward1>) tensor(-0.9963, dtype=torch.float64, grad_fn=<SelectBackward0>) tensor(1)\n",
      "Loss:  nan Reg Weight:  0.6931471805599453 Loss with Weight:  nan Accuracy:  tensor(0.5703)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "weight must sum to 1, got tensor([0.9999, 0.0000], dtype=torch.float64, grad_fn=<SelectBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m optimizer_weight\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtpcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_batch(outputs, target)\n\u001b[1;32m     19\u001b[0m reg_weight \u001b[38;5;241m=\u001b[39m tpcp_mps\u001b[38;5;241m.\u001b[39mregularize_weight(tpcp\u001b[38;5;241m.\u001b[39mW)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/notebooks/../../mps/tpcp_mps.py:210\u001b[0m, in \u001b[0;36mMPSTPCP.forward\u001b[0;34m(self, X, normalize)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrhos\u001b[38;5;241m.\u001b[39mappend(rho\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone())\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 210\u001b[0m     rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     next_rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rho(X[:, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    212\u001b[0m     rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_product(rho, next_rho)\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/notebooks/../../mps/tpcp_mps.py:300\u001b[0m, in \u001b[0;36mMPSTPCP.partial\u001b[0;34m(self, rho, site, weight)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(weight\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()), \u001b[38;5;241m1.0\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight must sum to 1, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# print(weight.norm() - 1)\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Reshape => (batch_size, d, d, d, d)\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# We can call these indices: (n, a, b, c, d).\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# But we'll just match your original naming pattern:\u001b[39;00m\n\u001b[1;32m    306\u001b[0m rho_reshaped \u001b[38;5;241m=\u001b[39m rho\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md)\n",
      "\u001b[0;31mAssertionError\u001b[0m: weight must sum to 1, got tensor([0.9999, 0.0000], dtype=torch.float64, grad_fn=<SelectBackward0>)"
     ]
    }
   ],
   "source": [
    "from mps.StiefelOptimizers import StiefelAdam\n",
    "from mps.radam import RiemannianAdam\n",
    "optimizer = RiemannianAdam(tpcp.kraus_ops.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "optimizer_weight = torch.optim.Adam([tpcp.W], lr=0.0001)\n",
    "# optimzier = StiefelAdam(tpcp.parameters(), lr=0.0001, expm_method=\"ForwardEuler\")\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    acc_tot = 0\n",
    "    loss_tot = 0\n",
    "    for data, target in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_weight.zero_grad()\n",
    "        outputs = tpcp(data)\n",
    "        loss = loss_batch(outputs, target)\n",
    "        reg_weight = tpcp_mps.regularize_weight(tpcp.W)\n",
    "        loss_with_weight = loss + reg_weight * 0.1\n",
    "        loss_with_weight.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_weight.step()\n",
    "        acc = calculate_accuracy(outputs, target)\n",
    "        acc_tot += acc\n",
    "        loss_tot += loss.item()\n",
    "        print(\"Loss: \", loss.item(), \"Reg Weight: \", reg_weight.item(), \"Loss with Weight: \", loss_with_weight.item(), \"Accuracy: \", acc)\n",
    "\n",
    "    print(f\"Epoch {epoch} / {epochs} / Loss: {loss_tot / len(trainloader)} / Accuracy: {acc_tot / len(trainloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Number of qubits must match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43msmps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m out \u001b[38;5;241m=\u001b[39m logsoftmax(out)\n\u001b[1;32m      3\u001b[0m out\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/notebooks/../../mps/simple_mps.py:252\u001b[0m, in \u001b[0;36mSimpleMPS.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    251\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of qubits must match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN):\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mops[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_qubit_inds[i]] \u001b[38;5;241m=\u001b[39m X[i]\n",
      "\u001b[0;31mAssertionError\u001b[0m: Number of qubits must match"
     ]
    }
   ],
   "source": [
    "out = smps(data)\n",
    "out = logsoftmax(out)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 1.], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tpcp(data.permute(1, 0, 2))\n",
    "\n",
    "# loss_batch(out, target)\n",
    "(torch.sgn(out) + 1) / 2 + target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
