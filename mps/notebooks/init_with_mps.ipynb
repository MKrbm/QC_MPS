{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opt_einsum as oe\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mps.simple_mps' from '/Users/keisuke/Documents/presentation/QC_MPS/mps/notebooks/../../mps/simple_mps.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from mps import simple_mps, tpcp_mps\n",
    "reload(simple_mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def embedding_pixel(X):\n",
    "#     \"\"\"\n",
    "#     class label is assumed to be zero.\n",
    "#     \"\"\"\n",
    "#     # X  = torch.stack([torch.cos(X * np.pi / 2), torch.sin(X * np.pi / 2)], dim=-1)\n",
    "#     # X = torch.stack([X, 1-X], dim=-1)\n",
    "#     X = torch.stack([torch.cos(X * np.pi / 2), torch.sin(X * np.pi / 2)], dim=-1)\n",
    "#     X /= torch.sum(X, dim=-1).unsqueeze(-1)\n",
    "#     # X = X / torch.norm(X, dim=-1).unsqueeze(-1)\n",
    "#     return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_digits(dataset, allowed_digits=[0, 1]):\n",
    "    \"\"\"Return a subset of MNIST dataset containing only allowed_digits (0 or 1).\"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if label in allowed_digits:\n",
    "            indices.append(i)\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "\n",
    "def filiter_single_channel(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    MNIST is loaded as shape [C, H, W].\n",
    "    Take only the first channel => shape [H, W].\n",
    "    \"\"\"\n",
    "    return img[0, ...]\n",
    "\n",
    "\n",
    "def embedding_pixel(batch, label: int = 0):\n",
    "    \"\"\"\n",
    "    Flatten each image from shape [H, W] => [H*W],\n",
    "    then embed x => [x, 1-x], and L2-normalize along last dim.\n",
    "    \"\"\"\n",
    "    pixel_size = batch.shape[-1] * batch.shape[-2]\n",
    "    x = batch.view(*batch.shape[:-2], pixel_size)\n",
    "    x = torch.stack([x, 1 - x], dim=-1)\n",
    "    x = x / torch.sum(x, dim=-1).unsqueeze(-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Loss & Accuracy\n",
    "###############################################################################\n",
    "def loss_batch(outputs, labels):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy style loss for outputs in [0, 1].\n",
    "    For label=0 => prob=outputs[i], else => 1 - outputs[i].\n",
    "    \"\"\"\n",
    "    device = outputs.device\n",
    "    loss = torch.zeros(1, device=device, dtype=torch.float64)\n",
    "    for i in range(len(outputs)):\n",
    "        prob = outputs[i] if labels[i] == 0 else (1 - outputs[i])\n",
    "        loss -= torch.log(prob + 1e-8)\n",
    "        # Start of Selection\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Loss is NaN at i={i}\")\n",
    "            print(prob, outputs[i], labels[i])\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Threshold 0.5 => label 0 or 1. Compare to true labels.\n",
    "    \"\"\"\n",
    "    predictions = (outputs < 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    return correct / labels.numel()\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "img_size = 16\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(filiter_single_channel),\n",
    "        transforms.Lambda(embedding_pixel),\n",
    "        transforms.Lambda(lambda x: x.to(torch.float64)),  # double precision\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "# Filter digits 0,1 only\n",
    "trainset = filter_digits(trainset, allowed_digits=[0, 1])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS with random matrices\n"
     ]
    }
   ],
   "source": [
    "# ---------- Build MPS model ----------\n",
    "N = img_size * img_size\n",
    "d = l = 2 #data input dimension and class label dimension \n",
    "chi_umps = 2\n",
    "chi_max = 2\n",
    "reload(simple_mps)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "smps = simple_mps.SimpleMPS(\n",
    "    N, \n",
    "    2,\n",
    "    d, \n",
    "    l, \n",
    "    layers=2,\n",
    "    device=device, \n",
    "    dtype=torch.float64, \n",
    "    optimize=\"greedy\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/12665 (0%)]\tLoss: 0.453299 Accuracy: 57.03%\n",
      "Train Epoch: 0 [128/12665 (1%)]\tLoss: 0.495638 Accuracy: 52.34%\n",
      "Train Epoch: 0 [256/12665 (2%)]\tLoss: 0.440528 Accuracy: 53.91%\n",
      "Train Epoch: 0 [384/12665 (3%)]\tLoss: 0.429119 Accuracy: 53.12%\n",
      "Train Epoch: 0 [512/12665 (4%)]\tLoss: 0.428116 Accuracy: 53.91%\n",
      "Train Epoch: 0 [640/12665 (5%)]\tLoss: 0.412463 Accuracy: 51.56%\n",
      "Train Epoch: 0 [768/12665 (6%)]\tLoss: 0.374117 Accuracy: 59.38%\n",
      "Train Epoch: 0 [896/12665 (7%)]\tLoss: 0.389972 Accuracy: 64.84%\n",
      "Train Epoch: 0 [1024/12665 (8%)]\tLoss: 0.401432 Accuracy: 75.00%\n",
      "Train Epoch: 0 [1152/12665 (9%)]\tLoss: 0.358179 Accuracy: 78.12%\n",
      "Train Epoch: 0 [1280/12665 (10%)]\tLoss: 0.329464 Accuracy: 81.25%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[280], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m logsoftmax(outputs)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m nnloss(outputs, target)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m data_size \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:570\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_tensor_str\u001b[38;5;241m.\u001b[39m_str(\u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents)\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    572\u001b[0m ):\n\u001b[1;32m    573\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \n\u001b[1;32m    575\u001b[0m \u001b[38;5;124;03m    The graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;124;03m            used to compute the :attr:`tensors`.\u001b[39;00m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def accuracy(outputs, target):\n",
    "    return (outputs.argmax(dim=-1) == target).float().mean()\n",
    "losses = []\n",
    "running_loss = 0\n",
    "running_accuracy = 0\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "nnloss = torch.nn.NLLLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(smps.parameters(), lr=0.001)\n",
    "n_samples = 0\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        target = target.to(device).to(torch.int64)\n",
    "        data = data.to(device).permute(1, 0, 2)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = smps(data)\n",
    "        outputs = logsoftmax(outputs)\n",
    "        loss = nnloss(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data_size = data.shape[1]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        # print(torch.exp(outputs[:10]), target[:10])\n",
    "        \n",
    "        running_loss += loss.item() * data_size\n",
    "        n_samples += data_size\n",
    "        \n",
    "        if batch_idx % 1 == 0:\n",
    "            avg_loss = running_loss / n_samples\n",
    "            avg_accuracy = accuracy(outputs, target)\n",
    "            losses.append(avg_loss)\n",
    "            running_loss = 0\n",
    "            running_accuracy = 0\n",
    "            n_samples = 0\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Accuracy: {:.2f}%'.format(\n",
    "                epoch, batch_idx * data_size, len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), avg_loss, avg_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mps import tpcp_mps  \n",
    "\n",
    "reload(tpcp_mps)\n",
    "\n",
    "tpcp = tpcp_mps.MPSTPCP(N, K=1, d=2, with_identity=True, manifold=tpcp_mps.ManifoldType.EXACT)\n",
    "tpcp.W.data[:, 1] = 0\n",
    "tpcp.W.data[:, 0] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7238, -0.6897],\n",
      "        [ 0.0000, -0.0206]], dtype=torch.float64)\n",
      "tensor([[-1., -0.],\n",
      "        [-0., -1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "tpcp.set_canonical_mps(smps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5859)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(outputs, target):\n",
    "    correct = (outputs < 0).float() == target.float()\n",
    "    return correct.float().sum() / target.numel()\n",
    "\n",
    "out = tpcp(data)\n",
    "\n",
    "calculate_accuracy(out, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7238, -0.6897],\n",
      "        [ 0.0000, -0.0206]], dtype=torch.float64)\n",
      "tensor([[-1., -0.],\n",
      "        [-0., -1.]], dtype=torch.float64)\n",
      "Loss:  165.57302303892448 Accuracy:  tensor(0.6719)\n",
      "Loss:  136.72517229357248 Accuracy:  tensor(0.5859)\n",
      "Loss:  214.9545905259381 Accuracy:  tensor(0.6406)\n",
      "Loss:  253.91501380386796 Accuracy:  tensor(0.5859)\n",
      "Loss:  141.90039993553148 Accuracy:  tensor(0.6250)\n",
      "Loss:  176.6534983625568 Accuracy:  tensor(0.5703)\n",
      "Loss:  158.49419001701222 Accuracy:  tensor(0.6406)\n",
      "Loss:  178.77590048242553 Accuracy:  tensor(0.6641)\n",
      "Loss:  213.98763350344754 Accuracy:  tensor(0.5781)\n",
      "Loss:  199.19376933620217 Accuracy:  tensor(0.5781)\n",
      "Loss:  164.44858785599 Accuracy:  tensor(0.6562)\n",
      "Loss:  184.13926336246774 Accuracy:  tensor(0.5859)\n",
      "Loss:  133.45233923736976 Accuracy:  tensor(0.6016)\n",
      "Loss:  172.4889233851241 Accuracy:  tensor(0.5859)\n",
      "Loss:  141.63168050465572 Accuracy:  tensor(0.6875)\n",
      "Loss:  140.35010075661165 Accuracy:  tensor(0.6016)\n",
      "Loss:  154.4957060693978 Accuracy:  tensor(0.5547)\n",
      "Loss:  134.34951881878285 Accuracy:  tensor(0.6172)\n",
      "Loss:  137.03075237526926 Accuracy:  tensor(0.6328)\n",
      "Loss:  120.67726948185717 Accuracy:  tensor(0.5625)\n",
      "Loss:  116.09244612322938 Accuracy:  tensor(0.6250)\n",
      "Loss:  130.0958648959752 Accuracy:  tensor(0.5625)\n",
      "Loss:  113.70458437659333 Accuracy:  tensor(0.6484)\n",
      "Loss:  130.46479524853294 Accuracy:  tensor(0.6250)\n",
      "Loss:  69.40501904308465 Accuracy:  tensor(0.7422)\n",
      "Loss:  79.46577443049296 Accuracy:  tensor(0.7188)\n",
      "Loss:  107.15132012466447 Accuracy:  tensor(0.6484)\n",
      "Loss:  70.30511924996578 Accuracy:  tensor(0.7422)\n",
      "Loss:  86.93899565028664 Accuracy:  tensor(0.6953)\n",
      "Loss:  92.91186684360396 Accuracy:  tensor(0.6484)\n",
      "Loss:  70.69386879199668 Accuracy:  tensor(0.7344)\n",
      "Loss:  76.3471047627098 Accuracy:  tensor(0.7031)\n",
      "Loss:  118.94224412388321 Accuracy:  tensor(0.6328)\n",
      "Loss:  104.82568739195202 Accuracy:  tensor(0.6641)\n",
      "Loss:  106.98246817945369 Accuracy:  tensor(0.6172)\n",
      "Loss:  69.94293330745411 Accuracy:  tensor(0.7109)\n",
      "Loss:  91.46368160596178 Accuracy:  tensor(0.7266)\n",
      "Loss:  117.33177750389834 Accuracy:  tensor(0.6094)\n",
      "Loss:  111.36381996597372 Accuracy:  tensor(0.6484)\n",
      "Loss:  71.49579819275958 Accuracy:  tensor(0.7344)\n",
      "Loss:  72.42769071895859 Accuracy:  tensor(0.7422)\n",
      "Loss:  66.95898050673269 Accuracy:  tensor(0.7109)\n",
      "Loss:  93.14769683432716 Accuracy:  tensor(0.6953)\n",
      "Loss:  78.79017083360893 Accuracy:  tensor(0.6797)\n",
      "Loss:  73.5657873765732 Accuracy:  tensor(0.7500)\n",
      "Loss:  74.92088083184414 Accuracy:  tensor(0.7031)\n",
      "Loss:  91.03456908319427 Accuracy:  tensor(0.7031)\n",
      "Loss:  95.61614710575024 Accuracy:  tensor(0.6719)\n",
      "Loss:  80.67374715253413 Accuracy:  tensor(0.6562)\n",
      "Loss:  80.94946669237287 Accuracy:  tensor(0.6797)\n",
      "Loss:  62.39083407634815 Accuracy:  tensor(0.7500)\n",
      "Loss:  70.41955163328679 Accuracy:  tensor(0.7266)\n",
      "Loss:  47.61946788493756 Accuracy:  tensor(0.8594)\n",
      "Loss:  60.59983912306642 Accuracy:  tensor(0.7578)\n",
      "Loss:  62.86918471575532 Accuracy:  tensor(0.7500)\n",
      "Loss:  46.348970588238835 Accuracy:  tensor(0.8281)\n",
      "Loss:  53.15680626869571 Accuracy:  tensor(0.8203)\n",
      "Loss:  71.5846628849838 Accuracy:  tensor(0.6953)\n",
      "Loss:  55.56272980860867 Accuracy:  tensor(0.7734)\n",
      "Loss:  71.64227292515486 Accuracy:  tensor(0.7109)\n",
      "Loss:  83.16117088951499 Accuracy:  tensor(0.6953)\n",
      "Loss:  68.9608570098583 Accuracy:  tensor(0.7266)\n",
      "Loss:  57.02873738882221 Accuracy:  tensor(0.7891)\n",
      "Loss:  66.53644920027716 Accuracy:  tensor(0.7500)\n",
      "Loss:  72.58836794520671 Accuracy:  tensor(0.7266)\n",
      "Loss:  62.85632584582623 Accuracy:  tensor(0.7578)\n",
      "Loss:  62.970404921282835 Accuracy:  tensor(0.7266)\n",
      "Loss:  47.60600841405679 Accuracy:  tensor(0.8281)\n",
      "Loss:  47.76475984193666 Accuracy:  tensor(0.8125)\n",
      "Loss:  48.63837556249806 Accuracy:  tensor(0.8047)\n",
      "Loss:  54.418582279579226 Accuracy:  tensor(0.8125)\n",
      "Loss:  64.2825865310145 Accuracy:  tensor(0.7266)\n",
      "Loss:  46.14916478326406 Accuracy:  tensor(0.8203)\n",
      "Loss:  45.1510036292252 Accuracy:  tensor(0.8047)\n",
      "Loss:  50.87758905102121 Accuracy:  tensor(0.7969)\n",
      "Loss:  51.217316454720915 Accuracy:  tensor(0.7656)\n",
      "Loss:  42.82340557059318 Accuracy:  tensor(0.8594)\n",
      "Loss:  66.75751153050028 Accuracy:  tensor(0.7109)\n",
      "Loss:  48.492587563097004 Accuracy:  tensor(0.7969)\n",
      "Loss:  56.937947718171344 Accuracy:  tensor(0.7891)\n",
      "Loss:  63.66412660703711 Accuracy:  tensor(0.7031)\n",
      "Loss:  48.98074156374558 Accuracy:  tensor(0.7578)\n",
      "Loss:  44.85089029265335 Accuracy:  tensor(0.8047)\n",
      "Loss:  45.320473231170226 Accuracy:  tensor(0.8125)\n",
      "Loss:  50.303741250024515 Accuracy:  tensor(0.7812)\n",
      "Loss:  47.22528742207849 Accuracy:  tensor(0.8125)\n",
      "Loss:  60.58635743248437 Accuracy:  tensor(0.7500)\n",
      "Loss:  54.353597841952734 Accuracy:  tensor(0.7812)\n",
      "Loss:  46.454136893995376 Accuracy:  tensor(0.7812)\n",
      "Loss:  44.91422321589581 Accuracy:  tensor(0.7891)\n",
      "Loss:  42.91674405664644 Accuracy:  tensor(0.8047)\n",
      "Loss:  47.34873571878865 Accuracy:  tensor(0.8047)\n",
      "Loss:  48.94108531456117 Accuracy:  tensor(0.7734)\n",
      "Loss:  45.607174114563534 Accuracy:  tensor(0.7891)\n",
      "Loss:  43.01894236219282 Accuracy:  tensor(0.8359)\n",
      "Loss:  39.99847632335003 Accuracy:  tensor(0.8047)\n",
      "Loss:  41.99234604568373 Accuracy:  tensor(0.8438)\n",
      "Loss:  35.51500556950319 Accuracy:  tensor(0.8594)\n",
      "Loss:  42.70612339025171 Accuracy:  tensor(0.8264)\n",
      "Epoch 0 / 100 / Loss: 87.97332647254032 / Accuracy: 0.7166023850440979\n",
      "Loss:  44.90019797189246 Accuracy:  tensor(0.8047)\n",
      "Loss:  40.85698587843552 Accuracy:  tensor(0.8281)\n",
      "Loss:  47.388499271901495 Accuracy:  tensor(0.7891)\n",
      "Loss:  59.33630291883346 Accuracy:  tensor(0.7344)\n",
      "Loss:  36.17756509558864 Accuracy:  tensor(0.8516)\n",
      "Loss:  41.03709308997127 Accuracy:  tensor(0.8281)\n",
      "Loss:  38.93591645539361 Accuracy:  tensor(0.8203)\n",
      "Loss:  40.94917300774502 Accuracy:  tensor(0.8359)\n",
      "Loss:  56.33758722096856 Accuracy:  tensor(0.7422)\n",
      "Loss:  46.44582993464758 Accuracy:  tensor(0.7891)\n",
      "Loss:  42.16742199712703 Accuracy:  tensor(0.7891)\n",
      "Loss:  48.98480644155705 Accuracy:  tensor(0.8047)\n",
      "Loss:  37.3290589715161 Accuracy:  tensor(0.8281)\n",
      "Loss:  44.85836254648408 Accuracy:  tensor(0.7969)\n",
      "Loss:  43.1215267486861 Accuracy:  tensor(0.8125)\n",
      "Loss:  42.43317190833631 Accuracy:  tensor(0.7969)\n",
      "Loss:  44.776118597902254 Accuracy:  tensor(0.8359)\n",
      "Loss:  38.511489315320155 Accuracy:  tensor(0.8125)\n",
      "Loss:  46.26877303548291 Accuracy:  tensor(0.7734)\n",
      "Loss:  35.61083650447251 Accuracy:  tensor(0.8672)\n",
      "Loss:  36.63485228923744 Accuracy:  tensor(0.8203)\n",
      "Loss:  40.43697861399463 Accuracy:  tensor(0.8359)\n",
      "Loss:  36.66757432640988 Accuracy:  tensor(0.8359)\n",
      "Loss:  42.723848777854414 Accuracy:  tensor(0.7812)\n",
      "Loss:  24.635437237102103 Accuracy:  tensor(0.8906)\n",
      "Loss:  27.416481066583835 Accuracy:  tensor(0.9062)\n",
      "Loss:  38.248098161887484 Accuracy:  tensor(0.8438)\n",
      "Loss:  24.67401401116274 Accuracy:  tensor(0.9375)\n",
      "Loss:  33.82639677523165 Accuracy:  tensor(0.8672)\n",
      "Loss:  38.30175481516519 Accuracy:  tensor(0.8281)\n",
      "Loss:  23.764574124544186 Accuracy:  tensor(0.9219)\n",
      "Loss:  25.764120438472055 Accuracy:  tensor(0.9062)\n",
      "Loss:  40.92412655021273 Accuracy:  tensor(0.7891)\n",
      "Loss:  39.229584470929915 Accuracy:  tensor(0.8359)\n",
      "Loss:  36.8477859795711 Accuracy:  tensor(0.8359)\n",
      "Loss:  27.57600524526611 Accuracy:  tensor(0.9219)\n",
      "Loss:  34.60911777439673 Accuracy:  tensor(0.8203)\n",
      "Loss:  43.36667389134298 Accuracy:  tensor(0.7891)\n",
      "Loss:  41.95652931835926 Accuracy:  tensor(0.7656)\n",
      "Loss:  26.979228226426894 Accuracy:  tensor(0.8984)\n",
      "Loss:  28.08280540834037 Accuracy:  tensor(0.8984)\n",
      "Loss:  25.139069724713387 Accuracy:  tensor(0.9219)\n",
      "Loss:  35.793579963079175 Accuracy:  tensor(0.8281)\n",
      "Loss:  31.294601027302612 Accuracy:  tensor(0.8828)\n",
      "Loss:  26.903475560819256 Accuracy:  tensor(0.8984)\n",
      "Loss:  28.67484017479595 Accuracy:  tensor(0.8984)\n",
      "Loss:  37.57029722437082 Accuracy:  tensor(0.8203)\n",
      "Loss:  38.41592082767066 Accuracy:  tensor(0.8281)\n",
      "Loss:  32.65304251611005 Accuracy:  tensor(0.8906)\n",
      "Loss:  35.95675845952542 Accuracy:  tensor(0.8281)\n",
      "Loss:  27.529524114437216 Accuracy:  tensor(0.9141)\n",
      "Loss:  28.67771712676487 Accuracy:  tensor(0.8984)\n",
      "Loss:  22.827857041644776 Accuracy:  tensor(0.9219)\n",
      "Loss:  22.913555992579823 Accuracy:  tensor(0.9219)\n",
      "Loss:  25.85180119572861 Accuracy:  tensor(0.9219)\n",
      "Loss:  19.98507023705184 Accuracy:  tensor(0.9453)\n",
      "Loss:  21.279864513307313 Accuracy:  tensor(0.9375)\n",
      "Loss:  31.416770537853427 Accuracy:  tensor(0.8750)\n",
      "Loss:  22.099555194065324 Accuracy:  tensor(0.9297)\n",
      "Loss:  34.79358761770101 Accuracy:  tensor(0.8906)\n",
      "Loss:  32.31952266138134 Accuracy:  tensor(0.8516)\n",
      "Loss:  27.135817044438546 Accuracy:  tensor(0.8906)\n",
      "Loss:  26.162880844083592 Accuracy:  tensor(0.9141)\n",
      "Loss:  28.720861511488778 Accuracy:  tensor(0.8750)\n",
      "Loss:  29.656503479541744 Accuracy:  tensor(0.8750)\n",
      "Loss:  33.63647470665223 Accuracy:  tensor(0.8281)\n",
      "Loss:  26.707990185468542 Accuracy:  tensor(0.9297)\n",
      "Loss:  22.045083673013433 Accuracy:  tensor(0.9141)\n",
      "Loss:  26.440876655269317 Accuracy:  tensor(0.8984)\n",
      "Loss:  23.213073915644035 Accuracy:  tensor(0.9375)\n",
      "Loss:  27.26600421024104 Accuracy:  tensor(0.8984)\n",
      "Loss:  28.27597067719381 Accuracy:  tensor(0.8906)\n",
      "Loss:  19.709225966560474 Accuracy:  tensor(0.9219)\n",
      "Loss:  20.002908311182132 Accuracy:  tensor(0.9531)\n",
      "Loss:  19.746973769184457 Accuracy:  tensor(0.9531)\n",
      "Loss:  24.13199302916251 Accuracy:  tensor(0.9453)\n",
      "Loss:  23.417877492810074 Accuracy:  tensor(0.9141)\n",
      "Loss:  30.15198139392594 Accuracy:  tensor(0.8984)\n",
      "Loss:  23.093361445836653 Accuracy:  tensor(0.9141)\n",
      "Loss:  23.47579148501706 Accuracy:  tensor(0.9062)\n",
      "Loss:  25.157556879153354 Accuracy:  tensor(0.9219)\n",
      "Loss:  24.391007910570195 Accuracy:  tensor(0.9375)\n",
      "Loss:  24.5949061636504 Accuracy:  tensor(0.9062)\n",
      "Loss:  21.460371059636635 Accuracy:  tensor(0.9453)\n",
      "Loss:  23.15585190438139 Accuracy:  tensor(0.9297)\n",
      "Loss:  19.196317071518763 Accuracy:  tensor(0.9766)\n",
      "Loss:  32.49790773564782 Accuracy:  tensor(0.8594)\n",
      "Loss:  24.4510494611205 Accuracy:  tensor(0.9219)\n",
      "Loss:  19.136763109707616 Accuracy:  tensor(0.9844)\n",
      "Loss:  20.642246249227775 Accuracy:  tensor(0.9688)\n",
      "Loss:  18.89961942803038 Accuracy:  tensor(0.9844)\n",
      "Loss:  19.44457329297703 Accuracy:  tensor(0.9453)\n",
      "Loss:  20.749403480618522 Accuracy:  tensor(0.9531)\n",
      "Loss:  20.79149519865384 Accuracy:  tensor(0.9609)\n",
      "Loss:  20.184805161271314 Accuracy:  tensor(0.9531)\n",
      "Loss:  18.521298613252398 Accuracy:  tensor(0.9688)\n",
      "Loss:  20.044489745269708 Accuracy:  tensor(0.9375)\n",
      "Loss:  14.0085526941895 Accuracy:  tensor(0.9844)\n",
      "Loss:  23.483760949922633 Accuracy:  tensor(0.9091)\n",
      "Epoch 1 / 100 / Loss: 30.959522646779515 / Accuracy: 0.8796057105064392\n",
      "Loss:  21.091960227973335 Accuracy:  tensor(0.9531)\n",
      "Loss:  20.618963239517416 Accuracy:  tensor(0.9531)\n",
      "Loss:  22.31807207626759 Accuracy:  tensor(0.9375)\n",
      "Loss:  26.731391431746633 Accuracy:  tensor(0.8984)\n",
      "Loss:  17.187855393586172 Accuracy:  tensor(0.9688)\n",
      "Loss:  17.925027162238948 Accuracy:  tensor(0.9766)\n",
      "Loss:  17.958921285765484 Accuracy:  tensor(0.9766)\n",
      "Loss:  17.999686318248607 Accuracy:  tensor(0.9453)\n",
      "Loss:  28.03568975926552 Accuracy:  tensor(0.9219)\n",
      "Loss:  19.713849114319842 Accuracy:  tensor(0.9531)\n",
      "Loss:  19.6065799637089 Accuracy:  tensor(0.9609)\n",
      "Loss:  24.41880668386383 Accuracy:  tensor(0.8906)\n",
      "Loss:  17.10435561891601 Accuracy:  tensor(0.9609)\n",
      "Loss:  20.423940904478357 Accuracy:  tensor(0.9531)\n",
      "Loss:  21.38306301649456 Accuracy:  tensor(0.9297)\n",
      "Loss:  19.038783438821135 Accuracy:  tensor(0.9688)\n",
      "Loss:  21.018921076870715 Accuracy:  tensor(0.9531)\n",
      "Loss:  16.14312505807095 Accuracy:  tensor(0.9844)\n",
      "Loss:  24.07779660324336 Accuracy:  tensor(0.9453)\n",
      "Loss:  16.788385244816652 Accuracy:  tensor(0.9766)\n",
      "Loss:  17.06753088504731 Accuracy:  tensor(0.9688)\n",
      "Loss:  20.19136290291541 Accuracy:  tensor(0.9453)\n",
      "Loss:  17.04571662941008 Accuracy:  tensor(0.9688)\n",
      "Loss:  19.55496562308235 Accuracy:  tensor(0.9531)\n",
      "Loss:  12.071913645949786 Accuracy:  tensor(0.9766)\n",
      "Loss:  14.244094592036332 Accuracy:  tensor(0.9609)\n",
      "Loss:  21.436297853594095 Accuracy:  tensor(0.9453)\n",
      "Loss:  11.920233832840422 Accuracy:  tensor(0.9766)\n",
      "Loss:  19.33999705074 Accuracy:  tensor(0.9453)\n",
      "Loss:  22.770465527545547 Accuracy:  tensor(0.8906)\n",
      "Loss:  10.460533716724052 Accuracy:  tensor(0.9922)\n",
      "Loss:  12.247515840486843 Accuracy:  tensor(0.9766)\n",
      "Loss:  18.963634379658515 Accuracy:  tensor(0.9531)\n",
      "Loss:  19.483211724206445 Accuracy:  tensor(0.9688)\n",
      "Loss:  17.48582484401694 Accuracy:  tensor(0.9688)\n",
      "Loss:  15.797239021666982 Accuracy:  tensor(0.9688)\n",
      "Loss:  16.47278762960778 Accuracy:  tensor(0.9766)\n",
      "Loss:  21.104319850343792 Accuracy:  tensor(0.9531)\n",
      "Loss:  20.251154765386854 Accuracy:  tensor(0.9688)\n",
      "Loss:  12.496300841676032 Accuracy:  tensor(0.9766)\n",
      "Loss:  13.043219776468538 Accuracy:  tensor(0.9766)\n",
      "Loss:  12.102192851358037 Accuracy:  tensor(0.9766)\n",
      "Loss:  17.139929867457923 Accuracy:  tensor(0.9688)\n",
      "Loss:  15.761450782735919 Accuracy:  tensor(0.9844)\n",
      "Loss:  12.258102593801619 Accuracy:  tensor(0.9844)\n",
      "Loss:  13.299030165825904 Accuracy:  tensor(0.9766)\n",
      "Loss:  19.224889945503154 Accuracy:  tensor(0.9531)\n",
      "Loss:  21.310979443617878 Accuracy:  tensor(0.9531)\n",
      "Loss:  16.41550019807685 Accuracy:  tensor(0.9688)\n",
      "Loss:  19.237011097337145 Accuracy:  tensor(0.9531)\n",
      "Loss:  14.933712868976503 Accuracy:  tensor(0.9844)\n",
      "Loss:  14.273634888151319 Accuracy:  tensor(0.9844)\n",
      "Loss:  12.850910102504415 Accuracy:  tensor(0.9609)\n",
      "Loss:  10.65191069419674 Accuracy:  tensor(0.9922)\n",
      "Loss:  13.656000727312067 Accuracy:  tensor(0.9766)\n",
      "Loss:  10.214354722436246 Accuracy:  tensor(0.9844)\n",
      "Loss:  10.218457041512785 Accuracy:  tensor(0.9922)\n",
      "Loss:  17.85645889358909 Accuracy:  tensor(0.9531)\n",
      "Loss:  10.347045691940712 Accuracy:  tensor(0.9922)\n",
      "Loss:  20.794708315723778 Accuracy:  tensor(0.9453)\n",
      "Loss:  15.214004136356287 Accuracy:  tensor(0.9844)\n",
      "Loss:  13.310506968848825 Accuracy:  tensor(0.9922)\n",
      "Loss:  14.030173780234533 Accuracy:  tensor(0.9688)\n",
      "Loss:  15.569641061390593 Accuracy:  tensor(0.9844)\n",
      "Loss:  14.295729093598903 Accuracy:  tensor(0.9922)\n",
      "Loss:  19.52536402263365 Accuracy:  tensor(0.9453)\n",
      "Loss:  13.6154882609415 Accuracy:  tensor(0.9922)\n",
      "Loss:  12.259011342265676 Accuracy:  tensor(0.9688)\n",
      "Loss:  14.94716751722981 Accuracy:  tensor(0.9531)\n",
      "Loss:  12.425634344168998 Accuracy:  tensor(0.9688)\n",
      "Loss:  15.675770786503689 Accuracy:  tensor(0.9375)\n",
      "Loss:  14.043753720560002 Accuracy:  tensor(0.9844)\n",
      "Loss:  9.396538749007028 Accuracy:  tensor(0.9922)\n",
      "Loss:  10.133676695090605 Accuracy:  tensor(1.)\n",
      "Loss:  8.924353762134674 Accuracy:  tensor(1.)\n",
      "Loss:  13.353572553282987 Accuracy:  tensor(0.9766)\n",
      "Loss:  14.058075934586984 Accuracy:  tensor(0.9609)\n",
      "Loss:  16.49509319304563 Accuracy:  tensor(0.9688)\n",
      "Loss:  12.181286218517373 Accuracy:  tensor(0.9844)\n",
      "Loss:  11.204082844931383 Accuracy:  tensor(1.)\n",
      "Loss:  11.805731964702256 Accuracy:  tensor(1.)\n",
      "Loss:  13.245524290355686 Accuracy:  tensor(0.9688)\n",
      "Loss:  13.970926926185387 Accuracy:  tensor(0.9766)\n",
      "Loss:  12.199153761341535 Accuracy:  tensor(0.9766)\n",
      "Loss:  12.117541202471315 Accuracy:  tensor(0.9844)\n",
      "Loss:  9.15189736632785 Accuracy:  tensor(1.)\n",
      "Loss:  19.463932680404167 Accuracy:  tensor(0.9531)\n",
      "Loss:  12.75123751860941 Accuracy:  tensor(0.9844)\n",
      "Loss:  9.013111414732707 Accuracy:  tensor(1.)\n",
      "Loss:  10.78851540886807 Accuracy:  tensor(0.9844)\n",
      "Loss:  9.703965605671657 Accuracy:  tensor(0.9922)\n",
      "Loss:  9.2562489863895 Accuracy:  tensor(1.)\n",
      "Loss:  10.164466375070965 Accuracy:  tensor(1.)\n",
      "Loss:  10.716221067400102 Accuracy:  tensor(0.9922)\n",
      "Loss:  11.199316532413503 Accuracy:  tensor(0.9922)\n",
      "Loss:  9.406983213732738 Accuracy:  tensor(0.9844)\n",
      "Loss:  11.160189550181602 Accuracy:  tensor(0.9688)\n",
      "Loss:  6.470337628179477 Accuracy:  tensor(1.)\n",
      "Loss:  14.89314607972221 Accuracy:  tensor(0.9669)\n",
      "Epoch 2 / 100 / Loss: 15.492092404058237 / Accuracy: 0.9695208668708801\n",
      "Loss:  11.388764637923009 Accuracy:  tensor(0.9844)\n",
      "Loss:  11.60375028530159 Accuracy:  tensor(0.9766)\n",
      "Loss:  12.763942447145329 Accuracy:  tensor(0.9844)\n",
      "Loss:  13.794048125850168 Accuracy:  tensor(1.)\n",
      "Loss:  9.48890869323764 Accuracy:  tensor(0.9844)\n",
      "Loss:  9.026832408690348 Accuracy:  tensor(0.9922)\n",
      "Loss:  10.085577664773794 Accuracy:  tensor(0.9844)\n",
      "Loss:  9.237528898858733 Accuracy:  tensor(0.9922)\n",
      "Loss:  16.20003085481923 Accuracy:  tensor(0.9766)\n",
      "Loss:  10.252570982999295 Accuracy:  tensor(0.9922)\n",
      "Loss:  10.588967603664672 Accuracy:  tensor(0.9766)\n",
      "Loss:  14.226297939403468 Accuracy:  tensor(0.9766)\n",
      "Loss:  9.2609381928732 Accuracy:  tensor(0.9922)\n",
      "Loss:  10.931096269525474 Accuracy:  tensor(0.9844)\n",
      "Loss:  13.053200793275439 Accuracy:  tensor(0.9844)\n",
      "Loss:  9.727423216408503 Accuracy:  tensor(0.9922)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[294], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtpcp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_batch(outputs, target)\n\u001b[1;32m     14\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/notebooks/../../mps/tpcp_mps.py:212\u001b[0m, in \u001b[0;36mMPSTPCP.forward\u001b[0;34m(self, X, normalize)\u001b[0m\n\u001b[1;32m    210\u001b[0m         rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial(rho, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[i])\n\u001b[1;32m    211\u001b[0m         next_rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_rho(X[:, i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 212\u001b[0m         rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrho\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_rho\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# Start of Selection\u001b[39;00m\n\u001b[1;32m    215\u001b[0m rho_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial(rho, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/notebooks/../../mps/tpcp_mps.py:233\u001b[0m, in \u001b[0;36mMPSTPCP.tensor_product\u001b[0;34m(rho1, rho2)\u001b[0m\n\u001b[1;32m    231\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m rho1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    232\u001b[0m d \u001b[38;5;241m=\u001b[39m rho1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbij,bkl->bikjl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(batch_size, d\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, d\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/functional.py:407\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    409\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mps.StiefelOptimizers import StiefelAdam\n",
    "from mps.radam import RiemannianAdam\n",
    "tpcp.set_canonical_mps(smps)\n",
    "W = torch.zeros(tpcp.L, 2, dtype=torch.float64)\n",
    "tpcp.initialize_W()\n",
    "# optimizer = RiemannianAdam(tpcp.parameters(), lr=0.0001)\n",
    "optimzier = StiefelAdam(tpcp.parameters(), lr=0.0001, expm_method=\"ForwardEuler\")\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    acc_tot = 0\n",
    "    loss_tot = 0\n",
    "    for data, target in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = tpcp(data)\n",
    "        loss = loss_batch(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = calculate_accuracy(outputs, target)\n",
    "        acc_tot += acc\n",
    "        loss_tot += loss.item()\n",
    "        print(\"Loss: \", loss.item(), \"Accuracy: \", acc)\n",
    "\n",
    "    print(f\"Epoch {epoch} / {epochs} / Loss: {loss_tot / len(trainloader)} / Accuracy: {acc_tot / len(trainloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0300e+01, -3.3639e-05],\n",
       "        [-1.0873e+01, -1.8957e-05],\n",
       "        [-1.1319e+01, -1.2135e-05],\n",
       "        [-1.0072e+01, -4.2227e-05],\n",
       "        [-1.1331e+01, -1.2001e-05],\n",
       "        [-8.8451e+00, -1.4410e-04],\n",
       "        [ 0.0000e+00, -4.8987e+01],\n",
       "        [-3.3434e-08, -1.7214e+01],\n",
       "        [-9.8337e+00, -5.3613e-05],\n",
       "        [-1.0920e+01, -1.8086e-05],\n",
       "        [-3.4195e-14, -3.1010e+01],\n",
       "        [-1.0579e+01, -2.5457e-05],\n",
       "        [-6.0174e-14, -3.0443e+01],\n",
       "        [-7.9001e+00, -3.7078e-04],\n",
       "        [ 0.0000e+00, -3.8507e+01],\n",
       "        [-9.9722e+00, -4.6679e-05],\n",
       "        [-4.7244e-11, -2.3776e+01],\n",
       "        [-5.5502e-12, -2.5917e+01],\n",
       "        [-1.0454e+01, -2.8819e-05],\n",
       "        [-3.4917e-05, -1.0263e+01],\n",
       "        [-2.2871e-14, -3.1411e+01],\n",
       "        [ 0.0000e+00, -4.2346e+01],\n",
       "        [ 0.0000e+00, -4.4464e+01],\n",
       "        [-1.0186e+01, -3.7694e-05],\n",
       "        [-3.7323e-08, -1.7104e+01],\n",
       "        [ 0.0000e+00, -7.0261e+01],\n",
       "        [-8.3386e+00, -2.3913e-04],\n",
       "        [-8.4054e+00, -2.2367e-04],\n",
       "        [-2.2990e-12, -2.6799e+01],\n",
       "        [-9.8108e+00, -5.4855e-05],\n",
       "        [ 0.0000e+00, -1.0694e+02],\n",
       "        [ 0.0000e+00, -5.5585e+01],\n",
       "        [ 0.0000e+00, -8.0064e+01],\n",
       "        [-1.0305e+01, -3.3460e-05],\n",
       "        [-2.6461e-03, -5.9360e+00],\n",
       "        [-1.1151e+01, -1.4368e-05],\n",
       "        [-1.0448e+01, -2.9003e-05],\n",
       "        [-1.1947e+01, -6.4773e-06],\n",
       "        [-1.2487e+01, -3.7749e-06],\n",
       "        [-7.9270e-14, -3.0165e+01],\n",
       "        [-8.7669e+00, -1.5583e-04],\n",
       "        [-7.7959e+00, -4.1152e-04],\n",
       "        [-1.0397e+01, -3.0529e-05],\n",
       "        [-9.1846e-11, -2.3111e+01],\n",
       "        [-1.4428e-10, -2.2659e+01],\n",
       "        [-1.1305e+01, -1.2313e-05],\n",
       "        [-5.7732e-15, -3.2788e+01],\n",
       "        [-8.8348e+00, -1.4558e-04],\n",
       "        [-4.3195e+00, -1.3396e-02],\n",
       "        [ 0.0000e+00, -4.5750e+01],\n",
       "        [-9.8743e+00, -5.1481e-05],\n",
       "        [ 0.0000e+00, -6.7362e+01],\n",
       "        [-7.5816e-11, -2.3303e+01],\n",
       "        [-9.4808e+00, -7.6304e-05],\n",
       "        [-7.2686e-08, -1.6437e+01],\n",
       "        [-5.1070e-15, -3.2899e+01],\n",
       "        [ 0.0000e+00, -7.3113e+01],\n",
       "        [-9.5608e+00, -7.0437e-05],\n",
       "        [ 0.0000e+00, -4.3780e+01],\n",
       "        [-1.3420e-10, -2.2732e+01],\n",
       "        [-2.4872e-04, -8.2993e+00],\n",
       "        [-1.0308e+01, -3.3352e-05],\n",
       "        [-1.0609e+01, -2.4692e-05],\n",
       "        [-1.0112e+01, -4.0607e-05],\n",
       "        [-9.2991e+00, -9.1509e-05],\n",
       "        [ 0.0000e+00, -7.9220e+01],\n",
       "        [-1.2370e+01, -4.2446e-06],\n",
       "        [-9.2417e+00, -9.6915e-05],\n",
       "        [-9.3244e+00, -8.9228e-05],\n",
       "        [-1.0151e+01, -3.9049e-05],\n",
       "        [-9.5334e+00, -7.2398e-05],\n",
       "        [-1.0867e+01, -1.9071e-05],\n",
       "        [-8.6078e+00, -1.8270e-04],\n",
       "        [-1.1359e+01, -1.1658e-05],\n",
       "        [-1.4509e-04, -8.8382e+00],\n",
       "        [-2.0113e-04, -8.5117e+00],\n",
       "        [-9.6000e-10, -2.0764e+01],\n",
       "        [-8.8818e-16, -3.4576e+01],\n",
       "        [-9.3467e+00, -8.7259e-05],\n",
       "        [-1.0057e+01, -4.2879e-05],\n",
       "        [-9.6938e+00, -6.1667e-05],\n",
       "        [-1.0341e+01, -3.2295e-05],\n",
       "        [-1.0348e+01, -3.2072e-05],\n",
       "        [ 0.0000e+00, -4.8151e+01],\n",
       "        [-8.7428e-09, -1.8555e+01],\n",
       "        [-1.1302e-13, -2.9811e+01],\n",
       "        [-1.2438e+01, -3.9643e-06],\n",
       "        [-1.1489e+01, -1.0242e-05],\n",
       "        [-1.0490e+01, -2.7825e-05],\n",
       "        [-1.0153e+01, -3.8944e-05],\n",
       "        [-1.1145e+01, -1.4441e-05],\n",
       "        [-1.1016e+01, -1.6445e-05],\n",
       "        [-5.0633e-12, -2.6009e+01],\n",
       "        [ 0.0000e+00, -4.5396e+01],\n",
       "        [-1.0687e+01, -2.2841e-05],\n",
       "        [-1.7497e-05, -1.0954e+01],\n",
       "        [-1.1177e+01, -1.3990e-05],\n",
       "        [-2.2204e-16, -3.5648e+01],\n",
       "        [-2.2204e-16, -3.5662e+01],\n",
       "        [-6.6613e-16, -3.5122e+01],\n",
       "        [ 0.0000e+00, -7.6281e+01],\n",
       "        [-1.0800e+01, -2.0392e-05],\n",
       "        [ 0.0000e+00, -5.8252e+01],\n",
       "        [-1.1082e-10, -2.2923e+01],\n",
       "        [-5.0003e-09, -1.9114e+01],\n",
       "        [ 0.0000e+00, -9.4440e+01],\n",
       "        [-9.5185e+00, -7.3479e-05],\n",
       "        [-4.9147e-04, -7.6184e+00],\n",
       "        [ 0.0000e+00, -8.6677e+01],\n",
       "        [ 0.0000e+00, -4.1636e+01],\n",
       "        [-1.0143e+01, -3.9352e-05],\n",
       "        [ 0.0000e+00, -1.2344e+02],\n",
       "        [-1.0308e+01, -3.3364e-05],\n",
       "        [ 0.0000e+00, -7.0941e+01],\n",
       "        [-8.5238e+00, -1.9870e-04],\n",
       "        [-1.0707e+01, -2.2397e-05],\n",
       "        [-1.1111e+01, -1.4950e-05],\n",
       "        [ 0.0000e+00, -5.3871e+01],\n",
       "        [-1.1711e+01, -8.2050e-06],\n",
       "        [-1.3547e+01, -1.3077e-06],\n",
       "        [-4.7142e-11, -2.3778e+01],\n",
       "        [-5.0426e-13, -2.8316e+01],\n",
       "        [-9.1436e+00, -1.0690e-04],\n",
       "        [-2.2956e-09, -1.9892e+01],\n",
       "        [-1.0867e+01, -1.9079e-05],\n",
       "        [ 0.0000e+00, -3.8002e+01],\n",
       "        [-9.7327e+00, -5.9315e-05],\n",
       "        [-9.7173e+00, -6.0232e-05]], dtype=torch.float64,\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = smps(data)\n",
    "out = logsoftmax(out)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 1.], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tpcp(data.permute(1, 0, 2))\n",
    "\n",
    "# loss_batch(out, target)\n",
    "(torch.sgn(out) + 1) / 2 + target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
