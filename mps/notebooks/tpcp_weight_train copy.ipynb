{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opt_einsum as oe\n",
    "import numpy as np\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'mps.simple_mps' from '/Users/keisuke/Documents/presentation/QC_MPS/mps/notebooks/../../mps/simple_mps.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "from mps import simple_mps, tpcp_mps\n",
    "reload(simple_mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_digits(dataset, allowed_digits=[0, 1]):\n",
    "    \"\"\"Return a subset of MNIST dataset containing only allowed_digits (0 or 1).\"\"\"\n",
    "    indices = []\n",
    "    for i in range(len(dataset)):\n",
    "        _, label = dataset[i]\n",
    "        if label in allowed_digits:\n",
    "            indices.append(i)\n",
    "    return torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "\n",
    "def filiter_single_channel(img: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    MNIST is loaded as shape [C, H, W].\n",
    "    Take only the first channel => shape [H, W].\n",
    "    \"\"\"\n",
    "    return img[0, ...]\n",
    "\n",
    "\n",
    "def embedding_pixel(batch, label: int = 0):\n",
    "    \"\"\"\n",
    "    Flatten each image from shape [H, W] => [H*W],\n",
    "    then embed x => [x, 1-x], and L2-normalize along last dim.\n",
    "    \"\"\"\n",
    "    pixel_size = batch.shape[-1] * batch.shape[-2]\n",
    "    x = batch.view(*batch.shape[:-2], pixel_size)\n",
    "    x = torch.stack([x, 1 - x], dim=-1)\n",
    "    x = x / torch.sum(x, dim=-1).unsqueeze(-1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Loss & Accuracy\n",
    "###############################################################################\n",
    "def loss_batch(outputs, labels):\n",
    "    \"\"\"\n",
    "    Binary cross-entropy style loss for outputs in [0, 1].\n",
    "    For label=0 => prob=outputs[i], else => 1 - outputs[i].\n",
    "    \"\"\"\n",
    "    device = outputs.device\n",
    "    loss = torch.zeros(1, device=device, dtype=torch.float64)\n",
    "    for i in range(len(outputs)):\n",
    "        prob = outputs[i] if labels[i] == 0 else (1 - outputs[i])\n",
    "        loss -= torch.log(prob + 1e-8)\n",
    "        # Start of Selection\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"Loss is NaN at i={i}\")\n",
    "            print(prob, outputs[i], labels[i])\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calculate_accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Threshold 0.5 => label 0 or 1. Compare to true labels.\n",
    "    \"\"\"\n",
    "    predictions = (outputs < 0.5).float()\n",
    "    correct = (predictions == labels).float().sum()\n",
    "    return correct / labels.numel()\n",
    "\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "img_size = 16\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(filiter_single_channel),\n",
    "        transforms.Lambda(embedding_pixel),\n",
    "        transforms.Lambda(lambda x: x.to(torch.float64)),  # double precision\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=\"data\", train=True, download=True, transform=transform\n",
    ")\n",
    "# Filter digits 0,1 only\n",
    "trainset = filter_digits(trainset, allowed_digits=[0, 1])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=batch_size, shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path is not set, setting...\n",
      "Found the path\n",
      "Initialized MPS with random matrices\n"
     ]
    }
   ],
   "source": [
    "# ---------- Build MPS model ----------\n",
    "N = img_size * img_size\n",
    "d = l = 2 #data input dimension and class label dimension \n",
    "chi_umps = 2\n",
    "chi_max = 2\n",
    "reload(simple_mps)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "smps = simple_mps.SimpleMPS(\n",
    "    N, \n",
    "    2,\n",
    "    d, \n",
    "    l, \n",
    "    layers=2,\n",
    "    device=device, \n",
    "    dtype=torch.float64, \n",
    "    optimize=\"greedy\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/12665 (0%)]\tLoss: 0.693380 Accuracy: 42.97%\n",
      "Train Epoch: 0 [128/12665 (1%)]\tLoss: 0.693088 Accuracy: 47.66%\n",
      "Train Epoch: 0 [256/12665 (2%)]\tLoss: 0.692907 Accuracy: 53.91%\n",
      "Train Epoch: 0 [384/12665 (3%)]\tLoss: 0.692651 Accuracy: 53.12%\n",
      "Train Epoch: 0 [512/12665 (4%)]\tLoss: 0.692105 Accuracy: 53.91%\n",
      "Train Epoch: 0 [640/12665 (5%)]\tLoss: 0.691262 Accuracy: 51.56%\n",
      "Train Epoch: 0 [768/12665 (6%)]\tLoss: 0.688629 Accuracy: 55.47%\n",
      "Train Epoch: 0 [896/12665 (7%)]\tLoss: 0.686424 Accuracy: 52.34%\n",
      "Train Epoch: 0 [1024/12665 (8%)]\tLoss: 0.683018 Accuracy: 49.22%\n",
      "Train Epoch: 0 [1152/12665 (9%)]\tLoss: 0.671369 Accuracy: 53.12%\n",
      "Train Epoch: 0 [1280/12665 (10%)]\tLoss: 0.652862 Accuracy: 54.69%\n",
      "Train Epoch: 0 [1408/12665 (11%)]\tLoss: 0.643309 Accuracy: 50.78%\n",
      "Train Epoch: 0 [1536/12665 (12%)]\tLoss: 0.600847 Accuracy: 57.03%\n",
      "Train Epoch: 0 [1664/12665 (13%)]\tLoss: 0.607812 Accuracy: 45.31%\n",
      "Train Epoch: 0 [1792/12665 (14%)]\tLoss: 0.448316 Accuracy: 62.50%\n",
      "Train Epoch: 0 [1920/12665 (15%)]\tLoss: 0.556803 Accuracy: 52.34%\n",
      "Train Epoch: 0 [2048/12665 (16%)]\tLoss: 0.619577 Accuracy: 45.31%\n",
      "Train Epoch: 0 [2176/12665 (17%)]\tLoss: 0.493634 Accuracy: 53.12%\n",
      "Train Epoch: 0 [2304/12665 (18%)]\tLoss: 0.467783 Accuracy: 55.47%\n",
      "Train Epoch: 0 [2432/12665 (19%)]\tLoss: 0.525273 Accuracy: 46.88%\n",
      "Train Epoch: 0 [2560/12665 (20%)]\tLoss: 0.464525 Accuracy: 56.25%\n",
      "Train Epoch: 0 [2688/12665 (21%)]\tLoss: 0.498498 Accuracy: 49.22%\n",
      "Train Epoch: 0 [2816/12665 (22%)]\tLoss: 0.447078 Accuracy: 54.69%\n",
      "Train Epoch: 0 [2944/12665 (23%)]\tLoss: 0.464165 Accuracy: 53.12%\n",
      "Train Epoch: 0 [3072/12665 (24%)]\tLoss: 0.399482 Accuracy: 58.59%\n",
      "Train Epoch: 0 [3200/12665 (25%)]\tLoss: 0.438300 Accuracy: 52.34%\n",
      "Train Epoch: 0 [3328/12665 (26%)]\tLoss: 0.430640 Accuracy: 52.34%\n",
      "Train Epoch: 0 [3456/12665 (27%)]\tLoss: 0.367207 Accuracy: 57.81%\n",
      "Train Epoch: 0 [3584/12665 (28%)]\tLoss: 0.367783 Accuracy: 58.59%\n",
      "Train Epoch: 0 [3712/12665 (29%)]\tLoss: 0.408274 Accuracy: 57.81%\n",
      "Train Epoch: 0 [3840/12665 (30%)]\tLoss: 0.357102 Accuracy: 59.38%\n",
      "Train Epoch: 0 [3968/12665 (31%)]\tLoss: 0.365995 Accuracy: 61.72%\n",
      "Train Epoch: 0 [4096/12665 (32%)]\tLoss: 0.375240 Accuracy: 72.66%\n",
      "Train Epoch: 0 [4224/12665 (33%)]\tLoss: 0.347924 Accuracy: 86.72%\n",
      "Train Epoch: 0 [4352/12665 (34%)]\tLoss: 0.367512 Accuracy: 79.69%\n",
      "Train Epoch: 0 [4480/12665 (35%)]\tLoss: 0.321474 Accuracy: 92.19%\n",
      "Train Epoch: 0 [4608/12665 (36%)]\tLoss: 0.343170 Accuracy: 81.25%\n",
      "Train Epoch: 0 [4736/12665 (37%)]\tLoss: 0.378336 Accuracy: 75.00%\n",
      "Train Epoch: 0 [4864/12665 (38%)]\tLoss: 0.317848 Accuracy: 86.72%\n",
      "Train Epoch: 0 [4992/12665 (39%)]\tLoss: 0.283388 Accuracy: 97.66%\n",
      "Train Epoch: 0 [5120/12665 (40%)]\tLoss: 0.264976 Accuracy: 96.88%\n",
      "Train Epoch: 0 [5248/12665 (41%)]\tLoss: 0.287292 Accuracy: 93.75%\n",
      "Train Epoch: 0 [5376/12665 (42%)]\tLoss: 0.264116 Accuracy: 96.09%\n",
      "Train Epoch: 0 [5504/12665 (43%)]\tLoss: 0.299542 Accuracy: 96.09%\n",
      "Train Epoch: 0 [5632/12665 (44%)]\tLoss: 0.211937 Accuracy: 99.22%\n",
      "Train Epoch: 0 [5760/12665 (45%)]\tLoss: 0.222654 Accuracy: 100.00%\n",
      "Train Epoch: 0 [5888/12665 (46%)]\tLoss: 0.185217 Accuracy: 99.22%\n",
      "Train Epoch: 0 [6016/12665 (47%)]\tLoss: 0.162523 Accuracy: 99.22%\n",
      "Train Epoch: 0 [6144/12665 (48%)]\tLoss: 0.098302 Accuracy: 98.44%\n",
      "Train Epoch: 0 [6272/12665 (49%)]\tLoss: 0.039280 Accuracy: 100.00%\n",
      "Train Epoch: 0 [6400/12665 (51%)]\tLoss: 0.047489 Accuracy: 98.44%\n",
      "Train Epoch: 0 [6528/12665 (52%)]\tLoss: 0.111772 Accuracy: 97.66%\n",
      "Train Epoch: 0 [6656/12665 (53%)]\tLoss: 0.041544 Accuracy: 98.44%\n",
      "Train Epoch: 0 [6784/12665 (54%)]\tLoss: 0.042638 Accuracy: 99.22%\n",
      "Train Epoch: 0 [6912/12665 (55%)]\tLoss: 0.030871 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7040/12665 (56%)]\tLoss: 0.012337 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7168/12665 (57%)]\tLoss: 0.012033 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7296/12665 (58%)]\tLoss: 0.001145 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7424/12665 (59%)]\tLoss: 0.000006 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7552/12665 (60%)]\tLoss: 0.021539 Accuracy: 99.22%\n",
      "Train Epoch: 0 [7680/12665 (61%)]\tLoss: 0.000002 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7808/12665 (62%)]\tLoss: 0.000003 Accuracy: 100.00%\n",
      "Train Epoch: 0 [7936/12665 (63%)]\tLoss: 0.173194 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8064/12665 (64%)]\tLoss: 0.166657 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8192/12665 (65%)]\tLoss: 0.000001 Accuracy: 100.00%\n",
      "Train Epoch: 0 [8320/12665 (66%)]\tLoss: 0.073696 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8448/12665 (67%)]\tLoss: 0.090236 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8576/12665 (68%)]\tLoss: 0.009253 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8704/12665 (69%)]\tLoss: 0.007290 Accuracy: 99.22%\n",
      "Train Epoch: 0 [8832/12665 (70%)]\tLoss: 0.003033 Accuracy: 100.00%\n",
      "Train Epoch: 0 [8960/12665 (71%)]\tLoss: 0.000552 Accuracy: 100.00%\n",
      "Train Epoch: 0 [9088/12665 (72%)]\tLoss: 0.013449 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9216/12665 (73%)]\tLoss: 0.081734 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9344/12665 (74%)]\tLoss: 0.229856 Accuracy: 89.06%\n",
      "Train Epoch: 0 [9472/12665 (75%)]\tLoss: 0.024697 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9600/12665 (76%)]\tLoss: 0.011546 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9728/12665 (77%)]\tLoss: 0.017467 Accuracy: 99.22%\n",
      "Train Epoch: 0 [9856/12665 (78%)]\tLoss: 0.006282 Accuracy: 100.00%\n",
      "Train Epoch: 0 [9984/12665 (79%)]\tLoss: 0.010869 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10112/12665 (80%)]\tLoss: 0.007901 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10240/12665 (81%)]\tLoss: 0.011657 Accuracy: 100.00%\n",
      "Train Epoch: 0 [10368/12665 (82%)]\tLoss: 0.017381 Accuracy: 100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m logsoftmax(outputs)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m nnloss(outputs, target)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m data_size \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def accuracy(outputs, target):\n",
    "    return (outputs.argmax(dim=-1) == target).float().mean()\n",
    "losses = []\n",
    "running_loss = 0\n",
    "running_accuracy = 0\n",
    "logsoftmax = torch.nn.LogSoftmax(dim=-1)\n",
    "nnloss = torch.nn.NLLLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(smps.parameters(), lr=0.001)\n",
    "n_samples = 0\n",
    "for epoch in range(10):\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        target = target.to(device).to(torch.int64)\n",
    "        data = data.to(device).permute(1, 0, 2)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = smps(data)\n",
    "        outputs = logsoftmax(outputs)\n",
    "        loss = nnloss(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data_size = data.shape[1]\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        # print(torch.exp(outputs[:10]), target[:10])\n",
    "        \n",
    "        running_loss += loss.item() * data_size\n",
    "        n_samples += data_size\n",
    "        \n",
    "        if batch_idx % 1 == 0:\n",
    "            avg_loss = running_loss / n_samples\n",
    "            avg_accuracy = accuracy(outputs, target)\n",
    "            losses.append(avg_loss)\n",
    "            running_loss = 0\n",
    "            running_accuracy = 0\n",
    "            n_samples = 0\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} Accuracy: {:.2f}%'.format(\n",
    "                epoch, batch_idx * data_size, len(trainloader.dataset),\n",
    "                100. * batch_idx / len(trainloader), avg_loss, avg_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mps import tpcp_mps  \n",
    "\n",
    "reload(tpcp_mps)\n",
    "\n",
    "tpcp = tpcp_mps.MPSTPCP(N, K=1, d=2, with_identity=True, manifold=tpcp_mps.ManifoldType.EXACT)\n",
    "tpcp.W.data[:, 1] = 0\n",
    "tpcp.W.data[:, 0] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpcp.set_canonical_mps(smps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5781)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(outputs, target):\n",
    "    correct = (outputs < 0).float() == target.float()\n",
    "    return correct.float().sum() / target.numel()\n",
    "\n",
    "data, target = next(iter(trainloader))\n",
    "out = tpcp(data)\n",
    "\n",
    "calculate_accuracy(out, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  152.3278759303782 Accuracy:  tensor(0.5781)\n",
      "Loss:  163.4813242453831 Accuracy:  tensor(0.5391)\n",
      "Loss:  168.54286556974026 Accuracy:  tensor(0.5391)\n",
      "Loss:  154.6920416529844 Accuracy:  tensor(0.5312)\n",
      "Loss:  148.96475346766428 Accuracy:  tensor(0.5547)\n",
      "Loss:  155.0554851535941 Accuracy:  tensor(0.5156)\n",
      "Loss:  134.8051555082037 Accuracy:  tensor(0.5547)\n",
      "Loss:  136.01640378461934 Accuracy:  tensor(0.5391)\n",
      "Loss:  155.10144471849856 Accuracy:  tensor(0.5078)\n",
      "Loss:  130.80626742558394 Accuracy:  tensor(0.5312)\n",
      "Loss:  132.78404800383308 Accuracy:  tensor(0.5547)\n",
      "Loss:  150.14285403325962 Accuracy:  tensor(0.5312)\n",
      "Loss:  109.59956925090549 Accuracy:  tensor(0.6016)\n",
      "Loss:  150.49303731536605 Accuracy:  tensor(0.4531)\n",
      "Loss:  95.44415424492747 Accuracy:  tensor(0.6250)\n",
      "Loss:  111.04002203129404 Accuracy:  tensor(0.5547)\n",
      "Loss:  144.69964718887564 Accuracy:  tensor(0.4766)\n",
      "Loss:  108.45220235635139 Accuracy:  tensor(0.5469)\n",
      "Loss:  123.5052850100724 Accuracy:  tensor(0.5625)\n",
      "Loss:  126.4465900508071 Accuracy:  tensor(0.4922)\n",
      "Loss:  104.50486845641296 Accuracy:  tensor(0.5938)\n",
      "Loss:  116.82573059930314 Accuracy:  tensor(0.5156)\n",
      "Loss:  107.62116887944107 Accuracy:  tensor(0.5547)\n",
      "Loss:  109.86954585964013 Accuracy:  tensor(0.5391)\n",
      "Loss:  89.39791582159819 Accuracy:  tensor(0.6172)\n",
      "Loss:  115.37257672201885 Accuracy:  tensor(0.5391)\n",
      "Loss:  118.30958560703299 Accuracy:  tensor(0.5156)\n",
      "Loss:  108.08854213012883 Accuracy:  tensor(0.5781)\n",
      "Loss:  98.17337842932183 Accuracy:  tensor(0.6094)\n",
      "Loss:  95.56783914758535 Accuracy:  tensor(0.6406)\n",
      "Loss:  107.41280422696212 Accuracy:  tensor(0.5781)\n",
      "Loss:  109.69932467606712 Accuracy:  tensor(0.5469)\n",
      "Loss:  119.40790059271865 Accuracy:  tensor(0.5000)\n",
      "Loss:  111.09177281764761 Accuracy:  tensor(0.5469)\n",
      "Loss:  121.07101484058661 Accuracy:  tensor(0.5000)\n",
      "Loss:  99.52074757391352 Accuracy:  tensor(0.5781)\n",
      "Loss:  100.41653363559207 Accuracy:  tensor(0.5469)\n",
      "Loss:  122.509511228922 Accuracy:  tensor(0.5000)\n",
      "Loss:  103.52282442407878 Accuracy:  tensor(0.5547)\n",
      "Loss:  103.49795487844247 Accuracy:  tensor(0.5703)\n",
      "Loss:  102.1129048917118 Accuracy:  tensor(0.5859)\n",
      "Loss:  99.48954496066185 Accuracy:  tensor(0.5781)\n",
      "Loss:  94.91298215886727 Accuracy:  tensor(0.5938)\n",
      "Loss:  115.774208495422 Accuracy:  tensor(0.5234)\n",
      "Loss:  94.1186100778663 Accuracy:  tensor(0.6016)\n",
      "Loss:  105.4360397079816 Accuracy:  tensor(0.5156)\n",
      "Loss:  95.93254273710434 Accuracy:  tensor(0.5703)\n",
      "Loss:  109.22599166461356 Accuracy:  tensor(0.5391)\n",
      "Loss:  106.99710299580279 Accuracy:  tensor(0.5156)\n",
      "Loss:  117.8297525004132 Accuracy:  tensor(0.5234)\n",
      "Loss:  100.74939911196496 Accuracy:  tensor(0.5547)\n",
      "Loss:  103.3465343337515 Accuracy:  tensor(0.5234)\n",
      "Loss:  94.96686686827852 Accuracy:  tensor(0.5859)\n",
      "Loss:  89.22453529225646 Accuracy:  tensor(0.6094)\n",
      "Loss:  100.87591864651297 Accuracy:  tensor(0.5469)\n",
      "Loss:  96.40942230652878 Accuracy:  tensor(0.5859)\n",
      "Loss:  100.77650785558018 Accuracy:  tensor(0.5547)\n",
      "Loss:  113.71780275880985 Accuracy:  tensor(0.5156)\n",
      "Loss:  92.7703121644332 Accuracy:  tensor(0.5781)\n",
      "Loss:  121.84411629854252 Accuracy:  tensor(0.5234)\n",
      "Loss:  100.68396027425631 Accuracy:  tensor(0.5078)\n",
      "Loss:  86.65816957573698 Accuracy:  tensor(0.5781)\n",
      "Loss:  101.00798247418885 Accuracy:  tensor(0.5234)\n",
      "Loss:  100.75496109491385 Accuracy:  tensor(0.5391)\n",
      "Loss:  86.78395140269934 Accuracy:  tensor(0.5859)\n",
      "Loss:  102.4242894363457 Accuracy:  tensor(0.5547)\n",
      "Loss:  116.92247300178352 Accuracy:  tensor(0.4609)\n",
      "Loss:  95.51874597745822 Accuracy:  tensor(0.5547)\n",
      "Loss:  102.06766364815243 Accuracy:  tensor(0.5000)\n",
      "Loss:  89.4690997415754 Accuracy:  tensor(0.5859)\n",
      "Loss:  91.13117269506911 Accuracy:  tensor(0.5859)\n",
      "Loss:  105.53508133579733 Accuracy:  tensor(0.5000)\n",
      "Loss:  83.64508216394685 Accuracy:  tensor(0.6094)\n",
      "Loss:  95.11797791176829 Accuracy:  tensor(0.5312)\n",
      "Loss:  103.72991907766257 Accuracy:  tensor(0.4922)\n",
      "Loss:  98.54947033972199 Accuracy:  tensor(0.5000)\n",
      "Loss:  76.69451115074195 Accuracy:  tensor(0.6172)\n",
      "Loss:  103.133945842223 Accuracy:  tensor(0.5000)\n",
      "Loss:  95.29951688755114 Accuracy:  tensor(0.5234)\n",
      "Loss:  84.64693497035218 Accuracy:  tensor(0.5625)\n",
      "Loss:  101.70228193496895 Accuracy:  tensor(0.4688)\n",
      "Loss:  93.4771752718476 Accuracy:  tensor(0.5547)\n",
      "Loss:  90.2168600153277 Accuracy:  tensor(0.5234)\n",
      "Loss:  101.26997732525872 Accuracy:  tensor(0.5078)\n",
      "Loss:  94.25281313621898 Accuracy:  tensor(0.5312)\n",
      "Loss:  92.03631061468683 Accuracy:  tensor(0.5078)\n",
      "Loss:  86.66970590658234 Accuracy:  tensor(0.5547)\n",
      "Loss:  93.15942547031776 Accuracy:  tensor(0.5391)\n",
      "Loss:  94.923411771306 Accuracy:  tensor(0.5156)\n",
      "Loss:  95.3085455689334 Accuracy:  tensor(0.5078)\n",
      "Loss:  97.88145496282223 Accuracy:  tensor(0.5078)\n",
      "Loss:  89.00700556793453 Accuracy:  tensor(0.5156)\n",
      "Loss:  83.37157433935666 Accuracy:  tensor(0.5469)\n",
      "Loss:  90.31579034538639 Accuracy:  tensor(0.5078)\n",
      "Loss:  85.427365732177 Accuracy:  tensor(0.5469)\n",
      "Loss:  88.98159433804327 Accuracy:  tensor(0.5234)\n",
      "Loss:  94.05482074091937 Accuracy:  tensor(0.5625)\n",
      "Loss:  87.42259055123994 Accuracy:  tensor(0.5156)\n",
      "Loss:  76.59850747602898 Accuracy:  tensor(0.5455)\n",
      "Epoch 0 / 100 / Loss: 107.33985639786023 / Accuracy: 0.5437830090522766\n",
      "Loss:  74.6355686696208 Accuracy:  tensor(0.5938)\n",
      "Loss:  81.14949752951459 Accuracy:  tensor(0.5703)\n",
      "Loss:  83.39440750423947 Accuracy:  tensor(0.5469)\n",
      "Loss:  82.14333210267007 Accuracy:  tensor(0.5391)\n",
      "Loss:  82.21077055513909 Accuracy:  tensor(0.5547)\n",
      "Loss:  84.08533060930581 Accuracy:  tensor(0.5234)\n",
      "Loss:  75.4086168793919 Accuracy:  tensor(0.5547)\n",
      "Loss:  79.59458593446035 Accuracy:  tensor(0.5312)\n",
      "Loss:  85.00281231976199 Accuracy:  tensor(0.5234)\n",
      "Loss:  80.5502721806049 Accuracy:  tensor(0.5312)\n",
      "Loss:  78.33150603996896 Accuracy:  tensor(0.5547)\n",
      "Loss:  82.04148617241604 Accuracy:  tensor(0.5391)\n",
      "Loss:  70.82189637328713 Accuracy:  tensor(0.5859)\n",
      "Loss:  92.8848945976143 Accuracy:  tensor(0.4609)\n",
      "Loss:  66.35924351536706 Accuracy:  tensor(0.6328)\n",
      "Loss:  77.56246081766683 Accuracy:  tensor(0.5312)\n",
      "Loss:  88.5718304953282 Accuracy:  tensor(0.4766)\n",
      "Loss:  75.90964486932084 Accuracy:  tensor(0.5312)\n",
      "Loss:  77.20823294983633 Accuracy:  tensor(0.5703)\n",
      "Loss:  85.91102464009303 Accuracy:  tensor(0.4844)\n",
      "Loss:  71.472514370605 Accuracy:  tensor(0.5703)\n",
      "Loss:  79.76285856920838 Accuracy:  tensor(0.5234)\n",
      "Loss:  71.20138741729987 Accuracy:  tensor(0.5547)\n",
      "Loss:  72.56256910898443 Accuracy:  tensor(0.5469)\n",
      "Loss:  60.78921962966389 Accuracy:  tensor(0.6250)\n",
      "Loss:  77.86456741912596 Accuracy:  tensor(0.5312)\n",
      "Loss:  77.4800358515592 Accuracy:  tensor(0.5391)\n",
      "Loss:  67.87555662017657 Accuracy:  tensor(0.5859)\n",
      "Loss:  67.47960758476185 Accuracy:  tensor(0.6250)\n",
      "Loss:  59.40450999479057 Accuracy:  tensor(0.6406)\n",
      "Loss:  70.16575479584034 Accuracy:  tensor(0.5781)\n",
      "Loss:  68.64428510643178 Accuracy:  tensor(0.5625)\n",
      "Loss:  73.82381870063325 Accuracy:  tensor(0.5078)\n",
      "Loss:  68.53355294149732 Accuracy:  tensor(0.5547)\n",
      "Loss:  76.86334056547449 Accuracy:  tensor(0.4922)\n",
      "Loss:  65.24505386650335 Accuracy:  tensor(0.5781)\n",
      "Loss:  65.89218733618476 Accuracy:  tensor(0.5469)\n",
      "Loss:  74.17830865224748 Accuracy:  tensor(0.5000)\n",
      "Loss:  64.28816531310858 Accuracy:  tensor(0.5703)\n",
      "Loss:  66.68855140085236 Accuracy:  tensor(0.5703)\n",
      "Loss:  65.88093176009492 Accuracy:  tensor(0.5938)\n",
      "Loss:  65.43708586375274 Accuracy:  tensor(0.5859)\n",
      "Loss:  59.08419131162289 Accuracy:  tensor(0.6016)\n",
      "Loss:  77.22762826390691 Accuracy:  tensor(0.5312)\n",
      "Loss:  60.75651825277319 Accuracy:  tensor(0.6016)\n",
      "Loss:  66.99103235487289 Accuracy:  tensor(0.5469)\n",
      "Loss:  63.50991449129073 Accuracy:  tensor(0.5938)\n",
      "Loss:  69.46967869845805 Accuracy:  tensor(0.5547)\n",
      "Loss:  68.74448850859211 Accuracy:  tensor(0.5234)\n",
      "Loss:  70.67053748466114 Accuracy:  tensor(0.5547)\n",
      "Loss:  66.47563371543096 Accuracy:  tensor(0.5625)\n",
      "Loss:  66.1712828860456 Accuracy:  tensor(0.5469)\n",
      "Loss:  62.464114208947265 Accuracy:  tensor(0.6016)\n",
      "Loss:  59.20022172818185 Accuracy:  tensor(0.6250)\n",
      "Loss:  63.356887250354035 Accuracy:  tensor(0.5703)\n",
      "Loss:  61.30715438028114 Accuracy:  tensor(0.5859)\n",
      "Loss:  64.30525027666606 Accuracy:  tensor(0.5703)\n",
      "Loss:  70.16579753722206 Accuracy:  tensor(0.5547)\n",
      "Loss:  60.988131933541055 Accuracy:  tensor(0.5781)\n",
      "Loss:  69.0828285112659 Accuracy:  tensor(0.5547)\n",
      "Loss:  65.29446168577108 Accuracy:  tensor(0.5469)\n",
      "Loss:  57.155029345868705 Accuracy:  tensor(0.6172)\n",
      "Loss:  64.80178788823078 Accuracy:  tensor(0.5859)\n",
      "Loss:  64.48750720026044 Accuracy:  tensor(0.5703)\n",
      "Loss:  57.125737584510425 Accuracy:  tensor(0.6016)\n",
      "Loss:  69.17841563350522 Accuracy:  tensor(0.5859)\n",
      "Loss:  73.09507244623013 Accuracy:  tensor(0.5156)\n",
      "Loss:  61.65046652481625 Accuracy:  tensor(0.5859)\n",
      "Loss:  63.79767738663681 Accuracy:  tensor(0.5703)\n",
      "Loss:  57.873470817006066 Accuracy:  tensor(0.6016)\n",
      "Loss:  61.84105831808892 Accuracy:  tensor(0.6016)\n",
      "Loss:  66.09417656454472 Accuracy:  tensor(0.5391)\n",
      "Loss:  57.69418892173026 Accuracy:  tensor(0.6641)\n",
      "Loss:  59.90759078651764 Accuracy:  tensor(0.5859)\n",
      "Loss:  64.91643881408812 Accuracy:  tensor(0.5547)\n",
      "Loss:  62.27963836652262 Accuracy:  tensor(0.5469)\n",
      "Loss:  50.90660189274748 Accuracy:  tensor(0.6797)\n",
      "Loss:  65.34110577428817 Accuracy:  tensor(0.5859)\n",
      "Loss:  60.10995206550231 Accuracy:  tensor(0.5625)\n",
      "Loss:  55.11093524590171 Accuracy:  tensor(0.6016)\n",
      "Loss:  63.68848862117104 Accuracy:  tensor(0.5469)\n",
      "Loss:  63.64570984947939 Accuracy:  tensor(0.6172)\n",
      "Loss:  57.330480042174386 Accuracy:  tensor(0.5938)\n",
      "Loss:  65.96911685494564 Accuracy:  tensor(0.5781)\n",
      "Loss:  60.18173539210587 Accuracy:  tensor(0.5625)\n",
      "Loss:  58.47722818229716 Accuracy:  tensor(0.6016)\n",
      "Loss:  60.66522047439806 Accuracy:  tensor(0.6719)\n",
      "Loss:  60.05620200758461 Accuracy:  tensor(0.5859)\n",
      "Loss:  60.793766584933735 Accuracy:  tensor(0.5469)\n",
      "Loss:  60.619254019541 Accuracy:  tensor(0.5859)\n",
      "Loss:  65.43468791229465 Accuracy:  tensor(0.5781)\n",
      "Loss:  57.3047302715035 Accuracy:  tensor(0.6016)\n",
      "Loss:  54.33242414569573 Accuracy:  tensor(0.6172)\n",
      "Loss:  57.774356181301236 Accuracy:  tensor(0.6172)\n",
      "Loss:  56.016792600224974 Accuracy:  tensor(0.6250)\n",
      "Loss:  56.811450989624575 Accuracy:  tensor(0.6172)\n",
      "Loss:  62.95127913803223 Accuracy:  tensor(0.6016)\n",
      "Loss:  56.794340650836105 Accuracy:  tensor(0.5781)\n",
      "Loss:  49.52898833468357 Accuracy:  tensor(0.6612)\n",
      "Epoch 1 / 100 / Loss: 67.49818318115265 / Accuracy: 0.5713879466056824\n",
      "Loss:  49.3565510400475 Accuracy:  tensor(0.6797)\n",
      "Loss:  52.55150638853479 Accuracy:  tensor(0.6328)\n",
      "Loss:  55.217988535858616 Accuracy:  tensor(0.6016)\n",
      "Loss:  53.6111211610501 Accuracy:  tensor(0.6172)\n",
      "Loss:  53.8703883577259 Accuracy:  tensor(0.6016)\n",
      "Loss:  54.897267905461256 Accuracy:  tensor(0.6094)\n",
      "Loss:  49.1013528493359 Accuracy:  tensor(0.7031)\n",
      "Loss:  52.04949689707298 Accuracy:  tensor(0.6562)\n",
      "Loss:  54.0910978394859 Accuracy:  tensor(0.6250)\n",
      "Loss:  52.9712475809577 Accuracy:  tensor(0.6016)\n",
      "Loss:  51.482210588534166 Accuracy:  tensor(0.6328)\n",
      "Loss:  53.42988223961374 Accuracy:  tensor(0.6016)\n",
      "Loss:  46.70263655939798 Accuracy:  tensor(0.6953)\n",
      "Loss:  58.52108084645692 Accuracy:  tensor(0.5781)\n",
      "Loss:  44.16916086134222 Accuracy:  tensor(0.7578)\n",
      "Loss:  50.825941812848235 Accuracy:  tensor(0.6562)\n",
      "Loss:  56.63711505301263 Accuracy:  tensor(0.6094)\n",
      "Loss:  49.72524101435595 Accuracy:  tensor(0.6641)\n",
      "Loss:  49.608159938371315 Accuracy:  tensor(0.6797)\n",
      "Loss:  55.98575931682954 Accuracy:  tensor(0.5938)\n",
      "Loss:  46.9325078398936 Accuracy:  tensor(0.6641)\n",
      "Loss:  53.99780986465468 Accuracy:  tensor(0.6641)\n",
      "Loss:  47.0454194726373 Accuracy:  tensor(0.7266)\n",
      "Loss:  46.945508640160085 Accuracy:  tensor(0.7266)\n",
      "Loss:  39.57731704733586 Accuracy:  tensor(0.7969)\n",
      "Loss:  48.86214371786744 Accuracy:  tensor(0.7266)\n",
      "Loss:  50.633102381301114 Accuracy:  tensor(0.7031)\n",
      "Loss:  43.334283587729935 Accuracy:  tensor(0.7500)\n",
      "Loss:  41.673387475189024 Accuracy:  tensor(0.7500)\n",
      "Loss:  37.919863031232616 Accuracy:  tensor(0.8047)\n",
      "Loss:  45.72137840471365 Accuracy:  tensor(0.7188)\n",
      "Loss:  44.28332531430502 Accuracy:  tensor(0.7109)\n",
      "Loss:  47.76973667204256 Accuracy:  tensor(0.7422)\n",
      "Loss:  41.36017044218232 Accuracy:  tensor(0.8594)\n",
      "Loss:  49.32445752553212 Accuracy:  tensor(0.6719)\n",
      "Loss:  49.35597113130786 Accuracy:  tensor(0.8594)\n",
      "Loss:  45.72165481509962 Accuracy:  tensor(0.8359)\n",
      "Loss:  47.38656511340022 Accuracy:  tensor(0.7344)\n",
      "Loss:  40.70732523980733 Accuracy:  tensor(0.8438)\n",
      "Loss:  42.368187904214544 Accuracy:  tensor(0.7500)\n",
      "Loss:  42.08969265641894 Accuracy:  tensor(0.7500)\n",
      "Loss:  44.05002590006223 Accuracy:  tensor(0.8125)\n",
      "Loss:  36.55840033397562 Accuracy:  tensor(0.8672)\n",
      "Loss:  45.09669843469633 Accuracy:  tensor(0.7500)\n",
      "Loss:  38.021658367831236 Accuracy:  tensor(0.8281)\n",
      "Loss:  41.66337994383202 Accuracy:  tensor(0.8047)\n",
      "Loss:  39.72540794645578 Accuracy:  tensor(0.8750)\n",
      "Loss:  46.161883199993646 Accuracy:  tensor(0.7734)\n",
      "Loss:  43.220739391739194 Accuracy:  tensor(0.8281)\n",
      "Loss:  40.014863778565946 Accuracy:  tensor(0.8594)\n",
      "Loss:  42.502392478573405 Accuracy:  tensor(0.8438)\n",
      "Loss:  41.31549766502741 Accuracy:  tensor(0.8359)\n",
      "Loss:  36.7093571442408 Accuracy:  tensor(0.8516)\n",
      "Loss:  35.82028865879576 Accuracy:  tensor(0.8203)\n",
      "Loss:  38.19172625380988 Accuracy:  tensor(0.8203)\n",
      "Loss:  35.94261767219442 Accuracy:  tensor(0.8359)\n",
      "Loss:  39.531972763901415 Accuracy:  tensor(0.7812)\n",
      "Loss:  37.74587939851971 Accuracy:  tensor(0.8906)\n",
      "Loss:  36.29272962023314 Accuracy:  tensor(0.9062)\n",
      "Loss:  38.62786288911478 Accuracy:  tensor(0.8984)\n",
      "Loss:  38.80620728047233 Accuracy:  tensor(0.9141)\n",
      "Loss:  33.023345188149335 Accuracy:  tensor(0.9219)\n",
      "Loss:  37.160238363237745 Accuracy:  tensor(0.8984)\n",
      "Loss:  40.85712106707813 Accuracy:  tensor(0.8672)\n",
      "Loss:  32.721491388100915 Accuracy:  tensor(0.9375)\n",
      "Loss:  36.31847790724513 Accuracy:  tensor(0.8828)\n",
      "Loss:  45.24072305039746 Accuracy:  tensor(0.8359)\n",
      "Loss:  34.78723623497414 Accuracy:  tensor(0.9219)\n",
      "Loss:  36.77741797601772 Accuracy:  tensor(0.9531)\n",
      "Loss:  31.581812994897806 Accuracy:  tensor(0.9375)\n",
      "Loss:  32.37431635804886 Accuracy:  tensor(0.9688)\n",
      "Loss:  38.74708100891294 Accuracy:  tensor(0.9375)\n",
      "Loss:  30.19307576477492 Accuracy:  tensor(0.9766)\n",
      "Loss:  34.24726479667127 Accuracy:  tensor(0.9297)\n",
      "Loss:  37.671083036315984 Accuracy:  tensor(0.9297)\n",
      "Loss:  34.7962805405898 Accuracy:  tensor(0.9844)\n",
      "Loss:  26.796730166161936 Accuracy:  tensor(0.9531)\n",
      "Loss:  36.1495305443437 Accuracy:  tensor(0.9375)\n",
      "Loss:  34.31839091943745 Accuracy:  tensor(0.9766)\n",
      "Loss:  29.886210419571498 Accuracy:  tensor(0.9922)\n",
      "Loss:  35.881249224275976 Accuracy:  tensor(0.9844)\n",
      "Loss:  31.66606399000005 Accuracy:  tensor(0.9844)\n",
      "Loss:  31.516695354461717 Accuracy:  tensor(0.9844)\n",
      "Loss:  35.72170314665362 Accuracy:  tensor(0.9844)\n",
      "Loss:  33.792502702757865 Accuracy:  tensor(0.9688)\n",
      "Loss:  32.06547869622029 Accuracy:  tensor(1.)\n",
      "Loss:  28.441820592654228 Accuracy:  tensor(1.)\n",
      "Loss:  32.614622471642186 Accuracy:  tensor(0.9609)\n",
      "Loss:  33.722605560566016 Accuracy:  tensor(0.9844)\n",
      "Loss:  32.8971508884511 Accuracy:  tensor(0.9766)\n",
      "Loss:  32.46779852526484 Accuracy:  tensor(1.)\n",
      "Loss:  30.89289430877495 Accuracy:  tensor(1.)\n",
      "Loss:  29.00667320138661 Accuracy:  tensor(1.)\n",
      "Loss:  31.74142992249722 Accuracy:  tensor(1.)\n",
      "Loss:  29.16192409225843 Accuracy:  tensor(0.9922)\n",
      "Loss:  30.94615421730568 Accuracy:  tensor(1.)\n",
      "Loss:  28.102096800947532 Accuracy:  tensor(0.9922)\n",
      "Loss:  30.432177824258144 Accuracy:  tensor(1.)\n",
      "Loss:  26.01344238550692 Accuracy:  tensor(1.)\n",
      "Epoch 2 / 100 / Loss: 40.95501935167841 / Accuracy: 0.8276515007019043\n",
      "Loss:  26.555959458610424 Accuracy:  tensor(1.)\n",
      "Loss:  28.27313067746153 Accuracy:  tensor(1.)\n",
      "Loss:  31.432680448645094 Accuracy:  tensor(0.9922)\n",
      "Loss:  28.454643712487435 Accuracy:  tensor(1.)\n",
      "Loss:  28.64721541119219 Accuracy:  tensor(1.)\n",
      "Loss:  29.386345112738585 Accuracy:  tensor(1.)\n",
      "Loss:  25.982478486215992 Accuracy:  tensor(1.)\n",
      "Loss:  27.474311880463848 Accuracy:  tensor(1.)\n",
      "Loss:  28.730525137519294 Accuracy:  tensor(1.)\n",
      "Loss:  29.617229394996123 Accuracy:  tensor(0.9922)\n",
      "Loss:  26.986429838042035 Accuracy:  tensor(1.)\n",
      "Loss:  29.359629344063773 Accuracy:  tensor(1.)\n",
      "Loss:  24.23567283593914 Accuracy:  tensor(1.)\n",
      "Loss:  31.46612821985814 Accuracy:  tensor(1.)\n",
      "Loss:  20.270596286595257 Accuracy:  tensor(1.)\n",
      "Loss:  26.450948521925692 Accuracy:  tensor(1.)\n",
      "Loss:  30.058244711626394 Accuracy:  tensor(1.)\n",
      "Loss:  25.781053786529952 Accuracy:  tensor(1.)\n",
      "Loss:  25.19596307632345 Accuracy:  tensor(1.)\n",
      "Loss:  29.85755239121708 Accuracy:  tensor(1.)\n",
      "Loss:  24.197816854465266 Accuracy:  tensor(1.)\n",
      "Loss:  25.91859271557407 Accuracy:  tensor(1.)\n",
      "Loss:  26.50369750042103 Accuracy:  tensor(0.9922)\n",
      "Loss:  24.11141332649393 Accuracy:  tensor(1.)\n",
      "Loss:  19.567721147401368 Accuracy:  tensor(1.)\n",
      "Loss:  25.312006345356018 Accuracy:  tensor(0.9922)\n",
      "Loss:  25.381085180303888 Accuracy:  tensor(0.9922)\n",
      "Loss:  21.910717700207808 Accuracy:  tensor(1.)\n",
      "Loss:  21.090118717629856 Accuracy:  tensor(1.)\n",
      "Loss:  19.114367039911627 Accuracy:  tensor(1.)\n",
      "Loss:  23.62154945989056 Accuracy:  tensor(1.)\n",
      "Loss:  22.836035063795748 Accuracy:  tensor(1.)\n",
      "Loss:  24.619196622730062 Accuracy:  tensor(1.)\n",
      "Loss:  20.86251820885212 Accuracy:  tensor(1.)\n",
      "Loss:  25.501369954832487 Accuracy:  tensor(1.)\n",
      "Loss:  23.199753617471835 Accuracy:  tensor(0.9844)\n",
      "Loss:  24.574614145051385 Accuracy:  tensor(0.9922)\n",
      "Loss:  24.78305528486436 Accuracy:  tensor(1.)\n",
      "Loss:  20.44396702601713 Accuracy:  tensor(1.)\n",
      "Loss:  21.73444640365742 Accuracy:  tensor(1.)\n",
      "Loss:  22.230401530324528 Accuracy:  tensor(1.)\n",
      "Loss:  20.53186756273999 Accuracy:  tensor(0.9922)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_batch(outputs, target)\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 18\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m acc \u001b[38;5;241m=\u001b[39m calculate_accuracy(outputs, target)\n\u001b[1;32m     20\u001b[0m acc_tot \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m acc\n",
      "File \u001b[0;32m~/miniconda3/envs/py310/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/presentation/QC_MPS/mps/notebooks/../../mps/radam.py:128\u001b[0m, in \u001b[0;36mRiemannianAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39mdiv(bias_correction2)\u001b[38;5;241m.\u001b[39msqrt_()\n\u001b[0;32m--> 128\u001b[0m direction \u001b[38;5;241m=\u001b[39m exp_avg\u001b[38;5;241m.\u001b[39mdiv(bias_correction1) \u001b[38;5;241m/\u001b[39m (\u001b[43mdenom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Update point on the Stiefel manifold using our expmap:\u001b[39;00m\n\u001b[1;32m    131\u001b[0m new_point \u001b[38;5;241m=\u001b[39m expmap_with_given_M(point, \u001b[38;5;241m-\u001b[39mdirection \u001b[38;5;241m*\u001b[39m learning_rate)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mps.StiefelOptimizers import StiefelAdam\n",
    "from mps.radam import RiemannianAdam\n",
    "W = torch.zeros(tpcp.L, 2, dtype=torch.float64)\n",
    "W[:, 0] = 1\n",
    "W[:, 1] = 0\n",
    "tpcp.initialize_W(W)\n",
    "optimizer = RiemannianAdam(tpcp.parameters(), lr=0.0001, betas=(0.9, 0.999))\n",
    "# optimzier = StiefelAdam(tpcp.parameters(), lr=0.0001, expm_method=\"ForwardEuler\")\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    acc_tot = 0\n",
    "    loss_tot = 0\n",
    "    for data, target in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = tpcp(data)\n",
    "        loss = loss_batch(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc = calculate_accuracy(outputs, target)\n",
    "        acc_tot += acc\n",
    "        loss_tot += loss.item()\n",
    "        print(\"Loss: \", loss.item(), \"Accuracy: \", acc)\n",
    "\n",
    "    print(f\"Epoch {epoch} / {epochs} / Loss: {loss_tot / len(trainloader)} / Accuracy: {acc_tot / len(trainloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0300e+01, -3.3639e-05],\n",
       "        [-1.0873e+01, -1.8957e-05],\n",
       "        [-1.1319e+01, -1.2135e-05],\n",
       "        [-1.0072e+01, -4.2227e-05],\n",
       "        [-1.1331e+01, -1.2001e-05],\n",
       "        [-8.8451e+00, -1.4410e-04],\n",
       "        [ 0.0000e+00, -4.8987e+01],\n",
       "        [-3.3434e-08, -1.7214e+01],\n",
       "        [-9.8337e+00, -5.3613e-05],\n",
       "        [-1.0920e+01, -1.8086e-05],\n",
       "        [-3.4195e-14, -3.1010e+01],\n",
       "        [-1.0579e+01, -2.5457e-05],\n",
       "        [-6.0174e-14, -3.0443e+01],\n",
       "        [-7.9001e+00, -3.7078e-04],\n",
       "        [ 0.0000e+00, -3.8507e+01],\n",
       "        [-9.9722e+00, -4.6679e-05],\n",
       "        [-4.7244e-11, -2.3776e+01],\n",
       "        [-5.5502e-12, -2.5917e+01],\n",
       "        [-1.0454e+01, -2.8819e-05],\n",
       "        [-3.4917e-05, -1.0263e+01],\n",
       "        [-2.2871e-14, -3.1411e+01],\n",
       "        [ 0.0000e+00, -4.2346e+01],\n",
       "        [ 0.0000e+00, -4.4464e+01],\n",
       "        [-1.0186e+01, -3.7694e-05],\n",
       "        [-3.7323e-08, -1.7104e+01],\n",
       "        [ 0.0000e+00, -7.0261e+01],\n",
       "        [-8.3386e+00, -2.3913e-04],\n",
       "        [-8.4054e+00, -2.2367e-04],\n",
       "        [-2.2990e-12, -2.6799e+01],\n",
       "        [-9.8108e+00, -5.4855e-05],\n",
       "        [ 0.0000e+00, -1.0694e+02],\n",
       "        [ 0.0000e+00, -5.5585e+01],\n",
       "        [ 0.0000e+00, -8.0064e+01],\n",
       "        [-1.0305e+01, -3.3460e-05],\n",
       "        [-2.6461e-03, -5.9360e+00],\n",
       "        [-1.1151e+01, -1.4368e-05],\n",
       "        [-1.0448e+01, -2.9003e-05],\n",
       "        [-1.1947e+01, -6.4773e-06],\n",
       "        [-1.2487e+01, -3.7749e-06],\n",
       "        [-7.9270e-14, -3.0165e+01],\n",
       "        [-8.7669e+00, -1.5583e-04],\n",
       "        [-7.7959e+00, -4.1152e-04],\n",
       "        [-1.0397e+01, -3.0529e-05],\n",
       "        [-9.1846e-11, -2.3111e+01],\n",
       "        [-1.4428e-10, -2.2659e+01],\n",
       "        [-1.1305e+01, -1.2313e-05],\n",
       "        [-5.7732e-15, -3.2788e+01],\n",
       "        [-8.8348e+00, -1.4558e-04],\n",
       "        [-4.3195e+00, -1.3396e-02],\n",
       "        [ 0.0000e+00, -4.5750e+01],\n",
       "        [-9.8743e+00, -5.1481e-05],\n",
       "        [ 0.0000e+00, -6.7362e+01],\n",
       "        [-7.5816e-11, -2.3303e+01],\n",
       "        [-9.4808e+00, -7.6304e-05],\n",
       "        [-7.2686e-08, -1.6437e+01],\n",
       "        [-5.1070e-15, -3.2899e+01],\n",
       "        [ 0.0000e+00, -7.3113e+01],\n",
       "        [-9.5608e+00, -7.0437e-05],\n",
       "        [ 0.0000e+00, -4.3780e+01],\n",
       "        [-1.3420e-10, -2.2732e+01],\n",
       "        [-2.4872e-04, -8.2993e+00],\n",
       "        [-1.0308e+01, -3.3352e-05],\n",
       "        [-1.0609e+01, -2.4692e-05],\n",
       "        [-1.0112e+01, -4.0607e-05],\n",
       "        [-9.2991e+00, -9.1509e-05],\n",
       "        [ 0.0000e+00, -7.9220e+01],\n",
       "        [-1.2370e+01, -4.2446e-06],\n",
       "        [-9.2417e+00, -9.6915e-05],\n",
       "        [-9.3244e+00, -8.9228e-05],\n",
       "        [-1.0151e+01, -3.9049e-05],\n",
       "        [-9.5334e+00, -7.2398e-05],\n",
       "        [-1.0867e+01, -1.9071e-05],\n",
       "        [-8.6078e+00, -1.8270e-04],\n",
       "        [-1.1359e+01, -1.1658e-05],\n",
       "        [-1.4509e-04, -8.8382e+00],\n",
       "        [-2.0113e-04, -8.5117e+00],\n",
       "        [-9.6000e-10, -2.0764e+01],\n",
       "        [-8.8818e-16, -3.4576e+01],\n",
       "        [-9.3467e+00, -8.7259e-05],\n",
       "        [-1.0057e+01, -4.2879e-05],\n",
       "        [-9.6938e+00, -6.1667e-05],\n",
       "        [-1.0341e+01, -3.2295e-05],\n",
       "        [-1.0348e+01, -3.2072e-05],\n",
       "        [ 0.0000e+00, -4.8151e+01],\n",
       "        [-8.7428e-09, -1.8555e+01],\n",
       "        [-1.1302e-13, -2.9811e+01],\n",
       "        [-1.2438e+01, -3.9643e-06],\n",
       "        [-1.1489e+01, -1.0242e-05],\n",
       "        [-1.0490e+01, -2.7825e-05],\n",
       "        [-1.0153e+01, -3.8944e-05],\n",
       "        [-1.1145e+01, -1.4441e-05],\n",
       "        [-1.1016e+01, -1.6445e-05],\n",
       "        [-5.0633e-12, -2.6009e+01],\n",
       "        [ 0.0000e+00, -4.5396e+01],\n",
       "        [-1.0687e+01, -2.2841e-05],\n",
       "        [-1.7497e-05, -1.0954e+01],\n",
       "        [-1.1177e+01, -1.3990e-05],\n",
       "        [-2.2204e-16, -3.5648e+01],\n",
       "        [-2.2204e-16, -3.5662e+01],\n",
       "        [-6.6613e-16, -3.5122e+01],\n",
       "        [ 0.0000e+00, -7.6281e+01],\n",
       "        [-1.0800e+01, -2.0392e-05],\n",
       "        [ 0.0000e+00, -5.8252e+01],\n",
       "        [-1.1082e-10, -2.2923e+01],\n",
       "        [-5.0003e-09, -1.9114e+01],\n",
       "        [ 0.0000e+00, -9.4440e+01],\n",
       "        [-9.5185e+00, -7.3479e-05],\n",
       "        [-4.9147e-04, -7.6184e+00],\n",
       "        [ 0.0000e+00, -8.6677e+01],\n",
       "        [ 0.0000e+00, -4.1636e+01],\n",
       "        [-1.0143e+01, -3.9352e-05],\n",
       "        [ 0.0000e+00, -1.2344e+02],\n",
       "        [-1.0308e+01, -3.3364e-05],\n",
       "        [ 0.0000e+00, -7.0941e+01],\n",
       "        [-8.5238e+00, -1.9870e-04],\n",
       "        [-1.0707e+01, -2.2397e-05],\n",
       "        [-1.1111e+01, -1.4950e-05],\n",
       "        [ 0.0000e+00, -5.3871e+01],\n",
       "        [-1.1711e+01, -8.2050e-06],\n",
       "        [-1.3547e+01, -1.3077e-06],\n",
       "        [-4.7142e-11, -2.3778e+01],\n",
       "        [-5.0426e-13, -2.8316e+01],\n",
       "        [-9.1436e+00, -1.0690e-04],\n",
       "        [-2.2956e-09, -1.9892e+01],\n",
       "        [-1.0867e+01, -1.9079e-05],\n",
       "        [ 0.0000e+00, -3.8002e+01],\n",
       "        [-9.7327e+00, -5.9315e-05],\n",
       "        [-9.7173e+00, -6.0232e-05]], dtype=torch.float64,\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = smps(data)\n",
    "out = logsoftmax(out)\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 1.], dtype=torch.float64, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = tpcp(data.permute(1, 0, 2))\n",
    "\n",
    "# loss_batch(out, target)\n",
    "(torch.sgn(out) + 1) / 2 + target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
